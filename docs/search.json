[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaughingRook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 5, 2022\n\n\nCase-Control Studies\n\n\nM249,Statistics\n\n\n\n\nSep 3, 2022\n\n\nAdvent of Code 2015, Day 3\n\n\nAdventOfCode\n\n\n\n\nSep 1, 2022\n\n\nStack ADT\n\n\nAbstractDataType,Algorithm\n\n\n\n\nAug 30, 2022\n\n\nCollege Tuition Fees in the USA\n\n\nTidyTuesday\n\n\n\n\nAug 29, 2022\n\n\nMatched Case-Control Studies\n\n\nM249,Statistics\n\n\n\n\nAug 27, 2022\n\n\nAdvent of Code 2015, Day 2\n\n\nAdventOfCode\n\n\n\n\nAug 25, 2022\n\n\nTravelling Salesperson Problem (Brute-force Search)\n\n\nAlgorithm,Graph\n\n\n\n\nAug 22, 2022\n\n\nStratified Analyses\n\n\nM249,Statistics\n\n\n\n\nAug 20, 2022\n\n\nAdvent of Code 2015, Day 1\n\n\nAdventOfCode\n\n\n\n\nAug 18, 2022\n\n\nInitialising an Undirected Graph in NetworkX\n\n\nGraph\n\n\n\n\nAug 15, 2022\n\n\nCohort Studies\n\n\nM249,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html",
    "title": "Cohort Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a cohort study.\nThis topics is covered by M249 Book 1, Part 1.\n\n\nData on a cohort study analysing the possible association between compulsory redundancies and incidents of serious self-inflicted injury (SSI) (Keefe, V., et al (2002)) was sourced. The exposure is being made compulsorily redundant, and the disease is incidents of serious self-inflicted injury.\nThe study results were as follows.\n\n\n\n\nSSI (+)\nno SSI (-)\n\n\n\n\nmade redundant (+)\n14\n1931\n\n\nnot made redundant (-)\n4\n1763\n\n\n\n\n\n\nThe analysis was undertaken using StatsModels and SciPy.\nThe data were stored remotely, so we defined a function cache_file to handle their retrieval. The data were stored in a CSV file with schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ndisease\nstr\nDescriptive label indicating category of disease\n\n\n\nThe exposure and disease labels were stored as two lists, with variables named exposures and diseases.\nNote, the orders of the lists are important given Table2x21 requires the data to be some sequence with shape (2, 2):\n\n\n\n\ndisease (+)\nno diease (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nAs such, the labels should be ordered:\n\nexposures = (exposed, not exposed)\ndiseases = (disease, no disease)\n\nThe data were cached and used to a initialise data, a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the exposure, disease columns as ordered Categorical data. They were ordered to ensure the data would be reshaped as expected.2\nThe DataFrame cat_data was taken as a Numpy NDArray with shape (2, 2) with name data_arr, and used to initialise an instance of Table2x2 named ctable.\nMeasures of association3 were calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. We rounded-off the analysis by performing Fisher’s exact test.4 5"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#dependencies",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#dependencies",
    "title": "Cohort Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#functions",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#functions",
    "title": "Cohort Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname, and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#main",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#main",
    "title": "Cohort Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = ['redundant', 'not redundant']\ndiseases = ['ssi', 'no ssi']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/redundancy_ssi.csv'),\n    fname='redundancy_ssi.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   disease   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\nOutput a view of data.\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      disease\n    \n  \n  \n    \n      0\n      14\n      redundant\n      ssi\n    \n    \n      1\n      1931\n      redundant\n      no ssi\n    \n    \n      2\n      4\n      not redundant\n      ssi\n    \n    \n      3\n      1763\n      not redundant\n      no ssi\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns exposure, disease as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    disease=pd.Categorical(data['disease'], diseases, ordered=True)\n).sort_values(\n    by=['exposure', 'disease']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   disease   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput a pivot table with the marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='disease',\n    aggfunc='sum',\n    margins=True,\n    margins_name='subtotal'\n).query(\n    \"exposure != 'subtotal'\"\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n      subtotal\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      redundant\n      14\n      1931\n      1945\n    \n    \n      not redundant\n      4\n      1763\n      1767\n    \n  \n\n\n\n\n\n\nInitialise the contingency table\nGet the data as a NumPy ndarray with shape (2, 2).\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\n\nInitialise an instance of Table2x2 using data_arr.\n\nctable = sm.stats.Table2x2(data_arr)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[  14. 1931.]\n [   4. 1763.]]\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    3.195495\nlcb      1.049877\nucb      9.726081\nName: odds ratio, dtype: float64\n\n\nReturn point and interval estimates of the relative risk.\n\npd.Series(\n    data=[ctable.riskratio,\n          ctable.riskratio_confint()[0],\n          ctable.riskratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='relative risk'\n)\n\npoint    3.179692\nlcb      1.048602\nucb      9.641829\nName: relative risk, dtype: float64\n\n\n\n\nChi-squared test for no association\nReturn the expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[   9.43157328, 1935.56842672],\n       [   8.56842672, 1758.43157328]])\n\n\nReturn the differences between the observed and expected frequencies.\n\ndata_arr - ctable.fittedvalues\n\narray([[ 4.56842672, -4.56842672],\n       [-4.56842672,  4.56842672]])\n\n\nReturn the contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[2.21283577, 0.01078263],\n       [2.43574736, 0.01186883]])\n\n\nReturn the results of the chi-squared test.6\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    4.671235\npvalue      0.030672\ndf                 1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.033877\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#references",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#references",
    "title": "Cohort Studies",
    "section": "References",
    "text": "References\nVera Keefe, Papaarangi Reid, Clint Ormsby, Bridget Robson, Gordon Purdie, Joanne Baxter, Ngäti Kahungunu Iwi Incorporated, Serious health events following involuntary job loss in New Zealand meat processing workers, International Journal of Epidemiology, Volume 31, Issue 6, December 2002, Pages 1155–1161, https://doi.org/10.1093/ije/31.6.1155\n\n%load_ext watermark\n%watermark --iv\n\nrequests   : 2.28.1\nnumpy      : 1.23.2\nscipy      : 1.9.0\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nstatsmodels: 0.13.2\npandas     : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "",
    "text": "Initialise and populate an undirected weighted graph using NetworkX.\nThe source data is a CSV file listing the road network in Europe as an edge list.1\nWe first imported the data into a Pandas DataFrame.2 Next, we exported the DataFrame as a collection of dictionaries.3 We initialised an empty graph,4 and populated it. We closed the notebook by showing how to access the nodes, neighbors, and edges of the graph.\nWhilst we could populate the graph during initialisation, we found it added unneeded complexity.\nThe |edges| ≠ |edge list| because NetworkX’s Graph class does not permit parallel edges between two nodes.5"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom dataclasses import dataclass\nimport pandas as pd\nimport networkx as nx\n\n\nClasses\n\n@dataclass(frozen=True)\nclass PandasERoad:\n    \"\"\"A dataclass to help the conversion of the raod network data as a\n    graph.\n\n    Stores the remote url and maps the column titles to common graph\n    terminology\n    \"\"\"\n\n    url: str = ('https://raw.githubusercontent.com/ljk233'\n                + '/laughingrook-datasets/main/graphs/eroads_edge_list.csv')\n    u: str = 'origin_reference_place'\n    v: str = 'destination_reference_place'\n    uco: str = 'origin_country_code'\n    vco: str = 'destination_country_code'\n    w: str = 'distance'\n    rn: str = 'road_number'\n    wc: str = 'watercrossing'"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Functions",
    "text": "Functions\n\ndef edge_to_tuple(edge: dict, er: PandasERoad) -> tuple:\n    return (\n        edge[er.u],\n        edge[er.v],\n        {'weight': edge[er.w], er.rn: edge[er.rn], er.wc: edge[er.wc]}\n    )"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Main",
    "text": "Main\n\ner = PandasERoad()\n\n\nImport the data\n\neroads = pd.read_csv(er.url)\neroads.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 7 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   road_number                  1250 non-null   object\n 1   origin_country_code          1250 non-null   object\n 2   origin_reference_place       1250 non-null   object\n 3   destination_country_code     1250 non-null   object\n 4   destination_reference_place  1250 non-null   object\n 5   distance                     1250 non-null   int64 \n 6   watercrossing                1250 non-null   bool  \ndtypes: bool(1), int64(1), object(5)\nmemory usage: 59.9+ KB\n\n\n\n\nExport data to a dictionary\nEach entry in the list is a dictionary representing a single row, where the keys are the column titles.\n\nedges = eroads.to_dict(orient='records')\nedges[0]\n\n{'road_number': 'E01',\n 'origin_country_code': 'GB',\n 'origin_reference_place': 'Larne',\n 'destination_country_code': 'GB',\n 'destination_reference_place': 'Belfast',\n 'distance': 36,\n 'watercrossing': False}\n\n\n\n\nInitalise and populate the graph\n\ng = nx.Graph()\n\nAdds the nodes. We perform it on both the source and destination nodes in the edge list to ensure we populate all the cities, given there’s a chance that a city does not appear as a source city in the data.\n\ng.add_nodes_from((e[er.u], {'country': e[er.uco]}) for e in edges)\ng.add_nodes_from((e[er.v], {'country': e[er.vco]}) for e in edges)\n\nAdd the edges. Given this is an undirected graph, there is no need to add the reverse edges V → U.\nExample of output from edge_to_tuple\n\nedge_to_tuple(edges[0], er)\n\n('Larne',\n 'Belfast',\n {'weight': 36, 'road_number': 'E01', 'watercrossing': False})\n\n\nPopulate the edges.\n\ng.add_edges_from(edge_to_tuple(edge, er) for edge in edges)\n\n\n\nInspect the graph\nGet a description of the graph.\n\nprint(g)\n\nGraph with 894 nodes and 1198 edges\n\n\nGet a selection of the nodes.\n\n[n for n in g][:5]\n\n['Larne', 'Belfast', 'Dublin', 'Wexford', 'Rosslare']\n\n\nOutput a more descriptive list of nodes by calling the nodes() method.\n\n[n for n in g.nodes(data=True)][:5]\n\n[('Larne', {'country': 'GB'}),\n ('Belfast', {'country': 'GB'}),\n ('Dublin', {'country': 'IRL'}),\n ('Wexford', {'country': 'IRL'}),\n ('Rosslare', {'country': 'IRL'})]\n\n\nView the neighbours of the Roma node.\n\n[neighbor for neighbor in g['Roma']]\n\n['Arezzo', 'Grosseto', 'Pescara', 'San Cesareo']\n\n\nWe can get a more descriptive output of a node’s neighbours by not using list comprehension.\n\ng['Roma']\n\nAtlasView({'Arezzo': {'weight': 219, 'road_number': 'E35', 'watercrossing': False}, 'Grosseto': {'weight': 182, 'road_number': 'E80', 'watercrossing': False}, 'Pescara': {'weight': 209, 'road_number': 'E80', 'watercrossing': False}, 'San Cesareo': {'weight': 36, 'road_number': 'E821', 'watercrossing': False}})\n\n\nFinally, we can simply output the edges of the Roma node.\n\n[e for e in g.edges('Roma', data=True)]\n\n[('Roma',\n  'Arezzo',\n  {'weight': 219, 'road_number': 'E35', 'watercrossing': False}),\n ('Roma',\n  'Grosseto',\n  {'weight': 182, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'Pescara',\n  {'weight': 209, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'San Cesareo',\n  {'weight': 36, 'road_number': 'E821', 'watercrossing': False})]\n\n\n\n%load_ext watermark\n%watermark --iversions\n\nnetworkx: 2.8.6\npandas  : 1.4.3\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "title": "Advent of Code 2015, Day 1",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 1: Not Quite Lisp."
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "title": "Advent of Code 2015, Day 1",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom itertools import accumulate\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "title": "Advent of Code 2015, Day 1",
    "section": "Functions",
    "text": "Functions\n\ndef find_first(x, A) -> int:\n    \"\"\"Find the first index i where A[i] = x.\n\n    Precondtions:\n    - x in A\n    - A is 1-dimensional\n    - A support iteration\n    \"\"\"\n    return next(i for i, a in enumerate(A) if a == x)"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "title": "Advent of Code 2015, Day 1",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 1)\nprint(f\"line = '{line[:5]}'\")\n\nline = '()((('\n\n\n\n\nTransform the input\n\nm = {'(': 1, ')': -1}\ndirections = [m[bracket] for bracket in line]\nprint(f\"directions = {directions[:5]}\")\n\ndirections = [1, -1, 1, 1, 1]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(directions)}\")\n\nSolution = 138\n\n\n\n\nPart 2\n\nprint(f\"Solution = {find_first(-1, accumulate(directions)) + 1}\")\n\nSolution = 1771\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(directions)\nprint('Part 2 =')\n%timeit find_first(-1, accumulate(directions)) + 1\n\nPart 1 =\n62.5 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nPart 2 =\n73.6 µs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html",
    "title": "Stratified Analyses",
    "section": "",
    "text": "Perform a stratified analyses on the results of a stratified case-control study.\nThis topic is covered in M249, Book 1, Part 2.\n\n\nData were taken from investigating the possible association between alcohol consumption and fatal car accidents in New York (J.R. McCarroll and W. Haddon Jr, 1962). The data were stratified by marital status, which was believed to be a possible confounder. The exposure was blood alcohol level of 100mg% or greater; Cases were drivers who were killed in car accidents for which they were considered to be responsible; and controls were selected drivers passing the locations where the accidents of the cases occurred, at the same time of day and on the same day of the week.\nThe results were as follows.\nLevel: Married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n4\n5\n\n\nunder 100mg% (-)\n5\n103\n\n\n\nLevel: Not married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n10\n3\n\n\nunder 100mg% (-)\n5\n43\n\n\n\n\n\n\nThe analysis was undertaken using StatsModels.\nThe data was stored on remotely, so we defined the function cache_file to handle the retrieval of the file. The datay were stored in a CSV file with schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nlevel\nstr\nDescriptive label indicating category of level\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ncasecon\nstr\nDescriptive label indicating category of case or control\n\n\n\nThe function get_odds_ratio_arr was defined to return a point and interval estimates of the odds ratio as a list.\nThe exposure, and case-control labels were stored as two lists, with variables named exposures, and casecons.\nNote, the orders of exposures, casesons are important: Table2x21 requires the data to be some sequence with shape (2, 2):\n\n\n\nexposure\ndisease (+)\nno diease (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nAnd StratifiedTable2 requires the data to be some sequence with shape (2, 2, n), where n is the number of levels in the data:\n\n\n\nlevel\nexposure\ndisease (+)\nno diease (-)\n\n\n\n\none\nexposed (+)\na\nb\n\n\n\nnot exposed (-)\nc\nd\n\n\ntwo\nexposed (+)\ne\nf\n\n\n\nnot exposed (-)\ng\nh\n\n\n\nAs such, the labels should be ordered:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\nThe data were cached and used to a initialise a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the level exposure, casecon columns as ordered Categorical data. We casted the exposure and casecons columns to ordered Categorical data, to ensure the data would appear as expected.3 The level columns was taken as levels, a NumPy NDArray.\nThe main difficulty with a stratified analysis is that we need to deal with the data in many different shapes. Three variables were initialised to handle the different views of the data. These were:\n\nlevel_tables, a collection of Table2x2 instances, where each instance represents a contingency table for a specific level\naggregated_table, an instance of Table2x2 using the aggregated data4\nstrat_table, an instance of StratifiedTable\n\nThe level-specific, unadjusted5 and adjusted6 odds ratios were calculated, including their confidence interval estimates. Two hypothesis tests were performed: a test of no association and a test of homogeneity."
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#dependencies",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#dependencies",
    "title": "Stratified Analyses",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nfrom dataclasses import dataclass\nimport numpy as np\nimport pandas as pd\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#functions",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#functions",
    "title": "Stratified Analyses",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path\n\n\ndef get_odds_ratio_arr(table: sm.stats.Table2x2, alpha: float = 0.05) -> list:\n    \"\"\"Return the point and (100-alpha)% lower and upper confidence\n    boundaries.\n\n    Preconditions:\n    - 0 < alpha < 1\n    \"\"\"\n    return [table.oddsratio,\n            table.oddsratio_confint(alpha=alpha)[0],\n            table.oddsratio_confint(alpha=alpha)[1]]"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#main",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#main",
    "title": "Stratified Analyses",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = ['over 100mg', 'under 100mg']\ncasecons = ['fatality', 'no fatality']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n        url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n             + '/main/m249/medical/drinkdriving.csv'),\n        fname='drinkdriving.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\nOutput a view of data.\n\ndata\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns level as a Categorical variable, and exposure, casecon as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    level=pd.Categorical(data['level']),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    casecon=pd.Categorical(data['casecon'], casecons, ordered=True)\n).sort_values(\n    by=['level', 'exposure', 'casecon']\n)\ncat_data.info()\n\nGet level as a NumPy NDArray.\n\nlevels = np.unique(cat_data['level'])\nlevels\n\nOutput pivot tables with marginal totals.\n\nfor level in levels:\n    print(\n        cat_data.query(\n            'level == @level'\n        ).pivot_table(\n            values='n',\n            index=['level', 'exposure'],\n            columns='casecon',\n            aggfunc='sum',\n            margins=True,\n            margins_name='subtotal'\n        ).query(\n            \"level in [@level, 'subtotal']\"\n        ),\n        '\\n'\n    )\n\n\n\nInitialise the contingency tables\nInitialise contingency tables for different views of the data.\n\n_make_table2x2 = lambda df, level: (\n    sm.stats.Table2x2(\n        df.query('level == @level')['n']\n          .to_numpy()\n          .reshape(2, 2)\n    )\n)\nlevel_tables = [_make_table2x2(cat_data, level) for level in levels]\nfor table, level in zip(level_tables, levels):\n    print(f'level={level}\\n{table}\\n')\n\n\naggregated_table = sm.stats.Table2x2(\n    cat_data.groupby(['exposure', 'casecon'])\n            .sum()\n            .to_numpy()\n            .reshape((2, 2))\n)\nprint(aggregated_table)\n\n\nstrat_table = (\n    sm.stats.StratifiedTable(\n                data.sort_values(by=['exposure', 'casecon', 'level'])['n']\n                    .to_numpy()\n                    .reshape((2, 2, 2))\n    )\n)\nprint(strat_table.table)\n\n\n\nOdds ratios\nReturn point and interval estimates of the level-specific, unadjusted, and adjusted odds ratios.\nReturn the level-specific odds ratios.\n\npd.DataFrame(\n    data=[get_odds_ratio_arr(table) for table in level_tables],\n    columns=['point', 'lcb', 'ucb'],\n    index=pd.Index(levels, name='level')\n).round(\n    5\n)\n\nReturn the unadjusted odds ratio.\n\npd.Series(\n    data=get_odds_ratio_arr(aggregated_table),\n    index=['point', 'lcb', 'ucb'],\n    name='unadjusted odds ratio'\n).round(\n    5\n)\n\nReturn the adjusted odds ratio.\n\npd.Series(\n    data=[strat_table.oddsratio_pooled,\n           strat_table.oddsratio_pooled_confint()[0],\n           strat_table.oddsratio_pooled_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='adjusted odds ratio'\n).round(\n    3\n)\n\n\n\nHypothesis testing\nReturn the result of a test of no association.\n\n_r = strat_table.test_null_odds(correction=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of no association'\n).round(\n    3\n)\n\nReturn the result of a test of homogeneity.\n\n_r = strat_table.test_equal_odds(adjust=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of homogeneity'\n).round(\n    3\n)"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#references",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#references",
    "title": "Stratified Analyses",
    "section": "References",
    "text": "References\nMcCarroll, J.R. and Haddon Jr, W., 1962. A controlled study of fatal automobile accidents in New York City. Journal of chronic diseases, 15(8), pp.811-826.\n\n%load_ext watermark\n%watermark --iv"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html",
    "href": "posts/2022-08-25-tsp_bruteforce.html",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "",
    "text": "A solution to the Travelling Salesperson Problem using a brute-force search.\nThe Travelling Salesperson Problem is defined as:\n\nGiven a set of cities and distances between every pair of cities, the [Travelling Salesperson] problem is to find the shortest possible route that visits every city exactly once and returns to the starting point.\nTraveling Salesman Problem (TSP) Implementation (GeeksForGeeks)\n\nIn this implementation, we generate permutations and check if the |path| < |min path|.\nWe use a complete undirected graph, with each edge being assigned a random weight (representing the distance).\nWhilst the function works, it is unusuable when |nodes(g)| ≥ 11, given P(11, 11) = 36720000 permutations to check!"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "href": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport random as rand\nimport math\nimport itertools as it\nimport networkx as nx"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#function",
    "href": "posts/2022-08-25-tsp_bruteforce.html#function",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Function",
    "text": "Function\n\ndef bruteforce_tsp(G: nx.Graph, start: object) -> float | int:\n    \"\"\"Return the shortest route that visits every city exactly once and\n    ends back at the start.\n\n    Solves the travelling salesperson with a brute-force search using\n    permutations.\n\n    Preconditions:\n    - G is a complete weighted graph\n    - start in G\n    - WG[u, v]['weight'] is the distance u -> v\n    \"\"\"\n    neighbours = set((node for node in G.nodes if node != start))\n    min_dist = math.inf\n    for path in it.permutations(neighbours):\n        u, dist = start, 0\n        for v in path:\n            dist += G.edges[u, v]['weight']\n            u = v\n        min_dist = min(min_dist, dist + G.edges[u, start]['weight'])\n\n    return min_dist"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#main",
    "href": "posts/2022-08-25-tsp_bruteforce.html#main",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Main",
    "text": "Main\n\nInitialise the graph\nWe initialise a complete weighted undirected graph with 5 nodes.\n\ncg = nx.complete_graph(['origin', 'a', 'b', 'c', 'd'])\ng = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\nprint(f\"g = {g}\")\n\ng = Graph with 5 nodes and 10 edges\n\n\n\n\nFind the shortest path from the origin\n\nprint(f\"Shortest path from the origin = {bruteforce_tsp(g, 'origin')}\")\n\nShortest path from the origin = 27\n\n\n\n\nPerformance\n\nfor n in [4, 6, 8, 10]:\n    print(f\"|nodes(g)| = {n}\")\n    cg = nx.complete_graph(n)\n    g = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\n    %timeit bruteforce_tsp(g, 1)\n\n|nodes(g)| = 4\n8.19 µs ± 61.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n|nodes(g)| = 6\n206 µs ± 1.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n|nodes(g)| = 8\n11.1 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n|nodes(g)| = 10\n969 ms ± 6.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnetworkx: 2.8.6"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "title": "Advent of Code 2015, Day 2",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 2: I Was Told There Would Be No Math."
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "title": "Advent of Code 2015, Day 2",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "title": "Advent of Code 2015, Day 2",
    "section": "Functions",
    "text": "Functions\n\ndef paper_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the wrapping paper needed for a present with dims (l, w, h).\n    \"\"\"\n    planes = [(length * width), (width * height), (height * length)]\n    return min(planes) + (2 * sum(planes))\n\n\ndef ribbon_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the ribbon needed to cover a present with dims (l, w, h).\n    \"\"\"\n    bow = length * width * height\n    ribbon = 2 * sum(sorted([length, width, height])[:2])\n    return bow + ribbon"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "title": "Advent of Code 2015, Day 2",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nlines = lr.datasets.get_advent_input(2015, 2)\nprint(f\"lines = {lines[:5]}\")\n\nfile was cached.\nlines = ['4x23x21', '22x29x19', '11x4x11', '8x10x5', '24x18x16']\n\n\n\n\nTransform the input\n\nllines = (line.split('x') for line in lines)\npdims = [[int(x) for x in lline] for lline in llines]\nprint(f\"pdims = {pdims[:5]}\")\n\npdims = [[4, 23, 21], [22, 29, 19], [11, 4, 11], [8, 10, 5], [24, 18, 16]]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(paper_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 1598415\n\n\n\n\nPart 2\n\nprint(f\"Solution = {sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 3812909\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(paper_needed(l, w, h) for (l, w, h) in pdims)\nprint('Part 2 =')\n%timeit sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)\n\nPart 1 =\n433 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPart 2 =\n468 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html",
    "title": "Matched Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a 1-1 matched case-control study.\nData was taken from a case-control study undertaken to identify some of the riskfactors associated with death during heatwave in Chicago that occured from 12 July 1995 to 16 July 1995. Cases were persons aged 24+ years who died between 14-17 July 1995, with a cause mentioned on the death certificate that was possibly heat related. For each case, a matched control was selected of the same age and living in the same neighbourhood. The risk factor of interest was participation in group activities involving social interactions. (Semenza, Rubin, Falter, et al (1996))\nThe results were as follows.\n\n\n\n\n\n\n\n\nCases / Controls\nParticipated (+)\nDid not participate (-)\n\n\n\n\nParticipated (+)\n77\n63\n\n\nDid not participate (-)\n90\n74\n\n\n\nThe analysis was undertaken using StatsModels1 and a user-defined function.2 The results were initialised as a NumPy array.3\nThe data was stored on remotely, so we defined a function cache_file to handle the retrieval of the file.\nWe initialised a single list, casecons, which held the labels for the specific cases and controls.\nThe data were cached and used to a initialise a Pandas DataFrame.\nWe casted the cases and controls columns to ordered Categorical data, to ensure the data would appear as expected.4\nA Mantel-Haenszel odds ratio for the association between participation in group activities and dying of heat-related disease was calculated, including a 95% confidence interval estimate. Finally, McNemar’s test5 was performed to test the null hypothesis of no association between participation in group activities and dying of heat-related disease.\nThese topics are covered by M249 Book 1, Part 2."
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#dependencies",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#dependencies",
    "title": "Matched Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm\nfrom numpy.typing import ArrayLike"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#functions",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#functions",
    "title": "Matched Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path\n\n\ndef mh_odds_ratio(ctable: ArrayLike, alpha: float = 0.05) -> tuple:\n    \"\"\"Return point and 100(1-alpha)% intervel estimates of the\n    Mantel-Haenszel odds ratio.\n\n    Pre-conditions:\n    - arr represents the results of a 1-1 matched case-control study\n        - shape(arr) = 2, 2\n        - rows represent cases, columns represent controls\n        - row 0, col 0 represent (+)\n        - row 1, col 1 represent (-)\n    - 0 < alpha < 1\n\n    Post-conditions:\n    - tuple of float estimates, (point, lcb, ucb)\n    \"\"\"\n\n    f, g = ctable[0, 1], ctable[1, 0]\n    ste = (st.norm.ppf(1 - (alpha/2)) * np.sqrt(1/f + 1/g))\n    return f / g, f / g * np.exp(-ste), f / g * np.exp(ste)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#main",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#main",
    "title": "Matched Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\ncasecons = ['participated', 'not participated']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/heat_deaths.csv'),\n    fname='heat_deaths.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   cases     4 non-null      object\n 2   controls  4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      cases\n      controls\n    \n  \n  \n    \n      0\n      77\n      participated\n      participated\n    \n    \n      1\n      63\n      participated\n      not participated\n    \n    \n      2\n      90\n      not participated\n      participated\n    \n    \n      3\n      74\n      not participated\n      not participated\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns cases, controls as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    cases=pd.Categorical(data['cases'], casecons, ordered=True),\n    controls=pd.Categorical(data['controls'], casecons, ordered=True)\n).sort_values(\n    by=['cases', 'controls']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   cases     4 non-null      category\n 2   controls  4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput pivot tables with marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='cases',\n    columns='controls',\n    aggfunc='sum',\n    margins=True,\n    margins_name='subtotal'\n)\n\n\n\n\n\n  \n    \n      controls\n      participated\n      not participated\n      subtotal\n    \n    \n      cases\n      \n      \n      \n    \n  \n  \n    \n      participated\n      77\n      63\n      140\n    \n    \n      not participated\n      90\n      74\n      164\n    \n    \n      subtotal\n      167\n      137\n      304\n    \n  \n\n\n\n\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\ndata_arr\n\narray([[77, 63],\n       [90, 74]], dtype=int64)\n\n\n\n\nMatel-Haenszel odds ratio\n\n_point, _lcb, _ucb = mh_odds_ratio(data_arr)\npd.Series(\n    data=[_point, _lcb, _ucb],\n    index=['point', 'lcb', 'ucb'],\n    name='mantel-haenszel odds ratio'\n)\n\npoint    0.700000\nlcb      0.507309\nucb      0.965881\nName: mantel-haenszel odds ratio, dtype: float64\n\n\n\n\nMcNemar’s test\n\n_r = sm.stats.mcnemar(data_arr, exact=False)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistc', 'pvalue'],\n    name='mcnemar''s test'\n)\n\nstatistc    4.418301\npvalue      0.035555\nName: mcnemars test, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#references",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#references",
    "title": "Matched Case-Control Studies",
    "section": "References",
    "text": "References\nSemenza, J. C., Rubin, C. H., Falter, K. H., Selanikio, J. D., Flanders, W. D., Howe, H. L., & Wilhelm, J. L. (1996). Heat-related deaths during the July 1995 heat wave in Chicago. New England journal of medicine, 335, 84-90. https://doi.org/10.1056/NEJM199607113350203.\n\n%load_ext watermark\n%watermark --iv\n\nrequests   : 2.28.1\nscipy      : 1.9.0\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\npandas     : 1.4.3\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "title": "College Tuition Fees in the USA",
    "section": "",
    "text": "Visualising the average annual cost of College tuition fees in the USA.\n\n\nHappy to announce the newest #R4DS online learning community project! #TidyTuesday is your weekly #tidyverse practice!Each week we'll post data and a plot at https://t.co/8NaXR93uIX under the datasets link.You clean the data and tweak the plot in R!#rstats #ggplot2 pic.twitter.com/sDaHsB8uwL\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 2, 2018"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "title": "College Tuition Fees in the USA",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport pandas as pd\nimport polars as pl\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "title": "College Tuition Fees in the USA",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "title": "College Tuition Fees in the USA",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\nus_avg_tuition_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-02/us_avg_tuition.xlsx?raw=true'),\n    fname='us_avg_tuition.xlsx'\n)\n\n\nansi_path = cache_file(\n    url=('https://www2.census.gov/geo/docs/reference/state.txt'),\n    fname='state.txt'\n)\n\n\n\nLoad the data\n\nus_avg_tuition = pl.DataFrame(pd.read_excel(us_avg_tuition_path)).lazy()\nus_avg_tuition.schema\n\n{'State': polars.datatypes.Utf8,\n '2004-05': polars.datatypes.Float64,\n '2005-06': polars.datatypes.Float64,\n '2006-07': polars.datatypes.Float64,\n '  2007-08 ': polars.datatypes.Float64,\n '2008-09': polars.datatypes.Float64,\n '2009-10': polars.datatypes.Float64,\n '2010-11': polars.datatypes.Float64,\n '2011-12': polars.datatypes.Float64,\n '2012-13': polars.datatypes.Float64,\n '2013-14': polars.datatypes.Float64,\n '2014-15': polars.datatypes.Float64,\n '2015-16': polars.datatypes.Float64}\n\n\n\nansi = pl.DataFrame(pd.read_csv(ansi_path, sep='|')).lazy()\nansi.schema\n\n{'STATE': polars.datatypes.Int64,\n 'STUSAB': polars.datatypes.Utf8,\n 'STATE_NAME': polars.datatypes.Utf8,\n 'STATENS': polars.datatypes.Int64}\n\n\n\nstates = alt.topo_feature(vdata.us_10m.url, 'states')\n\n\n\nPrepare the data\n\nlazy_query = us_avg_tuition.select(\n    ['State',\n     '2010-11',\n     '2015-16']\n).melt(\n    id_vars='State',\n    variable_name='year',\n    value_name='tuition'\n).join(\n    other=ansi,\n    left_on='State',\n    right_on='STATE_NAME',\n    how='inner'\n).with_column(\n    pl.col('tuition').pct_change().over('State').alias('pct_change')\n).filter(\n    pl.col('pct_change').is_not_null()\n).select(\n    [pl.col('STATE').alias('state_id'),\n     pl.col('State').alias('state_name'),\n     pl.col('tuition'),\n     pl.col('pct_change').apply(lambda x: x * 100).round(1)]\n)\nlazy_query.schema\n\n{'state_id': polars.datatypes.Int64,\n 'state_name': polars.datatypes.Utf8,\n 'tuition': polars.datatypes.Float64,\n 'pct_change': polars.datatypes.Float64}\n\n\n\n\nVisualise the data\nBoth visualisations will use the same source, so we initialise a single instance of alt.Chart.\n\nch = alt.Chart(lazy_query.collect().to_pandas())\n\nPlot the percentage change in college annual tuition costs from 2010 to 2015 as a chloropleth heatmap.\n\nch.mark_geoshape(\n    stroke='black'\n).encode(\n    shape='geo:G',\n    color=alt.Color(\n                \"pct_change\",\n                scale=alt.Scale(scheme=\"oranges\"),\n                legend=alt.Legend(title='Change (%)')\n    ),\n    tooltip=[alt.Tooltip('state_name', title='State'),\n             alt.Tooltip('pct_change', title='Change (%)')]\n).transform_lookup(\n    lookup='state_id',\n    from_=alt.LookupData(data=states, key='id'),\n    as_='geo'\n).project(\n    type='albersUsa'\n).properties(\n    title='Percentage change in college tuition costs between 2010 and 2015',\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nPlot the annual cost of college tuition in the USA in 2015-16 as a dot plot.\n\nch.mark_circle(\n    size=60\n).encode(\n    x=alt.X('tuition', title='Tuition ($)'),\n    y=alt.Y('state_name', sort='-x', axis=alt.Axis(grid=True), title='State')\n).properties(\n    title='College tuition costs in the USA (2015-16)',\n    width=400,\n    height=600\n)\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\npolars  : 0.14.6\naltair  : 4.2.0\nrequests: 2.28.1\npandas  : 1.4.3"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html",
    "href": "posts/2022-09-01-stack_adt.html",
    "title": "Stack ADT",
    "section": "",
    "text": "Using Python’s collections.deque[^3] class as an implementation of the Stack ADT. A stack is:"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#dependencies",
    "href": "posts/2022-09-01-stack_adt.html#dependencies",
    "title": "Stack ADT",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import deque"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#function",
    "href": "posts/2022-09-01-stack_adt.html#function",
    "title": "Stack ADT",
    "section": "Function",
    "text": "Function\nWe used a deque for the Stack in this implementation of has_balanced_brackets.\n\ndef has_balanced_brackets(expression: str) -> bool:\n    \"\"\"Return true if the given expression has balanced brackets.\n    \"\"\"\n    stack = deque()\n    matching = {')': '(', '}': '{', ']': '[', '>': '<'}\n    left, right = set(matching.values()), matching.keys()\n    for char in expression:\n        if char in left:\n            stack.append(char)\n        elif char in right:\n            if len(stack) == 0 or stack[-1] != matching[char]:\n                return False\n            stack.pop()\n    return len(stack) == 0"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#main",
    "href": "posts/2022-09-01-stack_adt.html#main",
    "title": "Stack ADT",
    "section": "Main",
    "text": "Main\n\nTesting\n(Apologies for the non-standard table representation!)\n\n#         desc           expression                          exp result   \ncases = [['no text',     '',                                 True],\n         ['no brackets', 'brackets are like Russian dolls',  True],\n         ['matched',     '(3 + 4)',                          True],\n         ['mismatched',  '(3 + 4]',                          False],\n         ['not opened',  '3 + 4]',                           False],\n         ['not closed',  '(3 + 4',                           False],\n         ['wrong order', 'close ) before open (',            False],\n         ['no nesting',  '()[]\\{\\}<>',                       True],\n         ['nested',      '([{(<[{}]>)}])',                   True],\n         ['nested pair', 'items[(i - 1):(i + 1)]',           True]]\n\nfor (test, expr, exp_res) in cases:\n    assert has_balanced_brackets(expr) == exp_res, f'Test {test} failed'\nprint('Testing complete.')\n\nTesting complete.\n\n\n\n\nPerformance\nThe performance tests show a doubling in has_balanced_brackets’s time-to-run, as the |expression| doubles. It has a linear complexity.\n\nbrackets = \"()\"\nsizes = [100, 200, 400, 800]\nfor i, n in enumerate(sizes):\n    print(f'Test {i+1} (n={n}) =')\n    expr = brackets * n\n    %timeit -r 3 -n 5000 has_balanced_brackets(expr)\n\nTest 1 (n=100) =\n21.8 µs ± 917 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 2 (n=200) =\n41.7 µs ± 166 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 3 (n=400) =\n84 µs ± 795 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 4 (n=800) =\n158 µs ± 996 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "title": "Advent of Code 2015, Day 3",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 3: Perfectly Spherical Houses in a Vacuum."
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "title": "Advent of Code 2015, Day 3",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "title": "Advent of Code 2015, Day 3",
    "section": "Functions",
    "text": "Functions\n\ndef get_next(position, direction) -> tuple:\n    \"\"\"Return the next position based on the given direction.\n    \"\"\"\n    x, y = position\n    if direction == '>':\n        return (x+1, y)\n    elif direction == '<':\n        return (x-1, y)\n    elif direction == '^':\n        return (x, y+1)\n    else:\n        return (x, y-1)\n\n\ndef deliver_presents(directions: str) -> set:\n    \"\"\"Return a set of tuples representing the positions of houses where\n    presents were delivered.\n    \"\"\"\n    houses, position = set(), (0, 0)\n    houses.add(position)\n    for direction in directions:\n        next_position = get_next(position, direction)\n        houses.add(next_position)\n        position = next_position\n    return houses"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "title": "Advent of Code 2015, Day 3",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 3)\nprint(f\"line = '{line[:5]}'\")\n\nline = '^^<<v'\n\n\n\n\nPart 1\n\nprint(f'Solution = {len(deliver_presents(line))}')\n\nSolution = 2565\n\n\n\n\nPart 2\n\nsanta_houses = deliver_presents(line[::2])\nrobot_houses = deliver_presents(line[1::2])\nprint(f'Solution = {len(santa_houses.union(robot_houses))}')\n\nSolution = 2639\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit len(deliver_presents(line))\nprint('Part 2 =')\n%timeit len(deliver_presents(line[::2]).union(deliver_presents(line[1::2])))\n\nPart 1 =\n2.09 ms ± 38 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nPart 2 =\n2.16 ms ± 12.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html",
    "title": "Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a case-control study.\nThis topics is covered by M249 Book 1, Part 1.\n\n\nData on a case-control analysing the possible association between political activity and death by homicide was sourced (Mian, A., Mahmood, S.F., et al (2022)). In total, 35 vicitims of homicide were included in the study, and 85 controls with similar age and sex distributions as the victims. Household members were questioned on the policial activities of the study subjects. Of the 35 victims, 11 had attended political meeting, compared to two of the controls.\nThe study results were as follows.\n\n\n\n\nhomicide (+)\nnot homicide (-)\n\n\n\n\nattended (+)\n11\n2\n\n\ndid not attend (-)\n24\n83\n\n\n\n\n\n\nThe analysis was undertaken using StatsModels and SciPy.\nThe data were stored remotely, so we defined a function cache_file to handle their retrieval. The datay were stored in a CSV file with schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ncasecon\nstr\nDescriptive label indicating category of case or control\n\n\n\nThe exposure and case-controls labels were stored as two lists, with variables named exposures and casecons.\nNote, the orders of the lists are important given Table2x21 requires the data to be some sequence with shape (2, 2):\n\n\n\n\ncase (+)\ncontrol (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nAs such, the labels should be ordered:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\nThe data were cached and used to a initialise data, a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the exposure, case columns as ordered Categorical data. They were ordered to ensure the data would be reshaped as expected.2\nThe DataFrame cat_data was taken as a Numpy NDArray with shape (2, 2) with name data_arr, and used to initialise an instance of Table2x2 named ctable.\nThe odds ratio was calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. We rounded-off the analysis by performing Fisher’s exact test.3 4"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#dependencies",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#dependencies",
    "title": "Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#functions",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#functions",
    "title": "Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname, and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#main",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#main",
    "title": "Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = np.array(['attended', 'did not attend'])\ncasecons = np.array(['homicide', 'not homicide'])\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/karachi.csv'),\n    fname='karachi.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   casecon   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\nOutput a view of data.\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      casecon\n    \n  \n  \n    \n      0\n      11\n      attended\n      homicide\n    \n    \n      1\n      2\n      attended\n      not homicide\n    \n    \n      2\n      24\n      did not attend\n      homicide\n    \n    \n      3\n      83\n      did not attend\n      not homicide\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns exposure, disease as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    casecon=pd.Categorical(data['casecon'], casecons, ordered=True)\n).sort_values(\n    by=['exposure', 'casecon']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   casecon   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput a pivot table with the marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='casecon',\n    aggfunc='sum',\n    margins=True,\n    margins_name='subtotal'\n).drop(\n    columns='subtotal'\n)\n\n\n\n\n\n  \n    \n      casecon\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      11\n      2\n    \n    \n      did not attend\n      24\n      83\n    \n    \n      subtotal\n      35\n      85\n    \n  \n\n\n\n\n\n\nInitialise the contingency table\nGet the data as a NumPy ndarray with shape (2, 2).\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\n\nInitialise an instance of Table2x2 using data_arr.\n\nctable = sm.stats.Table2x2(data_arr)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[11.  2.]\n [24. 83.]]\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    19.020833\nlcb       3.942873\nucb      91.758497\nName: odds ratio, dtype: float64\n\n\n\n\nChi-squared test for no association\nReturn the expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[ 3.79166667,  9.20833333],\n       [31.20833333, 75.79166667]])\n\n\nReturn the differences between the observed and expected frequencies.\n\ndata_arr - ctable.fittedvalues\n\narray([[ 7.20833333, -7.20833333],\n       [-7.20833333,  7.20833333]])\n\n\nReturn the contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[13.70375458,  5.64272247],\n       [ 1.66494215,  0.68556441]])\n\n\nReturn the result of the chi-squared test.5\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    21.696984\npvalue       0.000003\ndf                  1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.000018\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#references",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#references",
    "title": "Case-Control Studies",
    "section": "References",
    "text": "References\nMian, A., Mahmood, S.F., Chotani, H. and Luby, S., 2002. Vulnerability to homicide in Karachi: political activity as a risk factor. International journal of epidemiology, 31(3), 581-585.\n\n%load_ext watermark\n%watermark --iv\n\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nrequests   : 2.28.1\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\npandas     : 1.4.3\nscipy      : 1.9.0"
  }
]