[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaughingRook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 12, 2022\n\n\nAdvent of Code Day 3, 2018\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdvent of Code Day 1, 2018\n\n\n\n\n\n\nOct 11, 2022\n\n\nAdvent of Code Day 2, 2018\n\n\n\n\n\n\nOct 8, 2022\n\n\nCollege Tuition Fees in the USA\n\n\nTidyTuesday\n\n\n\n\nOct 8, 2022\n\n\nConjugatePriors\n\n\nM249,Bayesian,PyPackage\n\n\n\n\nOct 8, 2022\n\n\nCache a remote file\n\n\nDataHandling,Recipe\n\n\n\n\nOct 8, 2022\n\n\nTidyTuesday Problem Picker\n\n\nTidyTuesday,Recipe\n\n\n\n\n\nOct 8, 2022\n\n\nWrite a CSV file To A SQLite3 Table\n\n\nDataIO,SQLite3\n\n\n\n\nOct 7, 2022\n\n\nModelling A Single Poisson Process\n\n\nM343,PoissonProcesses\n\n\n\n\nOct 6, 2022\n\n\nContinuous Random Variables\n\n\nStatistics,M343\n\n\n\n\nOct 5, 2022\n\n\nDiscrete Random Variables\n\n\nStatistics,M343\n\n\n\n\nOct 5, 2022\n\n\nBernoulli Process\n\n\nStatistics,M343,RandomEvents\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20220830-tt_y2018_w01_us_avg_tuition.html",
    "href": "posts/20220830-tt_y2018_w01_us_avg_tuition.html",
    "title": "College Tuition Fees in the USA",
    "section": "",
    "text": "Visualising the average annual cost of College tuition fees in the USA.\n\n\nHappy to announce the newest #R4DS online learning community project! #TidyTuesday is your weekly #tidyverse practice!Each week we'll post data and a plot at https://t.co/8NaXR93uIX under the datasets link.You clean the data and tweak the plot in R!#rstats #ggplot2 pic.twitter.com/sDaHsB8uwL\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 2, 2018\n\n\nThis is the first time we’ve created a visual using Seaborn 0.12.0. I have to say it is very very nice.\n\n\n\n2022-08-30\n\nNote initialised\n\n2022-10-08\n\nNotebook has been modernised\nPipelines have been refactored to use pure Pandas\nAdded laughingrook as a dependency\nswapped polars for straight pandas\n\nthe query is rather simple so it added an unneeded layer of complexity\n\nswapped out altair for seaborn\n\nseaborn-0.12.0 has been released…."
  },
  {
    "objectID": "posts/20220830-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "href": "posts/20220830-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "title": "College Tuition Fees in the USA",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom seaborn import objects as so\nimport laughingrook as lr\n\n\n%load_ext watermark\n%watermark --iv\n\npandas      : 1.5.0\nmatplotlib  : 3.6.0\nlaughingrook: 0.1.0\nseaborn     : 0.12.0"
  },
  {
    "objectID": "posts/20220830-tt_y2018_w01_us_avg_tuition.html#main",
    "href": "posts/20220830-tt_y2018_w01_us_avg_tuition.html#main",
    "title": "College Tuition Fees in the USA",
    "section": "Main",
    "text": "Main\n\nCache and load the data\n\nlocal_path = lr.datasets.cache_url(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-02/us_avg_tuition.xlsx?raw=true'),\n    fname='us_avg_tuition.xlsx'\n)\nus_avg_tuition = pd.read_excel(local_path)\n\n\n\nPreview the data\nFindings\n\nThere is a typo in the column containing the data for 2007-2008, so it has leading white space\nThe data does not need any preprocessing\n\nDataFrame is tidy\nNo missing values\nData are floats\n\n\n\nus_avg_tuition.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50 entries, 0 to 49\nData columns (total 13 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   State       50 non-null     object \n 1   2004-05     50 non-null     float64\n 2   2005-06     50 non-null     float64\n 3   2006-07     50 non-null     float64\n 4     2007-08   50 non-null     float64\n 5   2008-09     50 non-null     float64\n 6   2009-10     50 non-null     float64\n 7   2010-11     50 non-null     float64\n 8   2011-12     50 non-null     float64\n 9   2012-13     50 non-null     float64\n 10  2013-14     50 non-null     float64\n 11  2014-15     50 non-null     float64\n 12  2015-16     50 non-null     float64\ndtypes: float64(12), object(1)\nmemory usage: 5.2+ KB\n\n\n\nus_avg_tuition.head()\n\n\n\n\n\n  \n    \n      \n      State\n      2004-05\n      2005-06\n      2006-07\n      2007-08\n      2008-09\n      2009-10\n      2010-11\n      2011-12\n      2012-13\n      2013-14\n      2014-15\n      2015-16\n    \n  \n  \n    \n      0\n      Alabama\n      5682.838120\n      5840.549785\n      5753.496432\n      6008.168872\n      6475.091706\n      7188.954303\n      8071.133759\n      8451.902223\n      9098.069156\n      9358.928842\n      9496.084106\n      9751.101349\n    \n    \n      1\n      Alaska\n      4328.281362\n      4632.623449\n      4918.500619\n      5069.822132\n      5075.482406\n      5454.606610\n      5759.152951\n      5762.420526\n      6026.142669\n      6012.444735\n      6148.808010\n      6571.340317\n    \n    \n      2\n      Arizona\n      5138.495312\n      5415.516049\n      5481.419145\n      5681.637955\n      6058.463821\n      7263.204332\n      8839.604653\n      9966.716345\n      10133.503178\n      10296.199709\n      10413.843882\n      10646.278352\n    \n    \n      3\n      Arkansas\n      5772.301869\n      6082.379324\n      6231.977179\n      6414.900365\n      6416.503410\n      6627.092143\n      6900.912413\n      7028.991050\n      7286.580461\n      7408.495063\n      7606.410008\n      7867.296736\n    \n    \n      4\n      California\n      5285.921489\n      5527.881290\n      5334.825779\n      5672.472175\n      5897.888491\n      7258.771494\n      8193.738802\n      9436.425766\n      9360.573556\n      9274.193266\n      9186.824330\n      9269.844227\n    \n  \n\n\n\n\n\n\nPlot the data\n\n(so.Plot(\n    data=us_avg_tuition.sort_values(by=['2015-16']),\n    x='2015-16',\n    y='State'\n )\n .add(so.Dot())\n .layout(size=(6, 10))\n .label(\n    x='Fee',\n    y='',\n    title='College tuition costs in the USA in (2015-16)'\n )\n)  # noqa"
  },
  {
    "objectID": "posts/20221005-m343_b1i_discrete_rv.html",
    "href": "posts/20221005-m343_b1i_discrete_rv.html",
    "title": "Discrete Random Variables",
    "section": "",
    "text": "What problem is addressed in the notebook?\nHow we can use scipy and laughingrook to solve problems involving discrete random variables.\nIs the problem solved in a single notebook?\n(If no, then where are the other notebooks?)\nYes.\nDid you do anything novel in the notebook?\n(If yes, then what was the novel thing and should it be note for further use?)\nYes, we defined simulate_n_dice, a function that simulates the score of rolling n dice. It returns the score and the probability of rolling that score.\nIt probably does not need to be documented further, but it may be quite useful to simulate non-trivial distributions for testing.\nIs there anything you have done for the first time?\n(If yes, then what do you show for the first time?)\nWe initialised an instance of scipy’s rv_discete by passing an argument for values. The benefits of using an instance of rv_discrete to represent a discrete random variable is that we gain access to a suite of useful statistical methods, so we do not have define them ourselves.\nWe also show off the use of laughingrook’s graphics and rvhelpers modules, which contain convenient helper methods that can be used when working with scipy’s random variables.\nReferences\n\nM343 Book 1, Probability and random variables\nSciPy, scipy.stats.rv_discrete\nLaughingRook"
  },
  {
    "objectID": "posts/20221005-m343_b1i_discrete_rv.html#dependencies",
    "href": "posts/20221005-m343_b1i_discrete_rv.html#dependencies",
    "title": "Discrete Random Variables",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport itertools as it                     # used to get the cross product\nfrom collections import Counter            # Count occurrences of a score\nimport numpy as np                         # data structure\nimport pandas as pd                        # data structure\nfrom scipy import stats as st              # data anslysis\nfrom matplotlib import pyplot as plt       # plotting package\nimport seaborn as sns                      # advanced statistical visuals\nfrom laughingrook import graphics as gfx   # convenience plotting functions\nfrom laughingrook import rvhelpers as rvh  # convenience functions, scipy rv\n\n\n%load_ext watermark\n%watermark --iv\n\nmatplotlib  : 3.6.0\nnumpy       : 1.23.3\nlaughingrook: 0.1.0\nseaborn     : 0.12.0\nscipy       : 1.9.1\npandas      : 1.5.0"
  },
  {
    "objectID": "posts/20221005-m343_b1i_discrete_rv.html#functions",
    "href": "posts/20221005-m343_b1i_discrete_rv.html#functions",
    "title": "Discrete Random Variables",
    "section": "Functions",
    "text": "Functions\n\ndef simulate_n_dice_rolls(n: int) -> tuple[list, list]:\n    \"\"\"Return the event space and Pr(event) from the combined scores of\n    rolling two dice.\n\n    Preconditions\n    - n >= 1\n\n    Returns\n    - xs, the event space\n    - px, the P(X=x), x in xs\n    \"\"\"\n    dice = [np.arange(1, 7, dtype=int) for _ in range(n)]\n    candidates = [prod for prod in it.product(*dice)]\n    events_counter = Counter([sum(event) for event in [*candidates]])\n    n = events_counter.total()\n    return (\n        [x for x in events_counter],\n        [k/n for k in events_counter.values()]\n    )"
  },
  {
    "objectID": "posts/20221005-m343_b1i_discrete_rv.html#main",
    "href": "posts/20221005-m343_b1i_discrete_rv.html#main",
    "title": "Discrete Random Variables",
    "section": "Main",
    "text": "Main\n\n%precision 3\nsns.set_theme()\n\n\nInitialise the random variable\nWe are modelling the possible scores from rolling two die.\n\nxs, pr = simulate_n_dice_rolls(2)\nrv_scores = st.rv_discrete(values=(xs, pr), name='score')\n\n\n\nPlot the random variable\n\ngfx.std_stemplot(xs, rv_scores.pmf(xs), rv_scores.name, 'X', 'p(x)')\n\n\n\n\n\ngfx.std_stemplot(xs, rv_scores.cdf(xs), rv_scores.name, 'X', 'F(x)')\n\n\n\n\n\n\nCalculating probabilities\n\nrvh.probability_table(rv_scores).T\n\n\n\n\n\n  \n    \n      \n      pmf\n      cdf\n    \n    \n      score\n      \n      \n    \n  \n  \n    \n      2\n      0.027778\n      0.027778\n    \n    \n      3\n      0.055556\n      0.083333\n    \n    \n      4\n      0.083333\n      0.166667\n    \n    \n      5\n      0.111111\n      0.277778\n    \n    \n      6\n      0.138889\n      0.416667\n    \n    \n      7\n      0.166667\n      0.583333\n    \n    \n      8\n      0.138889\n      0.722222\n    \n    \n      9\n      0.111111\n      0.833333\n    \n    \n      10\n      0.083333\n      0.916667\n    \n    \n      11\n      0.055556\n      0.972222\n    \n    \n      12\n      0.027778\n      1.000000\n    \n  \n\n\n\n\n\nrv_scores.pmf(6)  # P(6)\n\n0.139\n\n\n\nrv_scores.cdf(6)  # F(6)\n\n0.417\n\n\n\nrv_scores.sf(6)   # P(X>6) == 1 - F(6)\n\n0.583\n\n\n\n\nCalculating quantiles\n\nrv_scores.ppf(0.5)       # median, could use rv.median()\n\n7.000\n\n\n\nrv_scores.interval(0.5)  # lower, upper quartiles\n\n(5.000, 9.000)\n\n\n\nrvh.iqr(rv_scores)       # interquartile range\n\n4.000\n\n\n\n\nExpectation and variance\n\nrvh.describe(rv_scores)\n\nmean         7.0\nvar     5.833333\nmin            2\nlq           5.0\nmed          7.0\nuq           9.0\nmax           12\nName: score, dtype: object\n\n\n\nrv_scores.mean()  # E(X)\n\n7.000\n\n\n\nrv_scores.var()   # V(X)\n\n5.833\n\n\n\n\nSampling the distribution\n\nrv_scores.rvs(size=9)  # sample the distribution\n\narray([ 4, 11,  6,  8,  9, 10,  7, 11,  3])\n\n\n\n# plot sampled values\nsns.countplot(x=rv_scores.rvs(size=300))\nplt.xlabel('score')\nplt.show()"
  },
  {
    "objectID": "posts/20221006-m343_b1ii_continuous_rv.html",
    "href": "posts/20221006-m343_b1ii_continuous_rv.html",
    "title": "Continuous Random Variables",
    "section": "",
    "text": "What problem is addressed in the notebook?\nHow we can use scipy and laughingrook to solve problems involving continuous random variables.\nIs the problem solved in a single notebook?\n(If no, then where are the other notebooks?)\nYes.\nDid you do anything novel in the notebook?\n(If yes, then what was the novel thing and should it be note for further use?)\nIs there anything you have done for the first time?\n(If yes, then what do you show for the first time?)\nWe defined a new class oil_change_gen to represent a continuous random variable. (It inherits from rv_continuous.) The benefits of inheriting from rv_continuous is that we gain access to a suite of useful statistical methods, so we do not have define them ourselves.\nWe also show off the use of laughingrook’s graphics and rvhelpers modules, which contain contain convenient helper methods that can be used when working with scipy’s random variables.\nReferences\n\nM343 Book 1, Probability and random variables\nSciPy, scipy.stats.rv_continuous\nLaughingRook"
  },
  {
    "objectID": "posts/20221006-m343_b1ii_continuous_rv.html#dependencies",
    "href": "posts/20221006-m343_b1ii_continuous_rv.html#dependencies",
    "title": "Continuous Random Variables",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd                        # data structure\nfrom scipy import stats as st              # data anslysis\nfrom matplotlib import pyplot as plt       # plotting package\nimport seaborn as sns                      # advanced statistical visuals\nfrom laughingrook import graphics as gfx   # convenience plotting functions\nfrom laughingrook import rvhelpers as rvh  # convenience functions, scipy rv\n\n\n%load_ext watermark\n%watermark --iv\n\nscipy       : 1.9.1\npandas      : 1.5.0\nseaborn     : 0.12.0\nmatplotlib  : 3.6.0\nlaughingrook: 0.1.0"
  },
  {
    "objectID": "posts/20221006-m343_b1ii_continuous_rv.html#classes",
    "href": "posts/20221006-m343_b1ii_continuous_rv.html#classes",
    "title": "Continuous Random Variables",
    "section": "Classes",
    "text": "Classes\n\n\n\n\n\n\nWarning\n\n\n\nWe override the _pdf when defining oild_change_gen, but M343 commonly gives the CDF.\n\n\n\nclass oil_change_gen(st.rv_continuous):\n    def _pdf(self, x):\n        return x / 48"
  },
  {
    "objectID": "posts/20221006-m343_b1ii_continuous_rv.html#main",
    "href": "posts/20221006-m343_b1ii_continuous_rv.html#main",
    "title": "Continuous Random Variables",
    "section": "Main",
    "text": "Main\n\n%precision 3\nsns.set_theme()\n\n\nInitialise the random variable\nWe are modelling the time it takes to perform an oil change in a car.\n\n\n\n\n\n\nWarning\n\n\n\nArguments a, b refer to the min, max values of X. If either are (np.inf, -np.inf), then do not pass arguments for it.\n\n\n\n\n\n\n\n\nTip\n\n\n\nArgument name is optional. (Default is Distribution.) It is called a few times by functions in laughingrook.rvhelpers.\n\n\n\nrv_oil_change = oil_change_gen(a=2, b=10, name='oil change')\nxs = rvh.continuous_xs(rv_oil_change)  # event space\n\n\n\nDescribe the random variable\n\nrvh.describe(rv_oil_change)\n\nmean    6.888889\nvar      4.54321\nmin            2\nlq      5.291503\nmed     7.211103\nuq      8.717798\nmax           10\nName: oil change, dtype: object\n\n\n\n\nPlot the random variable\n\ngfx.std_lineplot(xs, rv_oil_change.pdf(xs), rv_oil_change.name, 'X', 'f(x)')\n\n\n\n\n\ngfx.std_lineplot(xs, rv_oil_change.cdf(xs), rv_oil_change.name, 'X', 'F(x)')\n\n\n\n\n\n\nCalculating probabilities\n\nrv_oil_change.pdf(6)  # P(6)\n\n0.125\n\n\n\nrv_oil_change.cdf(6)  # F(6)\n\n0.333\n\n\n\nrv_oil_change.sf(6)   # P(X>6) == 1 - F(6)\n\n0.667\n\n\n\n\nCalculating quantiles\n\nrv_oil_change.ppf(0.5)       # could use rv.median()\n\n7.211\n\n\n\nrv_oil_change.interval(0.8)  # lower, upper deciles\n\n(3.688, 9.508)\n\n\n\nrvh.iqr(rv_oil_change)       # interquartile range\n\n3.426\n\n\n\n\nExpectation and variance\n\nrvh.describe(rv_oil_change)\n\nmean    6.888889\nvar      4.54321\nmin            2\nlq      5.291503\nmed     7.211103\nuq      8.717798\nmax           10\nName: oil change, dtype: object\n\n\n\nrv_oil_change.mean()  # E(X)\n\n6.889\n\n\n\nrv_oil_change.var()   # V(X)\n\n4.543\n\n\n\n\nSampling the distribution\n\nrv_oil_change.rvs(size=9)  # sample the distribution\n\narray([7.087, 5.509, 2.588, 7.958, 4.206, 5.092, 9.71 , 9.657, 8.248])\n\n\n\n# plot histogram of sampled values\nsns.histplot(data=rv_oil_change.rvs(size=300))\nplt.xlabel('time')\nplt.show()"
  },
  {
    "objectID": "posts/20221007-conjugatepriors.html",
    "href": "posts/20221007-conjugatepriors.html",
    "title": "ConjugatePriors",
    "section": "",
    "text": "Date created: 2022-10-04\nWhat problem is addressed in the notebook?\nThe aim of this notebook is to show how we can use the Python model conjugathepriors to perform a conjugate analysis.\nThe intention is is that this notebook will become the documentation for conjugatepriors.\nIs the problem solved in a single notebook?\n\nYes. I did originally spec these for a series of notebooks, but I decided against it. All the Prior classes in conjugatepriors share the same interface, and so, whilst they did cover every last part the module, the notebooks were essentially duplicates of each other that added little. Besides that, I did not plan for the notebooks to be theoretical or act as tutorials for conjugate analyses, so having full coverage for each Prior subclass was unneeded, as a workflow should work for each of the classes.\nDid you do anything novel in the notebook?\n\nOnly in the sense this is the first time I showed off the intended end-to-end use of the Prior class.\nIs there anything you have done for the first time?\n\nI did not not want to overburden the module with plotting functions, so I defined some recipes to help with plotting.\n\n\n\n\n\n\nNote\n\n\n\nI still need to document the functions and added them to my cookbook.\n\n\nReferences\n\nConjugatePriors\nM249 Book 4, Bayesian Statistics\nSciPy, scipy.stats.rv_discrete\n\n\n\n\n\nWrite the API\nPrior\nBeta\nGamma\nNormal\ncredible_interval(prior)\ndescribe_model(prior)\nplot_models(*priors)\nsample_fit(prior, *args)\n\nCookbook\nFrom prior beliefs\nFrom past observations\nUsing a Uniform prior\nSampling a fitted model\n\n\n\n\n\n\n2022-10-08\n\nAdded an example for using a Uniform prior\n\n2022-10-07\n\nAdded the API for the Beta and Gamma priors\nAdded an example for starting from prior observation\n\n2022-10-04\n\nNote initialised\nWrote the API for the Prior\nAdded an example for starting with prior beliefs"
  },
  {
    "objectID": "posts/20221007-conjugatepriors.html#api",
    "href": "posts/20221007-conjugatepriors.html#api",
    "title": "ConjugatePriors",
    "section": "API",
    "text": "API\n\nPrior\nA stub class that defines an interface for the data model. All other classes inherit for Prior. The intention is for Prior to be a term to represent its collection of subclasses.\n\nPrior()\n\nInitialise and return an instance of Prior.\n\nProperties\n\nmodel: str.rv_contrinuous (read-only)\n\nThe likelihood distribution of \\(\\theta\\), the modelled parameter.\n\nClass methods\n\nfrom_belief(mode: float, lower: float, upper: float, spread: float) -> Prior\n\nReturn an initialised prior based on some belief about the location of \\(\\theta\\).\nArguments\n\nmode, the likely value of the \\(\\theta\\)\nlower, lower value of the equal-tailed 100(1-\\(\\alpha\\))% interval of \\(\\theta\\)\nlower, upper value of the equal-tailed 100(1-\\(\\alpha\\))% interval of \\(\\theta\\)\nalpha, alpha value in the 100(1-\\(\\alpha\\))% interval\n\nPreconditions\n\nmode, lower, upper are valid values of \\(\\theta\\)\nlower < mode < upper\n0 < alpha < 1\n\n\nfrom_obs(arr: ArrayLike) -> Prior\n\nReturn an initialised prior based on some prior observations of the fitted model.\nPreconditions\n\nlen(arr) >= 1\nEvery element of arr are integer or float values\n\n\n\n\nMethods\n\nfit() -> st.rv_continuous | st.rv_discrete\n\nReturn the paired fitted distribution, based on the most likely location of \\(\\theta\\).\n\nis_uniform() -> bool\n\nReturn true if self is an improper prior. Otherwise false.\nAn improper prior is akin Uniform distribution with one of both of the values being equal to \\(\\pm \\infty\\).\n\nto_posterior(arr: ArrayLike) -> Prior\n\nReturn a new instance of the prior, with its model of the likelihood distribution of \\(\\theta\\) updated to take into account the given array of observations.\nPreconditions\n\nlen(arr) >= 1\nEvery element of arr are integer or float values\n\n\n\n\nBeta(Prior)\nA class to represent the use of a beta conjugate prior.\nIt’s model is the likelihood distribution of \\(\\theta\\), the probability of success in a single Bernoulli trial, where \\(\\theta \\sim \\text{beta}(a, b)\\).\nIt fits a binomial random variable \\(b\\), where \\(b \\sim B(n, \\theta)\\).\n\nInitialisation\n\nBeta(a: float = 1.0, b: float = 1.0)\n\nInitialise and return an instance of Beta.\nPassing no arguments will return a uniform likelihood distribution for \\(\\theta : \\theta \\sim U(0, 1)\\).\n\n\nClass methods\n\nfrom_obs(arr: ArrayLike) -> Prior\n\nReturn an initialised instance of Beta based on some prior observations of the fitted model.\nPreconditions\n\nlen(arr) >= 1\narr[i] == 0 or arr[i] == 1, where 0 represent a failures and 1 represents a failure, for 0 <= i <= len(arr)\n\n\n\nMethods\n\nfit(n: int) -> st.rv_discrete\n\nReturn the paired fitted distribution, based on the most likely location of \\(\\theta\\).\nGiven a beta prior models p in a binomial distribution, n represents the number of observations. Thus the fitted model is \\(b \\sim B(n, \\theta)\\)\n\nto_posterior(arr: ArrayLike) -> Prior\n\nReturn a new instance of the Beta, with its likelihood distribution of \\(\\theta\\) updated to take into account the given array of observations.\nPreconditions\n\nlen(arr) >= 1\narr[i] == 0 or arr[i] == 1, where 0 represent a failures and 1 represents a failure, for 0 <= i <= len(arr)\n\n\n\n\nGamma(Prior)\nA class to represent the use of a gamma conjugate prior.\nIt’s model is the likelihood distribution of \\(\\theta\\), some parameter which may only take non-negate values.\nIt fits a Poisson random variable \\(p\\), where \\(p \\sim \\text{Poisson}(\\theta)\\).\n\nInitialisation\n\nGamma(a: float = 0.0, b: float = 0.0)\n\nInitialise and return an instance of Gamma.\nPassing no arguments will return a uniform likelihood distribution for \\(\\theta : \\theta \\sim U(0, \\infty)\\)."
  },
  {
    "objectID": "posts/20221007-conjugatepriors.html#cookbook",
    "href": "posts/20221007-conjugatepriors.html#cookbook",
    "title": "ConjugatePriors",
    "section": "Cookbook",
    "text": "Cookbook\n\nDependencies\n\nimport conjugatepriors as cp\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nimport seaborn as sns\n\n\nnp.set_printoptions(precision=6)\nsns.set_theme()\n\n\n%load_ext watermark\n%watermark --iv\n\nnumpy          : 1.23.3\nconjugatepriors: 0.2.0\npandas         : 1.5.0\nscipy          : 1.9.1\nseaborn        : 0.12.0\n\n\n\n\n\nFunctions\n\n# get the end points of rv, or approximate them\ndef get_minmax(rv):\n    # get the end points\n    a, b = rv.a, rv.b\n    # are either of the end-points undefined?\n    if a == -np.inf:\n        a = rv.ppf(0.001)\n    if b == np.inf:\n        b = rv.ppf(0.999)\n    return a, b\n\n\n# return rv's PMF as a pandas DataFrame\ndef pmf(rv, label, step=1):\n    a, b = get_minmax(rv)\n    # return the DataFrame\n    xs = np.arange(a, b + 1, step=step, dtype=int)\n    return (\n        pd.DataFrame()\n        .assign(\n            model=[label] * len(xs),\n            variable=xs,\n            value=rv.pmf(xs)\n        )\n    )\n\n\n\n1. Using a beta prior from a prior belief\n\nA University tutor believe the pass rate of their course is approximately 75%, but could be between 70-80%.\n\n\nInitialise the prior likelihood.\n\nbeta_prior = cp.Beta.from_belief(0.75, 0.70, 0.80)\n\n\n\nInspect the prior likelihood\n\n\n\n\n\n\nTip\n\n\n\nPrinting the instance will return a description of the beta likelihood distribution.\n\n\n\nprint(beta_prior)\n\nBeta(25.51, 9.17)\n\n\nGet the 99% credible interval.\n\ncp.credible_interval(beta_prior, alpha=0.01)\n\narray([0.525597, 0.896076])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe describe_model() functions returns a pandas Series.\n\n\nOutput a description of the likelihood distribution.\n\ncp.describe_model(beta_prior)\n\nmedian    0.740154\nvar       0.005450\nlcb       0.579364\nucb       0.866115\ndtype: float64\n\n\nPlot the prior likelihood of \\(\\theta\\).\n\ncp.plot_models(beta_prior)\n\n<seaborn.axisgrid.FacetGrid at 0x1a7de8eff40>\n\n\n\n\n\n\n\nFrom prior to posterior….\n\nA year has passed, and the tutor observes that 14 out of the 25 students pass the course at first attempt.\n\nUpdate the prior with the findings.\n\n\n\n\n\n\nTip\n\n\n\nIf you have the summarised observations, simply convert them to a list of 01s.\n\n\n\nbeta_post = beta_prior.to_posterior([0] * 14 + [1] * 11)\n\n\nprint(beta_post)\n\nBeta(36.51, 23.17)\n\n\nHow has the tutor’s belief changed?\nHow has the tutor’s beliefs about the pass rate of the course changed with the observations?\nCompare descriptions of the prior and posterior.\n\n(pd.DataFrame()\n .assign(\n     prior=cp.describe_model(beta_prior),\n     posterior=cp.describe_model(beta_post)\n )\n)\n\n\n\n\n\n  \n    \n      \n      prior\n      posterior\n    \n  \n  \n    \n      median\n      0.740154\n      0.613031\n    \n    \n      var\n      0.005450\n      0.003914\n    \n    \n      lcb\n      0.579364\n      0.486094\n    \n    \n      ucb\n      0.866115\n      0.730355\n    \n  \n\n\n\n\nCompare the prior and posterior likelihood distributions graphically.\n\ncp.plot_models(beta_prior, beta_post)\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e0ab0550>\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecipe\n\n\nHow do the fitted distributions compare?\n\npriors = [beta_prior, beta_post]\nlabels = ['prior', 'posterior']\n\ngs = pd.concat(\n    [pmf(prior.fit(20), label) for prior, label in zip(priors, labels)]\n)\nsns.catplot(data=gs, x='variable', y='value', hue='model', kind='point')\n\n<seaborn.axisgrid.FacetGrid at 0x1a79d5a6290>\n\n\n\n\n\n\n\n\n2. Using a gamma prior from prior observations\n\nA person is interested in the number of birds that fly by their window each hour. They keep track of the number for six hours.\n\n\nInitialise the prior likelihood\n\ngamma_prior = cp.Gamma.from_obs([5, 7, 3, 2, 4, 4])\n\n\n\nInspect the prior likelihood\n\n\n\n\n\n\nTip\n\n\n\nPrinting the instance will return a description of the beta likelihood distribution.\n\n\n\nprint(gamma_prior)\n\nGamma(25.00, 6.00)\n\n\nGet the 99% credible interval.\n\ncp.credible_interval(gamma_prior, alpha=0.01)\n\narray([2.332562, 6.624165])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe describe_model() functions returns a pandas Series.\n\n\nOutput a description of the likelihood distribution.\n\ncp.describe_model(gamma_prior)\n\nmedian    4.111245\nvar       0.694444\nlcb       2.696447\nucb       5.951683\ndtype: float64\n\n\nPlot the prior likelihood of \\(\\theta\\).\n\ngamma_prior.model.b == np.inf\n\nTrue\n\n\n\ncp.plot_models(gamma_prior)\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e0ec34f0>\n\n\n\n\n\n\n\nFrom prior to posterior….\n\nThe person counts the birds the next day for the same six hours. They update their prior with the new observations.\n\n\ngamma_post = gamma_prior.to_posterior([1, 2, 2, 1, 2, 3])\n\n\nprint(gamma_post)\n\nGamma(36.00, 12.00)\n\n\nHow has the tutor’s belief changed?\nHow has the tutor’s beliefs about the pass rate of the course changed with the observations?\nCompare descriptions of the prior and posterior.\n\n(pd.DataFrame()\n .assign(\n     prior=cp.describe_model(gamma_prior),\n     posterior=cp.describe_model(gamma_post)\n )\n)\n\n\n\n\n\n  \n    \n      \n      prior\n      posterior\n    \n  \n  \n    \n      median\n      4.111245\n      2.972268\n    \n    \n      var\n      0.694444\n      0.250000\n    \n    \n      lcb\n      2.696447\n      2.101163\n    \n    \n      ucb\n      5.951683\n      4.056377\n    \n  \n\n\n\n\nCompare the prior and posterior likelihood distributions graphically.\n\ncp.plot_models(gamma_prior, gamma_post)\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e11098a0>\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecipe\n\n\nHow do the fitted distributions compare?\n\npriors = [gamma_prior, gamma_post]\nlabels = ['prior', 'posterior']\n\ngs = pd.concat(\n    [pmf(prior.fit(), label) for prior, label in zip(priors, labels)]\n)\nsns.catplot(data=gs, x='variable', y='value', hue='model', kind='point')\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e11c34f0>\n\n\n\n\n\n\n\n\n3. Using a uniform prior\n\nAn amateur myrmecologist has been asked to provide an estimate of the expected size (in cm) of ant hills in the local area. They are told that other studies have shown that the variance is 10 (sq. cm).\n\n\nInitialise the prior likelihood\n\nnormal_prior = cp.Normal(10)\n\n\n\nInspect the prior likelihood\nIs it a uniform prior?\n\nnormal_prior.is_uniform()\n\nTrue\n\n\n\n\nFrom prior to posterior….\n\nThe myrmecologist travels to the local park and measures the hights of 12 ant hills….\n\n\n\n\n\n\n\nNote\n\n\n\nWe cheat a bit here and sample a normal rv.\n\n\n\nnormal_post = normal_prior.to_posterior(st.norm.rvs(40, 64, size=12))\n\n\nnormal_post.is_uniform()\n\nFalse\n\n\n\nprint(normal_post)\n\nNormal(22.43, 12.00)\n\n\nHow has the myrmecologist’s belief changed?\n\ncp.describe_model(normal_post)\n\nmedian    22.432831\nvar       12.000000\nlcb       15.643316\nucb       29.222345\ndtype: float64\n\n\nPlot the likelihood distribution for the expected height.\n\ncp.plot_models(normal_post)\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e11c2f50>\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecipe\n\n\nHow would this look if we were to fit it to 15 ant hills?\n\nfitted = normal_post.fit()\nx = np.linspace(fitted.ppf(0.001), fitted.ppf(0.999), 100)\nsns.relplot(x=x, y=fitted.pdf(x), kind='line')\n\n<seaborn.axisgrid.FacetGrid at 0x1a7e138a440>"
  },
  {
    "objectID": "posts/20221007-m343_b2p1ii_poiss_proc.html",
    "href": "posts/20221007-m343_b2p1ii_poiss_proc.html",
    "title": "Modelling A Single Poisson Process",
    "section": "",
    "text": "Model a Poisson Process using PoissonProcess, a user-defined class."
  },
  {
    "objectID": "posts/20221007-m343_b2p1ii_poiss_proc.html#dependencies",
    "href": "posts/20221007-m343_b2p1ii_poiss_proc.html#dependencies",
    "title": "Modelling A Single Poisson Process",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport poissonprocesses as pp\nfrom laughingrook import rvhelpers as rvh"
  },
  {
    "objectID": "posts/20221007-m343_b2p1ii_poiss_proc.html#main",
    "href": "posts/20221007-m343_b2p1ii_poiss_proc.html#main",
    "title": "Modelling A Single Poisson Process",
    "section": "Main",
    "text": "Main\n\n%precision 4\nsns.set_theme()\n\n\nInitialise the object\n\nIn an investigation into computer reliability, a particular unit failed on average every 652 seconds. Assuming that the incidence of failures may be adequately modelled by a Poisson process, (simulate a series of computer failures over ten minutes.)\n\nInitialise comp_failures, PoissonProcess, a Poisson process model for the occurrence of computer failures that occur at random over time.\n\n\n\n\n\n\nNote\n\n\n\nWe set the base rate to 1 hour.\n\n\n\ncomp_failures = pp.Events(1/652 * 3600)  # per hour\n\n\n\nDescribe the model\n\n# Print the models\nprint(comp_failures)\n\nN(t) ~ Poisson(5.521), T ~ M(5.521)\n\n\n\n# describe N(t)\nrvh.describe(comp_failures.n_time())\n\nmean    5.521472\nvar     5.521472\nmin            0\nlq           4.0\nmed          5.0\nuq           7.0\nmax          inf\nName: description, dtype: object\n\n\n\n# describe T\nrvh.describe(comp_failures.waiting_time())\n\nmean    0.181111\nvar     0.032801\nmin          0.0\nlq      0.052102\nmed     0.125537\nuq      0.251073\nmax          inf\nName: description, dtype: object\n\n\n\n\nPlot the models\n\n(comp_failures\n .plot_n_time()\n)\n\n\n\n\nPlot the modelled waiting time between successive computer failures,\n\ncomp_failures.plot_waiting_time()\n\n\n\n\n\n\nUse the model\n\nNumber of events\n\n# P(N(1) = 4)\n(comp_failures\n .n_time()\n .pmf(4)\n)\n\n0.1549\n\n\n\n# Expected number of events during 1 work dat\n(comp_failures\n .rescale(8)  # rescaled to 8 hours == 1 work day\n .n_time()\n .mean()\n)\n\n44.1718\n\n\n\n# Describe 1 work days\nrvh.describe(\n        comp_failures\n        .rescale(8)\n        .n_time()\n)\n\nmean    44.171779\nvar     44.171779\nmin             0\nlq           40.0\nmed          44.0\nuq           49.0\nmax           inf\nName: description, dtype: object\n\n\n\n\nTime to next event\n\n# Probability less than 10 minutes to the next failure\n(comp_failures\n .waiting_time()\n .cdf(1/6)\n)\n\n0.6016\n\n\n\n# Probability greater than 15 minutes to the next failure\n(comp_failures\n .waiting_time()\n .sf(1/4)\n)\n\n0.2515\n\n\n\n# describe the waiting time values\nrvh.describe(comp_failures.waiting_time())\n\nmean    0.181111\nvar     0.032801\nmin          0.0\nlq      0.052102\nmed     0.125537\nuq      0.251073\nmax          inf\nName: description, dtype: object\n\n\n\n\n\nSimulation\n\n\n\n\n\n\nNote\n\n\n\nWe don’t plan to add a simulation functionality to PoissonPlot. This script could be modifed for other models.\n\n\nSimulate a single work day, taken to be 8 hours.\nWhat is the 0.99-quantile of the number of events on a single work day, according to the model?\n\n(comp_failures\n .rescale(8)  # rescaled to 8 hours == 1 work day\n .n_time()\n .ppf(0.99)\n)\n\n60.0000\n\n\nGiven the 0.99-quantile of the number of events during a work day is approximately 60, let’s simulate 100 values and filter the results.\n\nstart_dt = pd.Timestamp('today').normalize() + pd.Timedelta('8 hours')\n(pd.DataFrame()\n .assign(value=comp_failures.waiting_time().rvs(100))\n .assign(cumsum=lambda df: df['value'].cumsum())\n .assign(timestamp=lambda df: pd.to_timedelta(df['cumsum'], unit='hours'))\n .query('cumsum <= 8')  # discard values that are 8 hours after the start\n .get(['timestamp'])    # discard the other columns\n .apply(lambda ser: start_dt + ser)  # adjust the timestamp\n).info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 44 entries, 0 to 43\nData columns (total 1 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   timestamp  44 non-null     datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 704.0 bytes"
  },
  {
    "objectID": "posts/20221007-m343_b2p1i_bernoulli_proc.html",
    "href": "posts/20221007-m343_b2p1i_bernoulli_proc.html",
    "title": "Bernoulli Process",
    "section": "",
    "text": "What problem is addressed in the notebook?\nIs the problem solved in a single notebook?\n(If no, then where are the other notebooks?)\nDid you do anything novel in the notebook?\n(If yes, then what was the novel thing and should it be note for further use?)\nIs there anything you have done for the first time?\n\nReferences\n\nM343 Book 1, Probability and random variables\nSciPy, scipy.stats.rv_discrete\nLaughingRook"
  },
  {
    "objectID": "posts/20221007-m343_b2p1i_bernoulli_proc.html#dependencies",
    "href": "posts/20221007-m343_b2p1i_bernoulli_proc.html#dependencies",
    "title": "Bernoulli Process",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np                         # data structure\nimport pandas as pd                        # data structure\nfrom scipy import stats as st              # data anslysis\nfrom matplotlib import pyplot as plt       # plotting package\nimport seaborn as sns                      # advanced statistical visuals\nfrom laughingrook import graphics as gfx   # convenience plotting functions\nfrom laughingrook import rvhelpers as rvh  # convenience functions, scipy rv\n\n\n%load_ext watermark\n%watermark --iv\n\nlaughingrook: 0.1.0\nscipy       : 1.9.1\nnumpy       : 1.23.3\nseaborn     : 0.12.0\nmatplotlib  : 3.6.0\npandas      : 1.5.0"
  },
  {
    "objectID": "posts/20221007-m343_b2p1i_bernoulli_proc.html#main",
    "href": "posts/20221007-m343_b2p1i_bernoulli_proc.html#main",
    "title": "Bernoulli Process",
    "section": "Main",
    "text": "Main\n\n%precision 3\nsns.set_theme()\n\n\nInitialise the model\n\nx = st.bernoulli(0.4)\nw = st.geom(0.4)\n\n\n\nDescribe the model\n\nrvh.describe(x)\n\nAttributeError: 'rv_discrete_frozen' object has no attribute 'name'"
  },
  {
    "objectID": "posts/20221008-cache_file.html",
    "href": "posts/20221008-cache_file.html",
    "title": "Cache a remote file",
    "section": "",
    "text": "Download a file to a cache folder and return the cached path."
  },
  {
    "objectID": "posts/20221008-cache_file.html#dependencies",
    "href": "posts/20221008-cache_file.html#dependencies",
    "title": "Cache a remote file",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\n\n\n%load_ext watermark\n%watermark --iv\n\nrequests: 2.28.1"
  },
  {
    "objectID": "posts/20221008-cache_file.html#function",
    "href": "posts/20221008-cache_file.html#function",
    "title": "Cache a remote file",
    "section": "Function",
    "text": "Function\n\ndef cache_file(url: str, fname, dir_: str = '__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    If fname not in dir_, then the file is downloaded.\n    Otherwise, the file at url is downloaded to dir_ with file name\n    dir_.\n\n    Preconditions:\n    - url is a valid url\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/20221008-cache_file.html#example",
    "href": "posts/20221008-cache_file.html#example",
    "title": "Cache a remote file",
    "section": "Example",
    "text": "Example\nWe use an example from the rfordatascience/tidytuesday GitHub library.\n\nurl = ('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master'\n       + '/data/2021/2021-09-28/combo_df.csv')\ncache_file(url, url.split('/')[-1])\n\n'__cache/combo_df.csv'"
  },
  {
    "objectID": "posts/20221008-tt_problempicker.html",
    "href": "posts/20221008-tt_problempicker.html",
    "title": "TidyTuesday Problem Picker",
    "section": "",
    "text": "Choose a random a problem for the TidyTuesday back catalogue, along with random data and graph libraries for a bit of spice."
  },
  {
    "objectID": "posts/20221008-tt_problempicker.html#dependencies",
    "href": "posts/20221008-tt_problempicker.html#dependencies",
    "title": "TidyTuesday Problem Picker",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom random import choice\nimport pandas as pd\n\n\n%load_ext watermark\n%watermark --iv\n\npandas: 1.5.0"
  },
  {
    "objectID": "posts/20221008-tt_problempicker.html#function",
    "href": "posts/20221008-tt_problempicker.html#function",
    "title": "TidyTuesday Problem Picker",
    "section": "Function",
    "text": "Function\n\ndef select_project() -> pd.Series:\n    def find_prev_tuesday(dt):\n        date = dt.copy()\n        while date.weekday != 2:\n            date = date - pd.Timedelta(days=1)\n        return date.date\n\n    def get_url(date):\n        url_str = ( 'https://github.com/rfordatascience/tidytuesday/blob/master'\n                   + f'/data/{date.year}')\n        print(url_str)\n        return url_str\n\n    return (\n        pd.DataFrame(\n                index=pd.date_range(\n                            start='2018-04-02',\n                            end=pd.to_datetime('today').normalize(),\n                            freq='W'\n                )\n        ).sample(1)\n        .assign(\n            #weekno=lambda df: [x + 1 for x in range(df.shape[0])],\n            date=lambda df: find_prev_tuesday(df.index),\n            url=lambda df: df.date.map(get_url),\n            datalib=choice(['pandas',\n                            'polars',\n                            'duckdb',\n                            'ibis',\n                            'sqlite3']),\n            graphlib=choice(['seaborn',\n                             'plotly',\n                             'altair'])\n        ).sample(1)\n    )"
  },
  {
    "objectID": "posts/20221008-tt_problempicker.html#main",
    "href": "posts/20221008-tt_problempicker.html#main",
    "title": "TidyTuesday Problem Picker",
    "section": "Main",
    "text": "Main\n\nselect_project()\n\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2019\n\n\n\n\n\n\n  \n    \n      \n      date\n      url\n      datalib\n      graphlib\n    \n  \n  \n    \n      2019-06-09\n      2019-06-05\n      https://github.com/rfordatascience/tidytuesday...\n      sqlite3\n      altair"
  },
  {
    "objectID": "posts/20221008-tt_problempicker.html#history",
    "href": "posts/20221008-tt_problempicker.html#history",
    "title": "TidyTuesday Problem Picker",
    "section": "History",
    "text": "History\n\n2021-09-28, Economic records > Ibis, Seaborn"
  },
  {
    "objectID": "posts/20221008-write_csv_to_sqlite3_table.html",
    "href": "posts/20221008-write_csv_to_sqlite3_table.html",
    "title": "Write a CSV file To A SQLite3 Table",
    "section": "",
    "text": "Write records stored in a local CSV file to a SQL database.\nSources:\n\nSee pandas.DataFrame.to_sql (Pandas) for full signature\nHow can I create an in-memory database with sqlite? (StackOverflow)\n\n\n\n\n2022-10-08\n\nNotebook initialised\n\n2022-10-09\n\nSwapped sqlite3 \\(\\to\\) sqlalchemy\nRemoved the function, added instead as example script"
  },
  {
    "objectID": "posts/20221008-write_csv_to_sqlite3_table.html#dependencies",
    "href": "posts/20221008-write_csv_to_sqlite3_table.html#dependencies",
    "title": "Write a CSV file To A SQLite3 Table",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nimport sqlalchemy as sql"
  },
  {
    "objectID": "posts/20221008-write_csv_to_sqlite3_table.html#main",
    "href": "posts/20221008-write_csv_to_sqlite3_table.html#main",
    "title": "Write a CSV file To A SQLite3 Table",
    "section": "Main",
    "text": "Main\nCreate new trival local CSV file in path, and create an in-memory SQLite database.\n\n(pd.DataFrame()\n .assign(\n     pk=[1, 2, 3],\n     user=['user1', 'user2', 'user3'],\n     join_date=['2001-01-01', '2002-02-02', '2003-03-03']\n )\n .to_csv('__cache\\\\club_users.csv')\n)\nengine = sql.create_engine('sqlite://', echo=False)\n\nWrite the CSV file to a table name user.\n\n(pd.read_csv('__cache\\\\club_users.csv')\n .to_sql('user', engine, if_exists='replace', index=False, chunksize=10000)\n)\n\n3\n\n\nConfirm the new table’s schema and content.\n\npd.read_sql('SELECT * FROM user', engine).info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  3 non-null      int64 \n 1   pk          3 non-null      int64 \n 2   user        3 non-null      object\n 3   join_date   3 non-null      object\ndtypes: int64(2), object(2)\nmemory usage: 224.0+ bytes\n\n\n\npd.read_sql('SELECT * FROM user', engine)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      pk\n      user\n      join_date\n    \n  \n  \n    \n      0\n      0\n      1\n      user1\n      2001-01-01\n    \n    \n      1\n      1\n      2\n      user2\n      2002-02-02\n    \n    \n      2\n      2\n      3\n      user3\n      2003-03-03\n    \n  \n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsqlalchemy: 1.4.41\npandas    : 1.5.0"
  },
  {
    "objectID": "posts/20221011-advent_of_code_2018_d1.html",
    "href": "posts/20221011-advent_of_code_2018_d1.html",
    "title": "Advent of Code Day 1, 2018",
    "section": "",
    "text": "Source: — Day 1: Chronal Calibration —"
  },
  {
    "objectID": "posts/20221011-advent_of_code_2018_d1.html#dependencies",
    "href": "posts/20221011-advent_of_code_2018_d1.html#dependencies",
    "title": "Advent of Code Day 1, 2018",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport itertools as it\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/20221011-advent_of_code_2018_d1.html#solution",
    "href": "posts/20221011-advent_of_code_2018_d1.html#solution",
    "title": "Advent of Code Day 1, 2018",
    "section": "Solution",
    "text": "Solution\n\nGet data\n\ndata = lr.dataio.get_advent_input(2018, 1)\nfreq_changes = [int(x) for x in data]\nfreq_changes[:5]\n\n[-7, -19, 18, 19, -10]\n\n\n\n\nPart 1\n\nprint(f'Part 1 = {sum(freq_changes)}')\n\nPart 1 = 518\n\n\n\n\nPart 2\n\ndef find_first_repeat(freqs: list) -> int:\n    f, visited = 0, set([0])\n    for df in it.cycle(freqs):\n        f += df\n        if f in visited:\n            return f\n        visited.add(f)\n\n\nprint(f'Part 2 = {find_first_repeat(freq_changes)}')\n\nPart 2 = 72889"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d2.html",
    "href": "posts/20221012-advent_of_code_2018_d2.html",
    "title": "Advent of Code Day 2, 2018",
    "section": "",
    "text": "— Day 2: Inventory Management System —"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d2.html#dependencies",
    "href": "posts/20221012-advent_of_code_2018_d2.html#dependencies",
    "title": "Advent of Code Day 2, 2018",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import Counter\nimport itertools as it\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d2.html#solution",
    "href": "posts/20221012-advent_of_code_2018_d2.html#solution",
    "title": "Advent of Code Day 2, 2018",
    "section": "Solution",
    "text": "Solution\n\nGet input\n\ndata = lr.dataio.get_advent_input(2018, 2)\ndata[:5]\n\n['rvefnvyxzbodgpnpkumawhijsc',\n 'rvefqtyxzsddglnppumawhijsc',\n 'rvefqtywzbodglnkkubawhijsc',\n 'rvefqpyxzbozglnpkumawhiqsc',\n 'rvefqtyxzbotgenpkuyawhijsc']\n\n\n\n\nPart 1\n\ndef checksum(boxes: list) -> int:\n    box_counters = [Counter(box) for box in boxes]\n    char_sets = [set(box_counter.values()) for box_counter in box_counters]\n    char_counts = Counter([char_count for char_set in char_sets\n                           for char_count in char_set])\n    return char_counts[2] * char_counts[3]\n\n\nprint(f'Part 1 = {checksum(data)}')\n\nPart 2 = 6150\n\n\n\n\nPart 2\nI went for a fairly brute-force approach. A more elegant solution using a trie was posted by mjpieters.\n\ndef find_matching_pattern(boxes: list[str]):\n    n_chars = len(boxes[0])\n    for idx in range(len(boxes)):\n        ibox = boxes[idx]\n        for jbox in boxes[idx+1:]:\n            common_chars = ''\n            for ichar, jchar in zip(ibox, jbox):\n                if ichar == jchar:\n                    common_chars += ichar\n            if len(common_chars) == n_chars - 1:\n                return common_chars\n\n\nprint(f'Part 2 = {find_matching_pattern(data)}')\n\nPart 2 = rteotyxzbodglnpkudawhijsc"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d3.html",
    "href": "posts/20221012-advent_of_code_2018_d3.html",
    "title": "Advent of Code Day 3, 2018",
    "section": "",
    "text": "— Day 3: No Matter How You Slice It —"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d3.html#dependencies",
    "href": "posts/20221012-advent_of_code_2018_d3.html#dependencies",
    "title": "Advent of Code Day 3, 2018",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport itertools as it\nimport numpy as np\nfrom collections import Counter\nimport re\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/20221012-advent_of_code_2018_d3.html#solution",
    "href": "posts/20221012-advent_of_code_2018_d3.html#solution",
    "title": "Advent of Code Day 3, 2018",
    "section": "Solution",
    "text": "Solution\n\nGet input\n_parse_line was taken from mjpieters/adventofcode/2018/Day 03.\n\nlines = lr.dataio.get_advent_input(2018, 3)\nlines[:3]\n\n['#1 @ 861,330: 20x10', '#2 @ 491,428: 28x23', '#3 @ 64,746: 20x27']\n\n\n\n_parse_line = re.compile(r'#(\\d+)\\s*@\\s*(\\d+),(\\d+):\\s*(\\d+)x(\\d+)').search\ndef proc_claim(line):\n    match = _parse_line(line)\n    assert match is not None\n    id_, left, top, width, height = map(int, match.groups())\n    return id_, range(left, left + width), range(top, top + height)\n\n\nclaims = [proc_claim(line) for line in lines]\nclaims[-1]\n\n(1317, range(382, 410), range(340, 369))\n\n\n\n\nGet shared input\nParts 1 and 2 both use a shared count of claimed squares.\n\nclaimed_squares = Counter([sq for claim in claims\n                           for sq in it.product(claim[1], claim[2])])\n\n\n\nPart 1\n\nprint(\n    'Part 1 ='\n    + f' {sum([1 for cl in filter(lambda x: x>=2, claimed_squares.values())])}'\n)\n\nPart 1 = 104439\n\n\n\n\nPart 2\nNice solution to this using a trie:\n\ndef find_unique_claim(claims, claimed_squares):\n    for claim in claims:\n        overlapping = False\n        for square in it.product(claim[1], claim[2]):\n            if claimed_squares[square] >= 2:\n                overlapping = True\n                break\n        if not overlapping:\n            return claim[0]\n\n\nprint(f'Part 2 = {find_unique_claim(claims, claimed_squares)}')\n\nPart 2 = 701"
  }
]