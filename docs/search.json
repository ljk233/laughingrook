[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaughingRook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 13, 2022\n\n\nGlobal Causes of Mortality\n\n\nTidyTuesday\n\n\n\n\nSep 10, 2022\n\n\nAdvent of Code 2015, Day 4\n\n\nAdventOfCode\n\n\n\n\nSep 8, 2022\n\n\nQueue ADT\n\n\nAbstractDataType\n\n\n\n\nSep 6, 2022\n\n\nNFL Salaries\n\n\nTidyTuesday\n\n\n\n\nSep 5, 2022\n\n\nCase-Control Studies\n\n\nM249,Statistics\n\n\n\n\nSep 3, 2022\n\n\nAdvent of Code 2015, Day 3\n\n\nAdventOfCode\n\n\n\n\nSep 1, 2022\n\n\nStack ADT\n\n\nAbstractDataType,Algorithm\n\n\n\n\nAug 30, 2022\n\n\nCollege Tuition Fees in the USA\n\n\nTidyTuesday\n\n\n\n\nAug 29, 2022\n\n\nMatched Case-Control Studies\n\n\nM249,Statistics\n\n\n\n\nAug 27, 2022\n\n\nAdvent of Code 2015, Day 2\n\n\nAdventOfCode\n\n\n\n\nAug 25, 2022\n\n\nTravelling Salesperson Problem (Brute-force Search)\n\n\nAlgorithm,Graph\n\n\n\n\nAug 22, 2022\n\n\nStratified Analyses\n\n\nM249,Statistics\n\n\n\n\nAug 20, 2022\n\n\nAdvent of Code 2015, Day 1\n\n\nAdventOfCode\n\n\n\n\nAug 18, 2022\n\n\nInitialising an Undirected Graph in NetworkX\n\n\nGraph\n\n\n\n\nAug 15, 2022\n\n\nCohort Studies\n\n\nM249,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html",
    "title": "Cohort Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a cohort study.\nThis topics is covered by M249 Book 1, Part 1.\n\n\nData on a cohort study analysing the possible association between compulsory redundancies and incidents of serious self-inflicted injury (SSI) (Keefe, V., et al (2002)) was sourced. The exposure is being made compulsorily redundant, and the disease is incidents of serious self-inflicted injury.\nThe study results were as follows.\n\n\n\n\nSSI (+)\nno SSI (-)\n\n\n\n\nmade redundant (+)\n14\n1931\n\n\nnot made redundant (-)\n4\n1763\n\n\n\n\n\n\nThe data were stored remotely as a CSV file in a tidy manner with the schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ndisease\nstr\nDescriptive label indicating category of disease\n\n\n\n\n\n\nThe analysis was undertaken using StatsModels and SciPy.\nWe defined a function cache_file to handle the retrieval of the data.\nThe exposure and disease labels were stored as two lists with variables named exposures, diseases.\n\n\n\n\n\n\nNote\n\n\n\nNote that the orders of the two lists are important, and should correspond with:\n\nexposures = (exposed, not exposed)\ndiseases = (disease, no disease)\n\n\n\nThe data were cached and used to initialise data, a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the exposure, disease columns as ordered Categorical1 data types, and was sorted by (exposure, disease).\nWe took column cat_data[n] as data_arr, a Numpy NDArray with shape (2, 2), and used it to initialise ctable, an instance of Table2x2.2.\nMeasures of association3 were calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. We rounded-off the analysis by performing Fisher’s exact test.4 5"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#dependencies",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#dependencies",
    "title": "Cohort Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#functions",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#functions",
    "title": "Cohort Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname, and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#main",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#main",
    "title": "Cohort Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = ['redundant', 'not redundant']\ndiseases = ['ssi', 'no ssi']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/redundancy_ssi.csv'),\n    fname='redundancy_ssi.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   disease   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\nOutput a view of data.\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      disease\n    \n  \n  \n    \n      0\n      14\n      redundant\n      ssi\n    \n    \n      1\n      1931\n      redundant\n      no ssi\n    \n    \n      2\n      4\n      not redundant\n      ssi\n    \n    \n      3\n      1763\n      not redundant\n      no ssi\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns exposure, disease as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    disease=pd.Categorical(data['disease'], diseases, ordered=True)\n).sort_values(\n    by=['exposure',\n        'disease']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   disease   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput a pivot table with the marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='disease',\n    aggfunc='sum',\n    margins=True,\n    margins_name='total'\n).query(\n    \"exposure != 'total'\"\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n      total\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      redundant\n      14\n      1931\n      1945\n    \n    \n      not redundant\n      4\n      1763\n      1767\n    \n  \n\n\n\n\n\n\nInitialise the contingency table\nGet cat_data[n] as a NumPy NDArray with shape (2, 2).\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\n\nInitialise an instance of Table2x2 using data_arr.\n\nctable = sm.stats.Table2x2(data_arr)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[  14. 1931.]\n [   4. 1763.]]\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    3.195495\nlcb      1.049877\nucb      9.726081\nName: odds ratio, dtype: float64\n\n\nReturn point and interval estimates of the relative risk.\n\npd.Series(\n    data=[ctable.riskratio,\n          ctable.riskratio_confint()[0],\n          ctable.riskratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='relative risk'\n)\n\npoint    3.179692\nlcb      1.048602\nucb      9.641829\nName: relative risk, dtype: float64\n\n\n\n\nChi-squared test for no association\nReturn the expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[   9.43157328, 1935.56842672],\n       [   8.56842672, 1758.43157328]])\n\n\nReturn the differences between the observed and expected frequencies.\n\ndata_arr - ctable.fittedvalues\n\narray([[ 4.56842672, -4.56842672],\n       [-4.56842672,  4.56842672]])\n\n\nReturn the contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[2.21283577, 0.01078263],\n       [2.43574736, 0.01186883]])\n\n\nReturn the results of the chi-squared test.6\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    4.671235\npvalue      0.030672\ndf                 1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.033877\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#references",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies.html#references",
    "title": "Cohort Studies",
    "section": "References",
    "text": "References\nVera Keefe, Papaarangi Reid, Clint Ormsby, Bridget Robson, Gordon Purdie, Joanne Baxter, Ngäti Kahungunu Iwi Incorporated, Serious health events following involuntary job loss in New Zealand meat processing workers, International Journal of Epidemiology, Volume 31, Issue 6, December 2002, Pages 1155–1161, https://doi.org/10.1093/ije/31.6.1155\n\n%load_ext watermark\n%watermark --iv\n\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\nrequests   : 2.28.1\nscipy      : 1.9.0\npandas     : 1.4.3\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "",
    "text": "Initialise and populate an undirected weighted graph using NetworkX.\nThe source data is a CSV file listing the road network in Europe as an edge list.1\nWe first imported the data into a Pandas DataFrame.2 Next, we exported the DataFrame as a collection of dictionaries.3 We initialised an empty graph,4 and populated it. We closed the notebook by showing how to access the nodes, neighbors, and edges of the graph.\nWhilst we could populate the graph during initialisation, we found it added unneeded complexity.\nThe |edges| ≠ |edge list| because NetworkX’s Graph class does not permit parallel edges between two nodes.5"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom dataclasses import dataclass\nimport pandas as pd\nimport networkx as nx\n\n\nClasses\n\n@dataclass(frozen=True)\nclass PandasERoad:\n    \"\"\"A dataclass to help the conversion of the raod network data as a\n    graph.\n\n    Stores the remote url and maps the column titles to common graph\n    terminology\n    \"\"\"\n\n    url: str = ('https://raw.githubusercontent.com/ljk233'\n                + '/laughingrook-datasets/main/graphs/eroads_edge_list.csv')\n    u: str = 'origin_reference_place'\n    v: str = 'destination_reference_place'\n    uco: str = 'origin_country_code'\n    vco: str = 'destination_country_code'\n    w: str = 'distance'\n    rn: str = 'road_number'\n    wc: str = 'watercrossing'"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Functions",
    "text": "Functions\n\ndef edge_to_tuple(edge: dict, er: PandasERoad) -> tuple:\n    return (\n        edge[er.u],\n        edge[er.v],\n        {'weight': edge[er.w], er.rn: edge[er.rn], er.wc: edge[er.wc]}\n    )"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Main",
    "text": "Main\n\ner = PandasERoad()\n\n\nImport the data\n\neroads = pd.read_csv(er.url)\neroads.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 7 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   road_number                  1250 non-null   object\n 1   origin_country_code          1250 non-null   object\n 2   origin_reference_place       1250 non-null   object\n 3   destination_country_code     1250 non-null   object\n 4   destination_reference_place  1250 non-null   object\n 5   distance                     1250 non-null   int64 \n 6   watercrossing                1250 non-null   bool  \ndtypes: bool(1), int64(1), object(5)\nmemory usage: 59.9+ KB\n\n\n\n\nExport data to a dictionary\nEach entry in the list is a dictionary representing a single row, where the keys are the column titles.\n\nedges = eroads.to_dict(orient='records')\nedges[0]\n\n{'road_number': 'E01',\n 'origin_country_code': 'GB',\n 'origin_reference_place': 'Larne',\n 'destination_country_code': 'GB',\n 'destination_reference_place': 'Belfast',\n 'distance': 36,\n 'watercrossing': False}\n\n\n\n\nInitalise and populate the graph\n\ng = nx.Graph()\n\nAdds the nodes. We perform it on both the source and destination nodes in the edge list to ensure we populate all the cities, given there’s a chance that a city does not appear as a source city in the data.\n\ng.add_nodes_from((e[er.u], {'country': e[er.uco]}) for e in edges)\ng.add_nodes_from((e[er.v], {'country': e[er.vco]}) for e in edges)\n\nAdd the edges. Given this is an undirected graph, there is no need to add the reverse edges V → U.\nExample of output from edge_to_tuple\n\nedge_to_tuple(edges[0], er)\n\n('Larne',\n 'Belfast',\n {'weight': 36, 'road_number': 'E01', 'watercrossing': False})\n\n\nPopulate the edges.\n\ng.add_edges_from(edge_to_tuple(edge, er) for edge in edges)\n\n\n\nInspect the graph\nGet a description of the graph.\n\nprint(g)\n\nGraph with 894 nodes and 1198 edges\n\n\nGet a selection of the nodes.\n\n[n for n in g][:5]\n\n['Larne', 'Belfast', 'Dublin', 'Wexford', 'Rosslare']\n\n\nOutput a more descriptive list of nodes by calling the nodes() method.\n\n[n for n in g.nodes(data=True)][:5]\n\n[('Larne', {'country': 'GB'}),\n ('Belfast', {'country': 'GB'}),\n ('Dublin', {'country': 'IRL'}),\n ('Wexford', {'country': 'IRL'}),\n ('Rosslare', {'country': 'IRL'})]\n\n\nView the neighbours of the Roma node.\n\n[neighbor for neighbor in g['Roma']]\n\n['Arezzo', 'Grosseto', 'Pescara', 'San Cesareo']\n\n\nWe can get a more descriptive output of a node’s neighbours by not using list comprehension.\n\ng['Roma']\n\nAtlasView({'Arezzo': {'weight': 219, 'road_number': 'E35', 'watercrossing': False}, 'Grosseto': {'weight': 182, 'road_number': 'E80', 'watercrossing': False}, 'Pescara': {'weight': 209, 'road_number': 'E80', 'watercrossing': False}, 'San Cesareo': {'weight': 36, 'road_number': 'E821', 'watercrossing': False}})\n\n\nFinally, we can simply output the edges of the Roma node.\n\n[e for e in g.edges('Roma', data=True)]\n\n[('Roma',\n  'Arezzo',\n  {'weight': 219, 'road_number': 'E35', 'watercrossing': False}),\n ('Roma',\n  'Grosseto',\n  {'weight': 182, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'Pescara',\n  {'weight': 209, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'San Cesareo',\n  {'weight': 36, 'road_number': 'E821', 'watercrossing': False})]\n\n\n\n%load_ext watermark\n%watermark --iversions\n\nnetworkx: 2.8.6\npandas  : 1.4.3\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "title": "Advent of Code 2015, Day 1",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 1: Not Quite Lisp."
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "title": "Advent of Code 2015, Day 1",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom itertools import accumulate\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "title": "Advent of Code 2015, Day 1",
    "section": "Functions",
    "text": "Functions\n\ndef find_first(x, A) -> int:\n    \"\"\"Find the first index i where A[i] = x.\n\n    Precondtions:\n    - x in A\n    - A is 1-dimensional\n    - A support iteration\n    \"\"\"\n    return next(i for i, a in enumerate(A) if a == x)"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "title": "Advent of Code 2015, Day 1",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 1)\nprint(f\"line = '{line[:5]}'\")\n\nline = '()((('\n\n\n\n\nTransform the input\n\nm = {'(': 1, ')': -1}\ndirections = [m[bracket] for bracket in line]\nprint(f\"directions = {directions[:5]}\")\n\ndirections = [1, -1, 1, 1, 1]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(directions)}\")\n\nSolution = 138\n\n\n\n\nPart 2\n\nprint(f\"Solution = {find_first(-1, accumulate(directions)) + 1}\")\n\nSolution = 1771\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(directions)\nprint('Part 2 =')\n%timeit find_first(-1, accumulate(directions)) + 1\n\nPart 1 =\n62.5 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nPart 2 =\n73.6 µs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html",
    "title": "Stratified Analyses",
    "section": "",
    "text": "Perform a stratified analyses on the results of a stratified case-control study.\nThis topic is covered in M249, Book 1, Part 2.\n\n\nData were taken from investigating the possible association between alcohol consumption and fatal car accidents in New York (J.R. McCarroll and W. Haddon Jr, 1962). The data were stratified by marital status, which was believed to be a possible confounder. The exposure was blood alcohol level of 100mg% or greater; Cases were drivers who were killed in car accidents for which they were considered to be responsible; and controls were selected drivers passing the locations where the accidents of the cases occurred, at the same time of day and on the same day of the week.\nThe results were as follows.\nLevel: Married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n4\n5\n\n\nunder 100mg% (-)\n5\n103\n\n\n\nLevel: Not married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n10\n3\n\n\nunder 100mg% (-)\n5\n43\n\n\n\n\n\n\nThe data were stored remotely as a CSV file in a tidy manner with the schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nlevel\nstr\nDescriptive label indicating category of level\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ncasecon\nstr\nDescriptive label indicating category of case or control\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe main difficulty with a stratified analysis is that we need to deal with the data in many different shapes. Be careful when handling and reshaping the data!\n\n\nThe analysis was undertaken using StatsModels.\nWe defined a function cache_file to handle the retrieval of the data.\nThe function get_odds_ratio_arr was defined to return a point and interval estimate of the odds ratio as a list.\nThe exposure and case-controls labels were stored as two lists with variables named exposures, casecons.\n\n\n\n\n\n\nNote\n\n\n\nNote that the orders of the two lists are important, and should correspond with:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\n\n\nThe data were cached and used to initialise data, a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the level, exposure, casecon columns as ordered Categorical1 data types, and was sorted by (exposure, disease, level). Column cat_data[level] columns was taken as levels, a NumPy NDArray.\nThree variables were initialised to handle the different views of the data. These were:\n\nlevel_tables, a collection of Table2x22 instances, where each instance represents a contingency table for a specific level\naggregated_table, an instance of Table2x2 using the aggregated data3\nstrat_table, an instance of StratifiedTable4\n\nThe level-specific, unadjusted5 and adjusted6 odds ratios were calculated, including their confidence interval estimates. Two hypothesis tests were performed: a test of no association and a test of homogeneity."
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#dependencies",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#dependencies",
    "title": "Stratified Analyses",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nfrom dataclasses import dataclass\nimport numpy as np\nimport pandas as pd\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#functions",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#functions",
    "title": "Stratified Analyses",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path\n\n\ndef get_odds_ratio_arr(table: sm.stats.Table2x2, alpha: float = 0.05) -> list:\n    \"\"\"Return the point and (100-alpha)% lower and upper confidence\n    boundaries.\n\n    Preconditions:\n    - 0 < alpha < 1\n    \"\"\"\n    return [table.oddsratio,\n            table.oddsratio_confint(alpha=alpha)[0],\n            table.oddsratio_confint(alpha=alpha)[1]]"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#main",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#main",
    "title": "Stratified Analyses",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = ['over 100mg', 'under 100mg']\ncasecons = ['fatality', 'no fatality']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n        url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n             + '/main/m249/medical/drinkdriving.csv'),\n        fname='drinkdriving.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8 entries, 0 to 7\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         8 non-null      int64 \n 1   level     8 non-null      object\n 2   exposure  8 non-null      object\n 3   casecon   8 non-null      object\ndtypes: int64(1), object(3)\nmemory usage: 384.0+ bytes\n\n\nOutput a view of data.\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      level\n      exposure\n      casecon\n    \n  \n  \n    \n      0\n      4\n      married\n      over 100mg\n      fatality\n    \n    \n      1\n      5\n      married\n      over 100mg\n      no fatality\n    \n    \n      2\n      5\n      married\n      under 100mg\n      fatality\n    \n    \n      3\n      103\n      married\n      under 100mg\n      no fatality\n    \n    \n      4\n      10\n      not married\n      over 100mg\n      fatality\n    \n    \n      5\n      3\n      not married\n      over 100mg\n      no fatality\n    \n    \n      6\n      5\n      not married\n      under 100mg\n      fatality\n    \n    \n      7\n      43\n      not married\n      under 100mg\n      no fatality\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns level as a Categorical variable, and exposure, casecon as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    level=pd.Categorical(data['level']),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    casecon=pd.Categorical(data['casecon'], casecons, ordered=True)\n).sort_values(\n    by=['exposure',\n        'casecon',\n        'level']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 8 entries, 0 to 7\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         8 non-null      int64   \n 1   level     8 non-null      category\n 2   exposure  8 non-null      category\n 3   casecon   8 non-null      category\ndtypes: category(3), int64(1)\nmemory usage: 524.0 bytes\n\n\nGet level as a NumPy NDArray.\n\nlevels = np.unique(cat_data['level'])\nlevels\n\narray(['married', 'not married'], dtype=object)\n\n\nOutput pivot tables with marginal totals.\n\nfor level in levels:\n    print(\n        cat_data.query(\n            'level == @level'\n        ).pivot_table(\n            values='n',\n            index=['level', 'exposure'],\n            columns='casecon',\n            aggfunc='sum',\n            margins=True,\n            margins_name='total'\n        ).query(\n            \"level in [@level, 'total']\"\n        ),\n        '\\n'\n    )\n\ncasecon              fatality  no fatality  total\nlevel   exposure                                 \nmarried over 100mg          4            5    9.0\n        under 100mg         5          103  108.0\ntotal                       9          108  117.0 \n\ncasecon                  fatality  no fatality  total\nlevel       exposure                                 \nnot married over 100mg         10            3   13.0\n            under 100mg         5           43   48.0\ntotal                          15           46   61.0 \n\n\n\n\n\nInitialise the contingency tables\nInitialise contingency tables for different views of the data.\n\n_make_table2x2 = lambda df, level: (\n    sm.stats.Table2x2(\n        df.query('level == @level')['n']\n          .to_numpy()\n          .reshape(2, 2)\n    )\n)\nlevel_tables = [_make_table2x2(cat_data, level) for level in levels]\nfor table, level in zip(level_tables, levels):\n    print(f'level={level}\\n{table}\\n')\n\nlevel=married\nA 2x2 contingency table with counts:\n[[  4.   5.]\n [  5. 103.]]\n\nlevel=not married\nA 2x2 contingency table with counts:\n[[10.  3.]\n [ 5. 43.]]\n\n\n\n\naggregated_table = sm.stats.Table2x2(\n    cat_data.groupby(['exposure', 'casecon'])\n            .sum()\n            .to_numpy()\n            .reshape((2, 2))\n)\nprint(aggregated_table)\n\nA 2x2 contingency table with counts:\n[[ 14.   8.]\n [ 10. 146.]]\n\n\n\nstrat_table = (\n    sm.stats.StratifiedTable(\n        data['n'].to_numpy().reshape((2, 2, 2))\n    )\n)\nprint(strat_table.table)\n\n[[[  4.   5.]\n  [  5. 103.]]\n\n [[ 10.   3.]\n  [  5.  43.]]]\n\n\n\n\nOdds ratios\nReturn point and interval estimates of the level-specific, unadjusted, and adjusted odds ratios.\nReturn the level-specific odds ratios.\n\npd.DataFrame(\n    data=[get_odds_ratio_arr(table) for table in level_tables],\n    columns=['point', 'lcb', 'ucb'],\n    index=pd.Index(levels, name='level')\n).round(\n    5\n)\n\n\n\n\n\n  \n    \n      \n      point\n      lcb\n      ucb\n    \n    \n      level\n      \n      \n      \n    \n  \n  \n    \n      married\n      16.48000\n      3.35421\n      80.96998\n    \n    \n      not married\n      28.66667\n      5.85662\n      140.31607\n    \n  \n\n\n\n\nReturn the unadjusted odds ratio.\n\npd.Series(\n    data=get_odds_ratio_arr(aggregated_table),\n    index=['point', 'lcb', 'ucb'],\n    name='unadjusted odds ratio'\n).round(\n    5\n)\n\npoint    25.55000\nlcb       8.68217\nucb      75.18883\nName: unadjusted odds ratio, dtype: float64\n\n\nReturn the adjusted odds ratio.\n\npd.Series(\n    data=[strat_table.oddsratio_pooled,\n           strat_table.oddsratio_pooled_confint()[0],\n           strat_table.oddsratio_pooled_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='adjusted odds ratio'\n).round(\n    3\n)\n\npoint    0.545\nlcb      0.182\nucb      1.634\nName: adjusted odds ratio, dtype: float64\n\n\n\n\nHypothesis testing\nReturn the result of a test of no association.\n\n_r = strat_table.test_null_odds(correction=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of no association'\n).round(\n    3\n)\n\nstatistic    0.612\npvalue       0.434\nName: test of no association, dtype: float64\n\n\nReturn the result of a test of homogeneity.\n\n_r = strat_table.test_equal_odds(adjust=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of homogeneity'\n).round(\n    3\n)\n\nstatistic    0.233\npvalue       0.629\nName: test of homogeneity, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#references",
    "href": "posts/2022-08-22-m249_b1p2i_stratified_analyses.html#references",
    "title": "Stratified Analyses",
    "section": "References",
    "text": "References\nMcCarroll, J.R. and Haddon Jr, W., 1962. A controlled study of fatal automobile accidents in New York City. Journal of chronic diseases, 15(8), pp.811-826.\n\n%load_ext watermark\n%watermark --iv\n\nstatsmodels: 0.13.2\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nrequests   : 2.28.1\nnumpy      : 1.23.2\npandas     : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html",
    "href": "posts/2022-08-25-tsp_bruteforce.html",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "",
    "text": "A solution to the Travelling Salesperson Problem using a brute-force search.\nThe Travelling Salesperson Problem is defined as:\n\nGiven a set of cities and distances between every pair of cities, the [Travelling Salesperson] problem is to find the shortest possible route that visits every city exactly once and returns to the starting point.\nTraveling Salesman Problem (TSP) Implementation (GeeksForGeeks)\n\nIn this implementation, we generate permutations and check if the |path| < |min path|.\nWe use a complete undirected graph, with each edge being assigned a random weight (representing the distance).\nWhilst the function works, it is unusuable when |nodes(g)| ≥ 11, given P(11, 11) = 36720000 permutations to check!"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "href": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport random as rand\nimport math\nimport itertools as it\nimport networkx as nx"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#function",
    "href": "posts/2022-08-25-tsp_bruteforce.html#function",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Function",
    "text": "Function\n\ndef bruteforce_tsp(G: nx.Graph, start: object) -> float | int:\n    \"\"\"Return the shortest route that visits every city exactly once and\n    ends back at the start.\n\n    Solves the travelling salesperson with a brute-force search using\n    permutations.\n\n    Preconditions:\n    - G is a complete weighted graph\n    - start in G\n    - WG[u, v]['weight'] is the distance u -> v\n    \"\"\"\n    neighbours = set((node for node in G.nodes if node != start))\n    min_dist = math.inf\n    for path in it.permutations(neighbours):\n        u, dist = start, 0\n        for v in path:\n            dist += G.edges[u, v]['weight']\n            u = v\n        min_dist = min(min_dist, dist + G.edges[u, start]['weight'])\n\n    return min_dist"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#main",
    "href": "posts/2022-08-25-tsp_bruteforce.html#main",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Main",
    "text": "Main\n\nInitialise the graph\nWe initialise a complete weighted undirected graph with 5 nodes.\n\ncg = nx.complete_graph(['origin', 'a', 'b', 'c', 'd'])\ng = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\nprint(f\"g = {g}\")\n\ng = Graph with 5 nodes and 10 edges\n\n\n\n\nFind the shortest path from the origin\n\nprint(f\"Shortest path from the origin = {bruteforce_tsp(g, 'origin')}\")\n\nShortest path from the origin = 27\n\n\n\n\nPerformance\n\nfor n in [4, 6, 8, 10]:\n    print(f\"|nodes(g)| = {n}\")\n    cg = nx.complete_graph(n)\n    g = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\n    %timeit bruteforce_tsp(g, 1)\n\n|nodes(g)| = 4\n8.19 µs ± 61.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n|nodes(g)| = 6\n206 µs ± 1.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n|nodes(g)| = 8\n11.1 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n|nodes(g)| = 10\n969 ms ± 6.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnetworkx: 2.8.6"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "title": "Advent of Code 2015, Day 2",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 2: I Was Told There Would Be No Math."
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "title": "Advent of Code 2015, Day 2",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "title": "Advent of Code 2015, Day 2",
    "section": "Functions",
    "text": "Functions\n\ndef paper_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the wrapping paper needed for a present with dims (l, w, h).\n    \"\"\"\n    planes = [(length * width), (width * height), (height * length)]\n    return min(planes) + (2 * sum(planes))\n\n\ndef ribbon_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the ribbon needed to cover a present with dims (l, w, h).\n    \"\"\"\n    bow = length * width * height\n    ribbon = 2 * sum(sorted([length, width, height])[:2])\n    return bow + ribbon"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "title": "Advent of Code 2015, Day 2",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nlines = lr.datasets.get_advent_input(2015, 2)\nprint(f\"lines = {lines[:5]}\")\n\nfile was cached.\nlines = ['4x23x21', '22x29x19', '11x4x11', '8x10x5', '24x18x16']\n\n\n\n\nTransform the input\n\nllines = (line.split('x') for line in lines)\npdims = [[int(x) for x in lline] for lline in llines]\nprint(f\"pdims = {pdims[:5]}\")\n\npdims = [[4, 23, 21], [22, 29, 19], [11, 4, 11], [8, 10, 5], [24, 18, 16]]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(paper_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 1598415\n\n\n\n\nPart 2\n\nprint(f\"Solution = {sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 3812909\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(paper_needed(l, w, h) for (l, w, h) in pdims)\nprint('Part 2 =')\n%timeit sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)\n\nPart 1 =\n433 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPart 2 =\n468 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html",
    "title": "Matched Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a 1-1 matched case-control study.\nData was taken from a case-control study undertaken to identify some of the riskfactors associated with death during heatwave in Chicago that occured from 12 July 1995 to 16 July 1995. Cases were persons aged 24+ years who died between 14-17 July 1995, with a cause mentioned on the death certificate that was possibly heat related. For each case, a matched control was selected of the same age and living in the same neighbourhood. The risk factor of interest was participation in group activities involving social interactions. (Semenza, Rubin, Falter, et al (1996))\nThe results were as follows.\n\n\n\n\n\n\n\n\nCases / Controls\nParticipated (+)\nDid not participate (-)\n\n\n\n\nParticipated (+)\n77\n63\n\n\nDid not participate (-)\n90\n74\n\n\n\nThe analysis was undertaken using StatsModels1 and a user-defined function.2 The results were initialised as a NumPy array.3\nThe data was stored on remotely, so we defined a function cache_file to handle the retrieval of the file.\nWe initialised a single list, casecons, which held the labels for the specific cases and controls.\nThe data were cached and used to a initialise a Pandas DataFrame.\nWe casted the cases and controls columns to ordered Categorical data, to ensure the data would appear as expected.4\nA Mantel-Haenszel odds ratio for the association between participation in group activities and dying of heat-related disease was calculated, including a 95% confidence interval estimate. Finally, McNemar’s test5 was performed to test the null hypothesis of no association between participation in group activities and dying of heat-related disease.\nThese topics are covered by M249 Book 1, Part 2."
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#dependencies",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#dependencies",
    "title": "Matched Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm\nfrom numpy.typing import ArrayLike"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#functions",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#functions",
    "title": "Matched Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path\n\n\ndef mh_odds_ratio(ctable: ArrayLike, alpha: float = 0.05) -> tuple:\n    \"\"\"Return point and 100(1-alpha)% intervel estimates of the\n    Mantel-Haenszel odds ratio.\n\n    Pre-conditions:\n    - arr represents the results of a 1-1 matched case-control study\n        - shape(arr) = 2, 2\n        - rows represent cases, columns represent controls\n        - row 0, col 0 represent (+)\n        - row 1, col 1 represent (-)\n    - 0 < alpha < 1\n\n    Post-conditions:\n    - tuple of float estimates, (point, lcb, ucb)\n    \"\"\"\n\n    f, g = ctable[0, 1], ctable[1, 0]\n    ste = (st.norm.ppf(1 - (alpha/2)) * np.sqrt(1/f + 1/g))\n    return f / g, f / g * np.exp(-ste), f / g * np.exp(ste)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#main",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#main",
    "title": "Matched Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\ncasecons = ['participated', 'not participated']\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/heat_deaths.csv'),\n    fname='heat_deaths.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   cases     4 non-null      object\n 2   controls  4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      cases\n      controls\n    \n  \n  \n    \n      0\n      77\n      participated\n      participated\n    \n    \n      1\n      63\n      participated\n      not participated\n    \n    \n      2\n      90\n      not participated\n      participated\n    \n    \n      3\n      74\n      not participated\n      not participated\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns cases, controls as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    cases=pd.Categorical(data['cases'], casecons, ordered=True),\n    controls=pd.Categorical(data['controls'], casecons, ordered=True)\n).sort_values(\n    by=['cases', 'controls']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   cases     4 non-null      category\n 2   controls  4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput pivot tables with marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='cases',\n    columns='controls',\n    aggfunc='sum',\n    margins=True,\n    margins_name='subtotal'\n)\n\n\n\n\n\n  \n    \n      controls\n      participated\n      not participated\n      subtotal\n    \n    \n      cases\n      \n      \n      \n    \n  \n  \n    \n      participated\n      77\n      63\n      140\n    \n    \n      not participated\n      90\n      74\n      164\n    \n    \n      subtotal\n      167\n      137\n      304\n    \n  \n\n\n\n\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\ndata_arr\n\narray([[77, 63],\n       [90, 74]], dtype=int64)\n\n\n\n\nMatel-Haenszel odds ratio\n\n_point, _lcb, _ucb = mh_odds_ratio(data_arr)\npd.Series(\n    data=[_point, _lcb, _ucb],\n    index=['point', 'lcb', 'ucb'],\n    name='mantel-haenszel odds ratio'\n)\n\npoint    0.700000\nlcb      0.507309\nucb      0.965881\nName: mantel-haenszel odds ratio, dtype: float64\n\n\n\n\nMcNemar’s test\n\n_r = sm.stats.mcnemar(data_arr, exact=False)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistc', 'pvalue'],\n    name='mcnemar''s test'\n)\n\nstatistc    4.418301\npvalue      0.035555\nName: mcnemars test, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#references",
    "href": "posts/2022-08-29-m249_b1p2ii_matched_case_control.html#references",
    "title": "Matched Case-Control Studies",
    "section": "References",
    "text": "References\nSemenza, J. C., Rubin, C. H., Falter, K. H., Selanikio, J. D., Flanders, W. D., Howe, H. L., & Wilhelm, J. L. (1996). Heat-related deaths during the July 1995 heat wave in Chicago. New England journal of medicine, 335, 84-90. https://doi.org/10.1056/NEJM199607113350203.\n\n%load_ext watermark\n%watermark --iv\n\nrequests   : 2.28.1\nscipy      : 1.9.0\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\npandas     : 1.4.3\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "title": "College Tuition Fees in the USA",
    "section": "",
    "text": "Visualising the average annual cost of College tuition fees in the USA.\n\n\nHappy to announce the newest #R4DS online learning community project! #TidyTuesday is your weekly #tidyverse practice!Each week we'll post data and a plot at https://t.co/8NaXR93uIX under the datasets link.You clean the data and tweak the plot in R!#rstats #ggplot2 pic.twitter.com/sDaHsB8uwL\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 2, 2018"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "title": "College Tuition Fees in the USA",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport pandas as pd\nimport polars as pl\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "title": "College Tuition Fees in the USA",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "title": "College Tuition Fees in the USA",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\nus_avg_tuition_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-02/us_avg_tuition.xlsx?raw=true'),\n    fname='us_avg_tuition.xlsx'\n)\n\n\nansi_path = cache_file(\n    url=('https://www2.census.gov/geo/docs/reference/state.txt'),\n    fname='state.txt'\n)\n\n\n\nLoad the data\n\nus_avg_tuition = pl.DataFrame(pd.read_excel(us_avg_tuition_path)).lazy()\nus_avg_tuition.schema\n\n{'State': polars.datatypes.Utf8,\n '2004-05': polars.datatypes.Float64,\n '2005-06': polars.datatypes.Float64,\n '2006-07': polars.datatypes.Float64,\n '  2007-08 ': polars.datatypes.Float64,\n '2008-09': polars.datatypes.Float64,\n '2009-10': polars.datatypes.Float64,\n '2010-11': polars.datatypes.Float64,\n '2011-12': polars.datatypes.Float64,\n '2012-13': polars.datatypes.Float64,\n '2013-14': polars.datatypes.Float64,\n '2014-15': polars.datatypes.Float64,\n '2015-16': polars.datatypes.Float64}\n\n\n\nansi = pl.DataFrame(pd.read_csv(ansi_path, sep='|')).lazy()\nansi.schema\n\n{'STATE': polars.datatypes.Int64,\n 'STUSAB': polars.datatypes.Utf8,\n 'STATE_NAME': polars.datatypes.Utf8,\n 'STATENS': polars.datatypes.Int64}\n\n\n\nstates = alt.topo_feature(vdata.us_10m.url, 'states')\n\n\n\nPrepare the data\n\nlazy_query = us_avg_tuition.select(\n    ['State',\n     '2010-11',\n     '2015-16']\n).melt(\n    id_vars='State',\n    variable_name='year',\n    value_name='tuition'\n).join(\n    other=ansi,\n    left_on='State',\n    right_on='STATE_NAME',\n    how='inner'\n).with_column(\n    pl.col('tuition').pct_change().over('State').alias('pct_change')\n).filter(\n    pl.col('pct_change').is_not_null()\n).select(\n    [pl.col('STATE').alias('state_id'),\n     pl.col('State').alias('state_name'),\n     pl.col('tuition'),\n     pl.col('pct_change').apply(lambda x: x * 100).round(1)]\n)\nlazy_query.schema\n\n{'state_id': polars.datatypes.Int64,\n 'state_name': polars.datatypes.Utf8,\n 'tuition': polars.datatypes.Float64,\n 'pct_change': polars.datatypes.Float64}\n\n\n\n\nVisualise the data\nBoth visualisations will use the same source, so we initialise a single instance of alt.Chart.\n\nch = alt.Chart(lazy_query.collect().to_pandas())\n\nPlot the percentage change in college annual tuition costs from 2010 to 2015 as a chloropleth heatmap.\n\nch.mark_geoshape(\n    stroke='black'\n).encode(\n    shape='geo:G',\n    color=alt.Color(\n                \"pct_change\",\n                scale=alt.Scale(scheme=\"oranges\"),\n                legend=alt.Legend(title='Change (%)')\n    ),\n    tooltip=[alt.Tooltip('state_name', title='State'),\n             alt.Tooltip('pct_change', title='Change (%)')]\n).transform_lookup(\n    lookup='state_id',\n    from_=alt.LookupData(data=states, key='id'),\n    as_='geo'\n).project(\n    type='albersUsa'\n).properties(\n    title='Percentage change in college tuition costs between 2010 and 2015',\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nPlot the annual cost of college tuition in the USA in 2015-16 as a dot plot.\n\nch.mark_circle(\n    size=60\n).encode(\n    x=alt.X('tuition', title='Tuition ($)'),\n    y=alt.Y('state_name', sort='-x', axis=alt.Axis(grid=True), title='State')\n).properties(\n    title='College tuition costs in the USA (2015-16)',\n    width=400,\n    height=600\n)\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\npolars  : 0.14.6\naltair  : 4.2.0\nrequests: 2.28.1\npandas  : 1.4.3"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html",
    "href": "posts/2022-09-01-stack_adt.html",
    "title": "Stack ADT",
    "section": "",
    "text": "Using Python’s collections.deque[^3] class as an implementation of the Stack ADT. A stack is:"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#dependencies",
    "href": "posts/2022-09-01-stack_adt.html#dependencies",
    "title": "Stack ADT",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import deque"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#function",
    "href": "posts/2022-09-01-stack_adt.html#function",
    "title": "Stack ADT",
    "section": "Function",
    "text": "Function\nWe used a deque for the Stack in this implementation of has_balanced_brackets.\n\ndef has_balanced_brackets(expression: str) -> bool:\n    \"\"\"Return true if the given expression has balanced brackets.\n    \"\"\"\n    stack = deque()\n    matching = {')': '(', '}': '{', ']': '[', '>': '<'}\n    left, right = set(matching.values()), matching.keys()\n    for char in expression:\n        if char in left:\n            stack.append(char)\n        elif char in right:\n            if len(stack) == 0 or stack[-1] != matching[char]:\n                return False\n            stack.pop()\n    return len(stack) == 0"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#main",
    "href": "posts/2022-09-01-stack_adt.html#main",
    "title": "Stack ADT",
    "section": "Main",
    "text": "Main\n\nTesting\n(Apologies for the non-standard table representation!)\n\n#         desc           expression                          exp result   \ncases = [['no text',     '',                                 True],\n         ['no brackets', 'brackets are like Russian dolls',  True],\n         ['matched',     '(3 + 4)',                          True],\n         ['mismatched',  '(3 + 4]',                          False],\n         ['not opened',  '3 + 4]',                           False],\n         ['not closed',  '(3 + 4',                           False],\n         ['wrong order', 'close ) before open (',            False],\n         ['no nesting',  '()[]\\{\\}<>',                       True],\n         ['nested',      '([{(<[{}]>)}])',                   True],\n         ['nested pair', 'items[(i - 1):(i + 1)]',           True]]\n\nfor (test, expr, exp_res) in cases:\n    assert has_balanced_brackets(expr) == exp_res, f'Test {test} failed'\nprint('Testing complete.')\n\nTesting complete.\n\n\n\n\nPerformance\nThe performance tests show a doubling in has_balanced_brackets’s time-to-run, as the |expression| doubles. It has a linear complexity.\n\nbrackets = \"()\"\nsizes = [100, 200, 400, 800]\nfor i, n in enumerate(sizes):\n    print(f'Test {i+1} (n={n}) =')\n    expr = brackets * n\n    %timeit -r 3 -n 5000 has_balanced_brackets(expr)\n\nTest 1 (n=100) =\n21.8 µs ± 917 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 2 (n=200) =\n41.7 µs ± 166 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 3 (n=400) =\n84 µs ± 795 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 4 (n=800) =\n158 µs ± 996 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "title": "Advent of Code 2015, Day 3",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 3: Perfectly Spherical Houses in a Vacuum."
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "title": "Advent of Code 2015, Day 3",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "title": "Advent of Code 2015, Day 3",
    "section": "Functions",
    "text": "Functions\n\ndef get_next(position, direction) -> tuple:\n    \"\"\"Return the next position based on the given direction.\n    \"\"\"\n    x, y = position\n    if direction == '>':\n        return (x+1, y)\n    elif direction == '<':\n        return (x-1, y)\n    elif direction == '^':\n        return (x, y+1)\n    else:\n        return (x, y-1)\n\n\ndef deliver_presents(directions: str) -> set:\n    \"\"\"Return a set of tuples representing the positions of houses where\n    presents were delivered.\n    \"\"\"\n    houses, position = set(), (0, 0)\n    houses.add(position)\n    for direction in directions:\n        next_position = get_next(position, direction)\n        houses.add(next_position)\n        position = next_position\n    return houses"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "title": "Advent of Code 2015, Day 3",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 3)\nprint(f\"line = '{line[:5]}'\")\n\nline = '^^<<v'\n\n\n\n\nPart 1\n\nprint(f'Solution = {len(deliver_presents(line))}')\n\nSolution = 2565\n\n\n\n\nPart 2\n\nsanta_houses = deliver_presents(line[::2])\nrobot_houses = deliver_presents(line[1::2])\nprint(f'Solution = {len(santa_houses.union(robot_houses))}')\n\nSolution = 2639\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit len(deliver_presents(line))\nprint('Part 2 =')\n%timeit len(deliver_presents(line[::2]).union(deliver_presents(line[1::2])))\n\nPart 1 =\n2.09 ms ± 38 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nPart 2 =\n2.16 ms ± 12.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html",
    "title": "Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a case-control study.\nThis topics is covered by M249 Book 1, Part 1.\n\n\nData on a case-control analysing the possible association between political activity and death by homicide was sourced (Mian, A., Mahmood, S.F., et al (2022)). In total, 35 vicitims of homicide were included in the study, and 85 controls with similar age and sex distributions as the victims. Household members were questioned on the policial activities of the study subjects. Of the 35 victims, 11 had attended political meeting, compared to two of the controls.\nThe study results were as follows.\n\n\n\n\nhomicide (+)\nnot homicide (-)\n\n\n\n\nattended (+)\n11\n2\n\n\ndid not attend (-)\n24\n83\n\n\n\n\n\n\nThe data were stored remotely as a CSV file in a tidy manner with the schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ncasecon\nstr\nDescriptive label indicating category of case or control\n\n\n\n\n\n\nThe analysis was undertaken using StatsModels and SciPy.\nWe defined a function cache_file to handle the retrieval of the data.\nThe exposure and case-controls labels were stored as two lists with variables named exposures, casecons.\n\n\n\n\n\n\nNote\n\n\n\nNote that the orders of the two lists are important, and should correspond with:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\n\n\nThe data were cached and used to initialise data, a Pandas DataFrame.\nA new DataFrame cat_data was initialised from data, with the exposure, casecon columns as ordered Categorical1 data types, and was sorted by (exposure, disease).\nWe took column cat_data[n] as data_arr, a Numpy NDArray with shape (2, 2), and used it to initialise ctable, an instance of Table2x2.2\nThe odds ratio was calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. We rounded-off the analysis by performing Fisher’s exact test.3 4"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#dependencies",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#dependencies",
    "title": "Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#functions",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#functions",
    "title": "Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname, and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#main",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#main",
    "title": "Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = np.array(['attended', 'did not attend'])\ncasecons = np.array(['homicide', 'not homicide'])\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/karachi.csv'),\n    fname='karachi.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   casecon   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\nOutput a view of data.\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      casecon\n    \n  \n  \n    \n      0\n      11\n      attended\n      homicide\n    \n    \n      1\n      2\n      attended\n      not homicide\n    \n    \n      2\n      24\n      did not attend\n      homicide\n    \n    \n      3\n      83\n      did not attend\n      not homicide\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns exposure, disease as ordered Categorical variables.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    casecon=pd.Categorical(data['casecon'], casecons, ordered=True)\n).sort_values(\n    by=['exposure', 'casecon']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   casecon   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput a pivot table with the marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='casecon',\n    aggfunc='sum',\n    margins=True,\n    margins_name='total'\n).drop(\n    columns='total'\n)\n\n\n\n\n\n  \n    \n      casecon\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      11\n      2\n    \n    \n      did not attend\n      24\n      83\n    \n    \n      total\n      35\n      85\n    \n  \n\n\n\n\n\n\nInitialise the contingency table\nGet cat_data[n] as a NumPy NDArray with shape (2, 2).\n\ndata_arr = cat_data['n'].to_numpy().reshape(2, 2)\n\nInitialise an instance of Table2x2 using data_arr.\n\nctable = sm.stats.Table2x2(data_arr)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[11.  2.]\n [24. 83.]]\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    19.020833\nlcb       3.942873\nucb      91.758497\nName: odds ratio, dtype: float64\n\n\n\n\nChi-squared test for no association\nReturn the expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[ 3.79166667,  9.20833333],\n       [31.20833333, 75.79166667]])\n\n\nReturn the differences between the observed and expected frequencies.\n\ndata_arr - ctable.fittedvalues\n\narray([[ 7.20833333, -7.20833333],\n       [-7.20833333,  7.20833333]])\n\n\nReturn the contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[13.70375458,  5.64272247],\n       [ 1.66494215,  0.68556441]])\n\n\nReturn the result of the chi-squared test.5\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    21.696984\npvalue       0.000003\ndf                  1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.000018\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#references",
    "href": "posts/2022-09-05-m249_b1p1i_case_control_studies.html#references",
    "title": "Case-Control Studies",
    "section": "References",
    "text": "References\nMian, A., Mahmood, S.F., Chotani, H. and Luby, S., 2002. Vulnerability to homicide in Karachi: political activity as a risk factor. International journal of epidemiology, 31(3), 581-585.\n\n%load_ext watermark\n%watermark --iv\n\nscipy      : 1.9.0\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nstatsmodels: 0.13.2\npandas     : 1.4.3\nnumpy      : 1.23.2\nrequests   : 2.28.1"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html",
    "title": "NFL Salaries",
    "section": "",
    "text": "Visualising the change in median earnings of NFL positions over time.\n\n\nWelcome to Week 2 of #TidyTuesday! We'll be exploring a 538 article on NFL salaries! Good luck & have fun!Data: https://t.co/8NaXR93uIXArticle: https://t.co/6GdoPb0hJcData Source: https://t.co/vliDMOl9LcBlog: https://t.co/cZJ94Hhz7U#tidyverse #rstats #dataviz #ggplot2 #r4ds pic.twitter.com/AF7qTFLvkj\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 9, 2018"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#dependencies",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#dependencies",
    "title": "NFL Salaries",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport polars as pl\nimport pandas as pd\nimport altair as alt\nfrom matplotlib import pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#functions",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#functions",
    "title": "NFL Salaries",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#main",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#main",
    "title": "NFL Salaries",
    "section": "Main",
    "text": "Main\n\nCache the data\n\nnfl_salary_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob'\n         + '/master/data/2018/2018-04-09/nfl_salary.xlsx?raw=true'),\n    fname='nfl_salary.xlsx'\n)\n\n\n\nLoad the data\n\nnfl_salary = pl.DataFrame(pd.read_excel(nfl_salary_path)).lazy()\nnfl_salary.schema\n\n{'year': polars.datatypes.Int64,\n 'Cornerback': polars.datatypes.Int64,\n 'Defensive Lineman': polars.datatypes.Int64,\n 'Linebacker': polars.datatypes.Int64,\n 'Offensive Lineman': polars.datatypes.Int64,\n 'Quarterback': polars.datatypes.Float64,\n 'Running Back': polars.datatypes.Int64,\n 'Safety': polars.datatypes.Int64,\n 'Special Teamer': polars.datatypes.Float64,\n 'Tight End': polars.datatypes.Int64,\n 'Wide Receiver': polars.datatypes.Int64}\n\n\n\n\nPrepare the data\nWe remove high outliers, those players with a salary greater than $30M.\n\nlazy_query = nfl_salary.melt(\n    id_vars='year',\n    variable_name='position',\n    value_name='salary'\n).with_column(\n    pl.col('salary').cast(int)\n).drop_nulls(\n).sort(\n    by=['position', 'year', 'salary'],\n    reverse=[False, False, True]\n).filter(\n    pl.col('salary') < 30_000_000\n).groupby(\n    by=['position', 'year'],\n    maintain_order=True\n).head(\n    16\n)\nlazy_query.schema\n\n{'position': polars.datatypes.Utf8,\n 'year': polars.datatypes.Int64,\n 'salary': polars.datatypes.Int64}\n\n\n\n\nVisualise the data\n\n_ch = alt.Chart(\n        lazy_query.collect().to_pandas()\n).properties(\n    width=110,\n    height=200\n)\n_line = _ch.mark_line(\n    color='darkorange'\n).encode(\n    x=alt.X('year:N', title=''),\n    y=alt.Y('mean(salary)', title='Average cap value ($USD)'),\n)\n_scatter = _ch.mark_circle(color='lightgrey').encode(\n    x='year:N',\n    y='salary'\n)\nalt.layer(_scatter, _line).facet(\n    facet=alt.Facet('position', title=''),\n    columns=5,\n    title=alt.TitleParams(\n        'The average pay for top running backs has stalled',\n        subtitle=('Average cap value of 16 highest-paid players in each '\n                  + ' position'),\n        fontSize=18\n    )\n)\n\n\n\n\n\n\n\n_gsource = lazy_query.with_columns(\n    [pl.col('salary').sum().over(['position', 'year']).alias('total_by_pos'),\n     pl.col('salary').sum().over(['year']).alias('total')]\n).with_column(\n    (pl.col('total_by_pos') / pl.col('total')).alias('prop_by_pos')\n).with_column(\n    (pl.col('prop_by_pos') * 100).round(1).alias('pct_by_pos')\n).select(\n    ['position',\n     'year',\n     'pct_by_pos']\n).unique(\n)\n\n_ch = alt.Chart(\n        _gsource.collect().to_pandas()\n).properties(\n    width=600,\n    height=400,\n    title=alt.TitleParams(\n        'Teams are spending less on RBs',\n        subtitle=('Percent of money spent on the top 16 players at each'\n                  + ' position'),\n        anchor='start',\n        fontSize=18\n    )\n)\n_line = _ch.mark_line(color='darkorange').encode(\n    x=alt.X('year:N', title=''),\n    y=alt.Y('pct_by_pos', title='Percent spent at each position'),\n    color='position'\n)\n_scatter = _ch.mark_circle(color='lightgrey').encode(\n    x='year:N',\n    y='pct_by_pos',\n    color='position'\n)\n_scatter + _line\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nrequests  : 2.28.1\npolars    : 0.14.6\npandas    : 1.4.3\nsys       : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\naltair    : 4.2.0\nmatplotlib: 3.5.3\nseaborn   : 0.11.2"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html",
    "href": "posts/2022-09-08-queue_adt.html",
    "title": "Queue ADT",
    "section": "",
    "text": "Using Python’s deque class as an implementation of the Queue ADT. A queue is:\n\n[A]n ordered collection of items where the addition of new items happens at one end, called the “rear,” and the removal of existing items occurs at the other end, commonly called the “front.” As an element enters the queue it starts at the rear and makes its way toward the front, waiting until that time when it is the next element to be removed.\nThe most recently added item in the queue must wait at the end of the collection. The item that has been in the collection the longest is at the front. This ordering principle is sometimes called FIFO, first-in first-out. It is also known as “first-come first-served.”\nWhat is a Queue? (Problem Solving with Algorithms and Data Structures using Python)\n\nIn this example, we take the left-hand side of the deque as the front of the queue, and the right-hand side as the back.\nThe table below shows the common operations of a queue. We take the left-hand side of the deque as the front of the queue, and the right-hand side as the back.\n\n\n\n\n\n\n\n\noperation\ndescription\npython\n\n\n\n\nnew\nInitialise an empty stack\ndeque()\n\n\nsize\nReturn the number of items in the queue\nlen(q)\n\n\nis empty\nReturn if the queue contains no items\nlen(q) == 0\n\n\nenqueue\nAdd an item to the back of the queue\nq.append(x)\n\n\nfront\nReturn the item at the front of the queue\nq[0]\n\n\nback\nReturn the item at the back of the queue\nq[-1]\n\n\ndequeue\nRemove and return the item at the front of the queue\nq.popleft()\n\n\n\nWe showed one example of how a Queue can be used: Perform a level-first traversal of a binary tree.[^1]\nThe function collect_by_level returns the values of the nodes of a binary tree as a level-order list.\nBelow is the algorithm we used to implement the level-first traveral. Queue operations are emphasised so they can be easily identified.\n\nlet q be a new queue\nlet collection be an empty list\nenqueue(q, tree)\nwhile not is empty(q)\n\nlet node = front(q)\nif node is not an empty node:\n\nappend value(node) to collection\nenqueue(q, left(node))\nenqueue(q, right(node))\n\ndequeue(q)"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#dependencies",
    "href": "posts/2022-09-08-queue_adt.html#dependencies",
    "title": "Queue ADT",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import deque\nimport binarytree as bt"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#functions",
    "href": "posts/2022-09-08-queue_adt.html#functions",
    "title": "Queue ADT",
    "section": "Functions",
    "text": "Functions\n\ndef collect_by_level(tree: bt.Node) -> list:\n    \"\"\"Return root and its descendant nodes as a list in a level-first\n    order.\n    \"\"\"\n    q = deque()\n    collection = []\n    q.append(tree)\n    while len(q) >= 1:\n        node = q[0]\n        if node is not None:\n            collection.append(node.val)\n            q.append(node.left)\n            q.append(node.right)\n        q.popleft()\n\n    return collection"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#main",
    "href": "posts/2022-09-08-queue_adt.html#main",
    "title": "Queue ADT",
    "section": "Main",
    "text": "Main\n\nTesting\n\nN = 10\narange = lambda n: [x for x in range(n)]\nassert all(\n    collect_by_level(bt.build2(list(range(n)))) == list(range(n))\n    for n in range(N)\n)\nprint('Testing complete')\n\nTesting complete\n\n\n\n\nExample use\nPopulate a binary tree with letters ‘a’ through ‘g’.\n\nletter_tree = bt.build2([chr(x) for x in range(97, 104)])\nprint(letter_tree)\n\n\n    __a__\n   /     \\\n  b       c\n / \\     / \\\nd   e   f   g\n\n\n\nPrint letter_tree in level-first order.\n\ncollect_by_level(letter_tree)\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\n\n\n\nPerformance\nThe performance tests show a doubling in collect_by_level’s time-to-run, as the |tree| doubles. It has a linear complexity.\n\nsizes = [100, 200, 400, 800]\nfor i, n in enumerate(sizes):\n    print(f'Test {i+1} (n={n}) =')\n    root = bt.build2(list(range(n)))\n    %timeit -r 3 -n 3000 collect_by_level(root)\n\nTest 1 (n=100) =\n30.7 µs ± 524 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 2 (n=200) =\n58.6 µs ± 564 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 3 (n=400) =\n120 µs ± 707 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 4 (n=800) =\n231 µs ± 1.16 µs per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nbinarytree: 6.5.1\nsys       : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html",
    "title": "Advent of Code 2015, Day 4",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 4: The Ideal Stocking Stuffer."
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#dependencies",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#dependencies",
    "title": "Advent of Code 2015, Day 4",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport hashlib"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#functions",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#functions",
    "title": "Advent of Code 2015, Day 4",
    "section": "Functions",
    "text": "Functions\n\ndef mine_advent_coins(key: str, n: int) -> int:\n    \"\"\"Return the suffix needed so md5(key + suffix).hex begins with\n    n_zeroes.\n    \"\"\"\n    def hex_prefix(suffix, n) -> str:\n        return hashlib.md5(f'{(key + str(suffix))}'.encode()).hexdigest()[:n]\n\n    suffix = 1\n    prefix, target_prefix =  hex_prefix(suffix, n), '0' * n\n    while prefix != target_prefix:\n        suffix += 1\n        prefix = hex_prefix(suffix, n)\n\n    return suffix"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#main",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#main",
    "title": "Advent of Code 2015, Day 4",
    "section": "Main",
    "text": "Main\n\nInitialise the input\n\nKEY = 'ckczppom'\n\n\n\nPart 1\n\nprint(f'Solution = {mine_advent_coins(KEY, 5)}')\n\nSolution = 117946\n\n\n\n\nPart 2\n\nprint(f'Solution = {mine_advent_coins(KEY, 6)}')\n\nSolution = 3938038\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit mine_advent_coins(KEY, 5)\nprint('Part 2 =')\n%timeit mine_advent_coins(KEY, 6)\n\nPart 1 =\n145 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nPart 2 =\n4.91 s ± 34.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html",
    "title": "Global Causes of Mortality",
    "section": "",
    "text": "Visualising the share of deaths by different causes by country.\n\n\n#r4ds presents Week 3 of #TidyTuesday! Let's explore global causes of mortality!Make a meaningful graphic, and post your code!Data: https://t.co/ygKv8PqOfIArticle: https://t.co/MOnlCBzdaLBlog: https://t.co/cZJ94Hhz7U #tidyverse #rstats #dataviz #ggplot2 @R4DScommunity pic.twitter.com/52rktsOcSQ\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 16, 2018"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#dependencies",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#dependencies",
    "title": "Global Causes of Mortality",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport polars as pl\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#functions",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#functions",
    "title": "Global Causes of Mortality",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#main",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#main",
    "title": "Global Causes of Mortality",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\ngmortality_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-16/global_mortality.xlsx?raw=true'),\n    fname='global_mortality.xlsx'\n)\n\n\niso_path = cache_file(\n    url=('https://raw.githubusercontent.com/lukes/'\n         + 'ISO-3166-Countries-with-Regional-Codes/master/all/all.csv'),\n    fname='iso_3166.csv'\n)\n\n\n\nLoad the data\n\nmortality = pl.DataFrame(pd.read_excel(gmortality_path)).lazy()\nmortality.schema\n\n{'country': polars.datatypes.Utf8,\n 'country_code': polars.datatypes.Utf8,\n 'year': polars.datatypes.Int64,\n 'Cardiovascular diseases (%)': polars.datatypes.Float64,\n 'Cancers (%)': polars.datatypes.Float64,\n 'Respiratory diseases (%)': polars.datatypes.Float64,\n 'Diabetes (%)': polars.datatypes.Float64,\n 'Dementia (%)': polars.datatypes.Float64,\n 'Lower respiratory infections (%)': polars.datatypes.Float64,\n 'Neonatal deaths (%)': polars.datatypes.Float64,\n 'Diarrheal diseases (%)': polars.datatypes.Float64,\n 'Road accidents (%)': polars.datatypes.Float64,\n 'Liver disease (%)': polars.datatypes.Float64,\n 'Tuberculosis (%)': polars.datatypes.Float64,\n 'Kidney disease (%)': polars.datatypes.Float64,\n 'Digestive diseases (%)': polars.datatypes.Float64,\n 'HIV/AIDS (%)': polars.datatypes.Float64,\n 'Suicide (%)': polars.datatypes.Float64,\n 'Malaria (%)': polars.datatypes.Float64,\n 'Homicide (%)': polars.datatypes.Float64,\n 'Nutritional deficiencies (%)': polars.datatypes.Float64,\n 'Meningitis (%)': polars.datatypes.Float64,\n 'Protein-energy malnutrition (%)': polars.datatypes.Float64,\n 'Drowning (%)': polars.datatypes.Float64,\n 'Maternal deaths (%)': polars.datatypes.Float64,\n 'Parkinson disease (%)': polars.datatypes.Float64,\n 'Alcohol disorders (%)': polars.datatypes.Float64,\n 'Intestinal infectious diseases (%)': polars.datatypes.Float64,\n 'Drug disorders (%)': polars.datatypes.Float64,\n 'Hepatitis (%)': polars.datatypes.Float64,\n 'Fire (%)': polars.datatypes.Float64,\n 'Heat-related (hot and cold exposure) (%)': polars.datatypes.Float64,\n 'Natural disasters (%)': polars.datatypes.Float64,\n 'Conflict (%)': polars.datatypes.Float64,\n 'Terrorism (%)': polars.datatypes.Float64}\n\n\n\niso = pl.DataFrame(pd.read_csv(iso_path)).lazy()\niso.schema\n\n{'name': polars.datatypes.Utf8,\n 'alpha-2': polars.datatypes.Utf8,\n 'alpha-3': polars.datatypes.Utf8,\n 'country-code': polars.datatypes.Int64,\n 'iso_3166-2': polars.datatypes.Utf8,\n 'region': polars.datatypes.Utf8,\n 'sub-region': polars.datatypes.Utf8,\n 'intermediate-region': polars.datatypes.Utf8,\n 'region-code': polars.datatypes.Float64,\n 'sub-region-code': polars.datatypes.Float64,\n 'intermediate-region-code': polars.datatypes.Float64}\n\n\n\ncountries = alt.topo_feature(vdata.world_110m.url, 'countries')\n\n\n\nVisualise the data\n\n_query = mortality.filter(\n    (pl.col('country') == 'Vanuatu')\n    & (pl.col('year') == 2003)\n).melt(\n    id_vars=['country', 'country_code', 'year'],\n    variable_name='cause',\n    value_name='share (%)'\n).with_columns(\n    [pl.col('cause').str.replace(r' (%)', '', literal=True),\n     pl.col('share (%)').round(2)]\n)\n\n_ch = alt.Chart(\n    _query.collect().to_pandas()\n).encode(\n    x='share (%):Q',\n    y=alt.Y('cause:N', sort='-x'),\n)\n_bars = _ch.mark_bar().encode(\n    color=alt.Color('cause:N', legend=None)\n)\n_text = _ch.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='share (%):Q',\n)\n(_bars + _text).properties(\n    width=500,\n    title='Share of deaths by cause, Vanuatu, 2003'\n).configure_title(\n    fontSize=20,\n    anchor='start'\n)\n\n\n\n\n\n\n\n_query = mortality.join(\n    other=iso,\n    left_on='country_code',\n    right_on='alpha-3'\n).filter(\n    (pl.col('year') == 2016)\n    & (pl.col('region') == 'Europe')\n).select(\n    ['country',\n     pl.col('country-code').alias('country_id'),\n     pl.col('Cardiovascular diseases (%)').round(2).alias('share (%)')]\n).collect(\n).to_pandas(\n)\n\nalt.Chart(\n    _query\n).mark_geoshape(\n    stroke='black'  # adds country borders\n).encode(\n    shape='geo:G',\n    color=alt.Color('share (%):Q', scale=alt.Scale(scheme=\"reds\")),\n    tooltip=['country:N',\n             'share (%):Q']\n).transform_lookup(\n    lookup='country_id',\n    from_=alt.LookupData(data=countries, key='id'),\n    as_='geo'\n).project(\n    type='mercator',\n    scale=415,\n    center=[15, 54],\n    clipExtent=[[0, 0], [600, 400]],\n).properties(\n    width=650,\n    height=400,\n    title=('Percentage share of deaths by Cardiovascular diseases'\n           + ' in Europe, 2016')\n)\n\n\n\n\n\n\nDue to an issue with plotting world maps in Altair1, we used a different kernel to prevent conflicts with dependencies."
  }
]