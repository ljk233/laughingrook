[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaughingRook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nAug 30, 2022\n\n\nCollege Tuition Fees in the USA\n\n\nTidyTuesday\n\n\n\n\nAug 29, 2022\n\n\nMatched Case-Control Studies\n\n\nM249,Statistics\n\n\n\n\nAug 27, 2022\n\n\nAdvent of Code 2015, Day 2\n\n\nAdventOfCode\n\n\n\n\nAug 25, 2022\n\n\nTravelling Salesperson Problem (Brute-force Search)\n\n\nAlgorithm,Graph\n\n\n\n\nAug 22, 2022\n\n\nStratified Analyses\n\n\nM249,Statistics\n\n\n\n\nAug 20, 2022\n\n\nAdvent of Code 2015, Day 1\n\n\nAdventOfCode\n\n\n\n\nAug 18, 2022\n\n\nInitialising an Undirected Graph in NetworkX\n\n\nGraph\n\n\n\n\nAug 15, 2022\n\n\nCohort and Case-Control Studies\n\n\nM249,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-15-cohort_case_control_studies.html",
    "href": "posts/2022-08-15-cohort_case_control_studies.html",
    "title": "Cohort and Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a cohort study analysing the possible association between compulsory redundancies and incidents of serious self-inflicted injury (SSI) (Keefe, V., et al (2002)). The exposure is being made compulsorily redundant, and the disease is incidents of serious self-inflicted injury.\nThe study results were as follows.\n\n\n\n\nSSI (+)\nno SSI (-)\n\n\n\n\nmade redundant (+)\n14\n1931\n\n\nnot made redundant (-)\n4\n1763\n\n\n\nThe analysis was undertaken using StatsModels class and SciPy.\nThe data was stored on remotely, so we defined a function cache_file to handle the retrieval of the file.\nThe exposure and disease labels were stored as two lists, with variables named exposures and diseases. The order of the elements is important: Table2x21 requires the data to be some sequence with shape (2, 2):\n\n\n\n\ndisease (+)\nno diease (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nAs such, the labels should be ordered:\n\nexposures = (exposed, not exposed)\ndiseases = (disease, no disease)\n\nThe data were cached and used to a initialise a Pandas DataFrame.\nThe DataFrame was exported as a Numpy NDArray with shape (2, 2) and used to initialise an instance of Table2x2.\nMeasures of association2 were calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. We rounded-off the analysis by performing Fisher’s exact test.3 4\nNote, some of the results were outputted as Pandas Series, rather than using the default return type. This was done done to provide a more standardised results output.\nThese topics are covered by M249 Book 1, Part 1."
  },
  {
    "objectID": "posts/2022-08-15-cohort_case_control_studies.html#dependencies",
    "href": "posts/2022-08-15-cohort_case_control_studies.html#dependencies",
    "title": "Cohort and Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-15-cohort_case_control_studies.html#functions",
    "href": "posts/2022-08-15-cohort_case_control_studies.html#functions",
    "title": "Cohort and Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname, and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-15-cohort_case_control_studies.html#main",
    "href": "posts/2022-08-15-cohort_case_control_studies.html#main",
    "title": "Cohort and Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nexposures = np.array(['redundant', 'not redundant'])\ndiseases = np.array(['ssi', 'no ssi'])\n\n\n\nCache the data\n\nlocal_path = cache_file(\n    url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n         + '/main/m249/medical/redundancy_ssi.csv'),\n    fname='redundancy_ssi.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(local_path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   disease   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      disease\n    \n  \n  \n    \n      0\n      14\n      redundant\n      ssi\n    \n    \n      1\n      1931\n      redundant\n      no ssi\n    \n    \n      2\n      4\n      not redundant\n      ssi\n    \n    \n      3\n      1763\n      not redundant\n      no ssi\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns exposure, disease as ordered Categorical variables to ensure that the data will be sorted as expected.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    disease=pd.Categorical(data['disease'], diseases, ordered=True)\n).sort_values(\n    by=['exposure', 'disease']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   disease   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\nOutput a pivot table with the marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='disease',\n    aggfunc='sum',\n    margins=True,\n    margins_name='subtotal'\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n      subtotal\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      redundant\n      14\n      1931\n      1945\n    \n    \n      not redundant\n      4\n      1763\n      1767\n    \n    \n      subtotal\n      18\n      3694\n      3712\n    \n  \n\n\n\n\n\n\nInitialise the contingency table\nGet the data as a NumPy ndarray with shape (2, 2,)\n\ndata_arr = cat_data['n'].to_numpy().reshape((2, 2))\n\nInitialise an instance of Table2x2 using data_arr.\n\nctable = sm.stats.Table2x2(data_arr)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[  14. 1931.]\n [   4. 1763.]]\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    3.195495\nlcb      1.049877\nucb      9.726081\nName: odds ratio, dtype: float64\n\n\nReturn point and interval estimates for the relative risk.\n\npd.Series(\n    data=[ctable.riskratio,\n          ctable.riskratio_confint()[0],\n          ctable.riskratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='relative risk'\n)\n\npoint    3.179692\nlcb      1.048602\nucb      9.641829\nName: relative risk, dtype: float64\n\n\n\n\nChi-squared test for no association\nReturn the expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[   9.43157328, 1935.56842672],\n       [   8.56842672, 1758.43157328]])\n\n\nReturn the differences between the observed and expected frequencies.\n\ndata_arr - ctable.fittedvalues\n\narray([[ 4.56842672, -4.56842672],\n       [-4.56842672,  4.56842672]])\n\n\nReturn the contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[2.21283577, 0.01078263],\n       [2.43574736, 0.01186883]])\n\n\nReturn the results of the chi-squared test.5\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    4.671235\npvalue      0.030672\ndf                 1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.033877\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-15-cohort_case_control_studies.html#references",
    "href": "posts/2022-08-15-cohort_case_control_studies.html#references",
    "title": "Cohort and Case-Control Studies",
    "section": "References",
    "text": "References\nVera Keefe, Papaarangi Reid, Clint Ormsby, Bridget Robson, Gordon Purdie, Joanne Baxter, Ngäti Kahungunu Iwi Incorporated, Serious health events following involuntary job loss in New Zealand meat processing workers, International Journal of Epidemiology, Volume 31, Issue 6, December 2002, Pages 1155–1161, https://doi.org/10.1093/ije/31.6.1155\n\n%load_ext watermark\n%watermark --iv\n\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnumpy      : 1.23.2\nstatsmodels: 0.13.2\nrequests   : 2.28.1\nscipy      : 1.9.0\npandas     : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "",
    "text": "Initialise and populate an undirected weighted graph using NetworkX.\nThe source data is a CSV file listing the road network in Europe as an edge list.1\nWe first imported the data into a Pandas DataFrame.2 Next, we exported the DataFrame as a collection of dictionaries.3 We initialised an empty graph,4 and populated it. We closed the notebook by showing how to access the nodes, neighbors, and edges of the graph.\nWhilst we could populate the graph during initialisation, we found it added unneeded complexity.\nThe |edges| ≠ |edge list| because NetworkX’s Graph class does not permit parallel edges between two nodes.5"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom dataclasses import dataclass\nimport pandas as pd\nimport networkx as nx\n\n\nClasses\n\n@dataclass(frozen=True)\nclass PandasERoad:\n    \"\"\"A dataclass to help the conversion of the raod network data as a\n    graph.\n\n    Stores the remote url and maps the column titles to common graph\n    terminology\n    \"\"\"\n\n    url: str = ('https://raw.githubusercontent.com/ljk233'\n                + '/laughingrook-datasets/main/graphs/eroads_edge_list.csv')\n    u: str = 'origin_reference_place'\n    v: str = 'destination_reference_place'\n    uco: str = 'origin_country_code'\n    vco: str = 'destination_country_code'\n    w: str = 'distance'\n    rn: str = 'road_number'\n    wc: str = 'watercrossing'"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Functions",
    "text": "Functions\n\ndef edge_to_tuple(edge: dict, er: PandasERoad) -> tuple:\n    return (\n        edge[er.u],\n        edge[er.v],\n        {'weight': edge[er.w], er.rn: edge[er.rn], er.wc: edge[er.wc]}\n    )"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Main",
    "text": "Main\n\ner = PandasERoad()\n\n\nImport the data\n\neroads = pd.read_csv(er.url)\neroads.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 7 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   road_number                  1250 non-null   object\n 1   origin_country_code          1250 non-null   object\n 2   origin_reference_place       1250 non-null   object\n 3   destination_country_code     1250 non-null   object\n 4   destination_reference_place  1250 non-null   object\n 5   distance                     1250 non-null   int64 \n 6   watercrossing                1250 non-null   bool  \ndtypes: bool(1), int64(1), object(5)\nmemory usage: 59.9+ KB\n\n\n\n\nExport data to a dictionary\nEach entry in the list is a dictionary representing a single row, where the keys are the column titles.\n\nedges = eroads.to_dict(orient='records')\nedges[0]\n\n{'road_number': 'E01',\n 'origin_country_code': 'GB',\n 'origin_reference_place': 'Larne',\n 'destination_country_code': 'GB',\n 'destination_reference_place': 'Belfast',\n 'distance': 36,\n 'watercrossing': False}\n\n\n\n\nInitalise and populate the graph\n\ng = nx.Graph()\n\nAdds the nodes. We perform it on both the source and destination nodes in the edge list to ensure we populate all the cities, given there’s a chance that a city does not appear as a source city in the data.\n\ng.add_nodes_from((e[er.u], {'country': e[er.uco]}) for e in edges)\ng.add_nodes_from((e[er.v], {'country': e[er.vco]}) for e in edges)\n\nAdd the edges. Given this is an undirected graph, there is no need to add the reverse edges V → U.\nExample of output from edge_to_tuple\n\nedge_to_tuple(edges[0], er)\n\n('Larne',\n 'Belfast',\n {'weight': 36, 'road_number': 'E01', 'watercrossing': False})\n\n\nPopulate the edges.\n\ng.add_edges_from(edge_to_tuple(edge, er) for edge in edges)\n\n\n\nInspect the graph\nGet a description of the graph.\n\nprint(g)\n\nGraph with 894 nodes and 1198 edges\n\n\nGet a selection of the nodes.\n\n[n for n in g][:5]\n\n['Larne', 'Belfast', 'Dublin', 'Wexford', 'Rosslare']\n\n\nOutput a more descriptive list of nodes by calling the nodes() method.\n\n[n for n in g.nodes(data=True)][:5]\n\n[('Larne', {'country': 'GB'}),\n ('Belfast', {'country': 'GB'}),\n ('Dublin', {'country': 'IRL'}),\n ('Wexford', {'country': 'IRL'}),\n ('Rosslare', {'country': 'IRL'})]\n\n\nView the neighbours of the Roma node.\n\n[neighbor for neighbor in g['Roma']]\n\n['Arezzo', 'Grosseto', 'Pescara', 'San Cesareo']\n\n\nWe can get a more descriptive output of a node’s neighbours by not using list comprehension.\n\ng['Roma']\n\nAtlasView({'Arezzo': {'weight': 219, 'road_number': 'E35', 'watercrossing': False}, 'Grosseto': {'weight': 182, 'road_number': 'E80', 'watercrossing': False}, 'Pescara': {'weight': 209, 'road_number': 'E80', 'watercrossing': False}, 'San Cesareo': {'weight': 36, 'road_number': 'E821', 'watercrossing': False}})\n\n\nFinally, we can simply output the edges of the Roma node.\n\n[e for e in g.edges('Roma', data=True)]\n\n[('Roma',\n  'Arezzo',\n  {'weight': 219, 'road_number': 'E35', 'watercrossing': False}),\n ('Roma',\n  'Grosseto',\n  {'weight': 182, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'Pescara',\n  {'weight': 209, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'San Cesareo',\n  {'weight': 36, 'road_number': 'E821', 'watercrossing': False})]\n\n\n\n%load_ext watermark\n%watermark --iversions\n\nnetworkx: 2.8.6\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\npandas  : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "title": "Advent of Code 2015, Day 1",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 1: Not Quite Lisp."
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "title": "Advent of Code 2015, Day 1",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom itertools import accumulate\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "title": "Advent of Code 2015, Day 1",
    "section": "Functions",
    "text": "Functions\n\ndef find_first(x, A) -> int:\n    \"\"\"Find the first index i where A[i] = x.\n\n    Precondtions:\n    - x in A\n    - A is 1-dimensional\n    - A support iteration\n    \"\"\"\n    return next(i for i, a in enumerate(A) if a == x)"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "title": "Advent of Code 2015, Day 1",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 1)\nprint(f\"line = '{line[:5]}'\")\n\nline = '()((('\n\n\n\n\nTransform the input\n\nm = {'(': 1, ')': -1}\ndirections = [m[bracket] for bracket in line]\nprint(f\"directions = {directions[:5]}\")\n\ndirections = [1, -1, 1, 1, 1]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(directions)}\")\n\nSolution = 138\n\n\n\n\nPart 2\n\nprint(f\"Solution = {find_first(-1, accumulate(directions)) + 1}\")\n\nSolution = 1771\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(directions)\nprint('Part 2 =')\n%timeit find_first(-1, accumulate(directions)) + 1\n\nPart 1 =\n62.5 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nPart 2 =\n73.6 µs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-22-stratified_analyses.html",
    "href": "posts/2022-08-22-stratified_analyses.html",
    "title": "Stratified Analyses",
    "section": "",
    "text": "Perform a stratified analyses on the results of a stratified case-control study.\nData were taken from investigating the possible association between alcohol consumption and fatal car accidents in New York (J.R. McCarroll and W. Haddon Jr, 1962). The data were stratified by marital status, which was believed to be a possible confounder. The exposure was blood alcohol level of 100mg% or greater; Cases were drivers who were killed in car accidents for which they were considered to be responsible; and controls were selected drivers passing the locations where the accidents of the cases occurred, at the same time of day and on the same day of the week.\nThe results were as follows.\nLevel: Married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n4\n5\n\n\nunder 100mg% (-)\n5\n103\n\n\n\nLevel: Not married\n\n\n\n\nfatality (+)\nno fatality (-)\n\n\n\n\nover 100mg% (+)\n10\n3\n\n\nunder 100mg% (-)\n5\n43\n\n\n\nThe analysis was undertaken using StatsModels.\nThe data was stored on remotely, so we defined a function cache_file to handle the retrieval of the file.\nThe main difficulty with a stratified analysis is that we need to deal with the data in many different shapes, so we defined two functions that return the data in the needed shapes. Two further functions were defined to handle a pieces of analysis.1\nThe level, exposure, and case-control labels were stored as three lists, with variables named levels,2 exposures, and casecons. The order of the elements in exposures and diseases lists is important: Table2x23 requires the data to be some sequence with shape (2, 2):\n\n\n\n\ndisease (+)\nno diease (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nAnd StratifiedTable4 requires the data to be some sequence with shape (2, 2, 2):\n\n\n\nlevel\nexposure\ndisease (+)\nno diease (-)\n\n\n\n\none\nexposed (+)\na\nb\n\n\n\nnot exposed (-)\nc\nd\n\n\ntwo\nexposed (+)\ne\nf\n\n\n\nnot exposed (-)\ng\nh\n\n\n\nAs such, the labels should be ordered:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\nThe data were cached and used to a initialise a Pandas DataFrame.\nThe main difficulty with a stratified analysis is that we need to deal with the data in many different shapes. Three variables were initialised to hold the different views of the data. These were:\n\nlevel_tables, a collection of Table2x2 instances, where each instance represents a contingency table for a specific level\naggregated_table, an instance of Table2x2 using the aggregated data\n\nAggregated by exposure, casecon\n\nstrat_table, an instance of StratifiedTable\n\nMeasures of association were calculated:\n\nLevel-specific odds ratios\nUnadjusted odds ratios5\nAdjusted odds ratios6\n\nAnd two hypothesis tets were performed:\n\nTest of no association\nTest of homogeneity\n\nThese topics are covered in M249, Book 1, Part 2."
  },
  {
    "objectID": "posts/2022-08-22-stratified_analyses.html#dependencies",
    "href": "posts/2022-08-22-stratified_analyses.html#dependencies",
    "title": "Stratified Analyses",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nfrom dataclasses import dataclass\nimport numpy as np\nimport pandas as pd\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-22-stratified_analyses.html#functions",
    "href": "posts/2022-08-22-stratified_analyses.html#functions",
    "title": "Stratified Analyses",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-22-stratified_analyses.html#main",
    "href": "posts/2022-08-22-stratified_analyses.html#main",
    "title": "Stratified Analyses",
    "section": "Main",
    "text": "Main\n\nInitialise the labels\n\nlevels = np.array(['married', 'not married'])\nexposures = np.array(['over 100mg', 'under 100mg'])\ncasecons = np.array(['fatality', 'no fatality'])\n\n\n\nCache the data\n\npath = cache_file(\n        url=('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n             + '/main/m249/medical/drinkdriving.csv'),\n        fname='drinkdriving.csv'\n)\n\n\n\nLoad the data\nUse the cached file to initialise a DataFrame.\n\ndata = pd.read_csv(path)\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8 entries, 0 to 7\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         8 non-null      int64 \n 1   level     8 non-null      object\n 2   exposure  8 non-null      object\n 3   casecon   8 non-null      object\ndtypes: int64(1), object(3)\nmemory usage: 384.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      level\n      exposure\n      casecon\n    \n  \n  \n    \n      0\n      4\n      married\n      over 100mg\n      fatality\n    \n    \n      1\n      5\n      married\n      over 100mg\n      no fatality\n    \n    \n      2\n      5\n      married\n      under 100mg\n      fatality\n    \n    \n      3\n      103\n      married\n      under 100mg\n      no fatality\n    \n    \n      4\n      10\n      not married\n      over 100mg\n      fatality\n    \n    \n      5\n      3\n      not married\n      over 100mg\n      no fatality\n    \n    \n      6\n      5\n      not married\n      under 100mg\n      fatality\n    \n    \n      7\n      43\n      not married\n      under 100mg\n      no fatality\n    \n  \n\n\n\n\n\n\nPrepare the data\nInitialise a new DataFrame using data, with the columns level, exposure, disease as ordered Categorical variables to ensure that the data will be sorted as expected.\n\ncat_data = pd.DataFrame().assign(\n    n=data['n'].to_numpy(),\n    level=pd.Categorical(data['level'], levels, ordered=True),\n    exposure=pd.Categorical(data['exposure'], exposures, ordered=True),\n    casecon=pd.Categorical(data['casecon'], casecons, ordered=True)\n).sort_values(\n    by=['level', 'exposure', 'casecon']\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 8 entries, 0 to 7\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         8 non-null      int64   \n 1   level     8 non-null      category\n 2   exposure  8 non-null      category\n 3   casecon   8 non-null      category\ndtypes: category(3), int64(1)\nmemory usage: 524.0 bytes\n\n\nOutput pivot tables with marginal totals.\n\nfor level in levels:\n    print(\n        cat_data.query(\n            'level == @level'\n        ).pivot_table(\n            values='n',\n            index=['level', 'exposure'],\n            columns='casecon',\n            aggfunc='sum',\n            margins=True,\n            margins_name='subtotal'\n        ).query(\n            \"level in [@level, 'subtotal']\"\n        ),\n        '\\n'\n    )\n\ncasecon               fatality  no fatality  subtotal\nlevel    exposure                                    \nmarried  over 100mg          4            5       9.0\n         under 100mg         5          103     108.0\nsubtotal                     9          108     117.0 \n\ncasecon                  fatality  no fatality  subtotal\nlevel       exposure                                    \nnot married over 100mg         10            3      13.0\n            under 100mg         5           43      48.0\nsubtotal                       15           46      61.0 \n\n\n\n\n\nInitialise the contingency tables\nInitialise contingency tables for different views of the data.\nVariable level_tables is collection of level-specific 2x2 contingency tables.\n\n_get_table = lambda df, level: (\n    sm.stats.Table2x2(\n        df.query('level == @level')['n'].to_numpy().reshape((2, 2))\n    )\n)\nlevel_tables = [_get_table(cat_data, level) for level in levels]\nfor table, level in zip(level_tables, levels):\n    print(f'level={level}\\n{table}\\n')\n\nlevel=married\nA 2x2 contingency table with counts:\n[[  4.   5.]\n [  5. 103.]]\n\nlevel=not married\nA 2x2 contingency table with counts:\n[[10.  3.]\n [ 5. 43.]]\n\n\n\nVariable aggregated_table is a 2x2 contingency table of the aggregated data.\n\naggregated_table = sm.stats.Table2x2(\n    cat_data.groupby(['exposure', 'casecon'])\n            .sum()\n            .to_numpy()\n            .reshape((2, 2))\n)\nprint(aggregated_table)\n\nA 2x2 contingency table with counts:\n[[ 14.   8.]\n [ 10. 146.]]\n\n\nVariable strat_table is a 2x2x2 contingency table that is stratified by level.\n\nstrat_table = (\n    sm.stats.StratifiedTable(\n                data.sort_values(by=['exposure', 'casecon', 'level'])['n']\n                    .to_numpy()\n                    .reshape((2, 2, 2))\n    )\n)\nprint(strat_table.table)\n\n[[[  4.  10.]\n  [  5.   3.]]\n\n [[  5.   5.]\n  [103.  43.]]]\n\n\n\n\nOdds ratios\nReturn point and interval estimates of the level-specific, unadjusted, and adjusted odds ratios.\nReturn the level-specific odds ratios.\n\n_get_measure = lambda table: (\n    np.array([table.oddsratio,\n              table.oddsratio_confint()[0],\n              table.oddsratio_confint()[1]]\n    )\n)\npd.DataFrame(\n    data=[_get_measure(table).round(5) for table in level_tables],\n    columns=['point', 'lcb', 'ucb'],\n    index=pd.Index(levels, name='level')\n)\n\n\n\n\n\n  \n    \n      \n      point\n      lcb\n      ucb\n    \n    \n      level\n      \n      \n      \n    \n  \n  \n    \n      married\n      16.48000\n      3.35421\n      80.96998\n    \n    \n      not married\n      28.66667\n      5.85662\n      140.31607\n    \n  \n\n\n\n\nReturn the unadjusted odds ratio.\n\npd.Series(\n    data=[aggregated_table.oddsratio,\n          aggregated_table.oddsratio_confint()[0],\n          aggregated_table.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='unadjusted odds ratio'\n).round(\n    5\n)\n\npoint    25.55000\nlcb       8.68217\nucb      75.18883\nName: unadjusted odds ratio, dtype: float64\n\n\nReturn the adjusted odds ratio.\n\npd.Series(\n    data=[strat_table.oddsratio_pooled,\n           strat_table.oddsratio_pooled_confint()[0],\n           strat_table.oddsratio_pooled_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='adjusted odds ratio'\n).round(\n    3\n)\n\npoint    23.001\nlcb       7.465\nucb      70.866\nName: adjusted odds ratio, dtype: float64\n\n\n\n\nHypothesis testing\nReturn the results of a test of no association and test of homogeneity.\n\n_r = strat_table.test_null_odds(correction=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of no association'\n).round(\n    3\n)\n\nstatistic    36.604\npvalue        0.000\nName: test of no association, dtype: float64\n\n\n\n_r = strat_table.test_equal_odds(adjust=True)\npd.Series(\n    data=[_r.statistic, _r.pvalue],\n    index=['statistic', 'pvalue'],\n    name='test of homogeneity'\n).round(\n    3\n)\n\nstatistic    0.236\npvalue       0.627\nName: test of homogeneity, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-22-stratified_analyses.html#references",
    "href": "posts/2022-08-22-stratified_analyses.html#references",
    "title": "Stratified Analyses",
    "section": "References",
    "text": "References\nMcCarroll, J.R. and Haddon Jr, W., 1962. A controlled study of fatal automobile accidents in New York City. Journal of chronic diseases, 15(8), pp.811-826.\n\n%load_ext watermark\n%watermark --iv\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nrequests   : 2.28.1\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\npandas     : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html",
    "href": "posts/2022-08-25-tsp_bruteforce.html",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "",
    "text": "A solution to the Travelling Salesperson Problem using a brute-force search.\nThe Travelling Salesperson Problem is defined as:\n\nGiven a set of cities and distances between every pair of cities, the [Travelling Salesperson] problem is to find the shortest possible route that visits every city exactly once and returns to the starting point.\nTraveling Salesman Problem (TSP) Implementation (GeeksForGeeks)\n\nIn this implementation, we generate permutations and check if the |path| < |min path|.\nWe use a complete undirected graph, with each edge being assigned a random weight (representing the distance).\nWhilst the function works, it is unusuable when |nodes(g)| ≥ 11, given P(11, 11) = 36720000 permutations to check!"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "href": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport random as rand\nimport math\nimport itertools as it\nimport networkx as nx\n%load_ext watermark"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#function",
    "href": "posts/2022-08-25-tsp_bruteforce.html#function",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Function",
    "text": "Function\n\ndef bruteforce_tsp(G: nx.Graph, start: object) -> float | int:\n    \"\"\"Return the shortest route that visits every city exactly once and\n    ends back at the start.\n\n    Solves the travelling salesperson with a brute-force search using\n    permutations.\n\n    Preconditions:\n    - G is a complete weighted graph\n    - start in G\n    - WG[u, v]['weight'] is the distance u -> v\n    \"\"\"\n    neighbours = set((node for node in G.nodes if node != start))\n    min_dist = math.inf\n    for path in it.permutations(neighbours):\n        u, dist = start, 0\n        for v in path:\n            dist += G.edges[u, v]['weight']\n            u = v\n        min_dist = min(min_dist, dist + G.edges[u, start]['weight'])\n\n    return min_dist"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#main",
    "href": "posts/2022-08-25-tsp_bruteforce.html#main",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Main",
    "text": "Main\n\nInitialise the graph\nWe initialise a complete weighted undirected graph with 5 nodes.\n\ncg = nx.complete_graph(['origin', 'a', 'b', 'c', 'd'])\ng = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\nprint(f\"g = {g}\")\n\ng = Graph with 5 nodes and 10 edges\n\n\n\n\nFind the shortest path from the origin\n\nprint(f\"Shortest path from the origin = {bruteforce_tsp(g, 'origin')}\")\n\nShortest path from the origin = 22\n\n\n\n\nPerformance\n\nfor n in [4, 6, 8, 10]:\n    print(f\"|nodes(g)| = {n}\")\n    cg = nx.complete_graph(n)\n    g = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\n    %timeit bruteforce_tsp(g, 1)\n\n|nodes(g)| = 4\n8 µs ± 40.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n|nodes(g)| = 6\n202 µs ± 532 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n|nodes(g)| = 8\n10.9 ms ± 38.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n|nodes(g)| = 10\n960 ms ± 7.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nprint('kernel\\t: lr_compsci')\n%watermark --iversions\n\nkernel  : lr_compsci\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnetworkx: 2.8.6"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "title": "Advent of Code 2015, Day 2",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 2: I Was Told There Would Be No Math."
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "title": "Advent of Code 2015, Day 2",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "title": "Advent of Code 2015, Day 2",
    "section": "Functions",
    "text": "Functions\n\ndef paper_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the wrapping paper needed for a present with dims (l, w, h).\n    \"\"\"\n    planes = [(length * width), (width * height), (height * length)]\n    return min(planes) + (2 * sum(planes))\n\n\ndef ribbon_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the ribbon needed to cover a present with dims (l, w, h).\n    \"\"\"\n    bow = length * width * height\n    ribbon = 2 * sum(sorted([length, width, height])[:2])\n    return bow + ribbon"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "title": "Advent of Code 2015, Day 2",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nlines = lr.datasets.get_advent_input(2015, 2)\nprint(f\"lines = {lines[:5]}\")\n\nfile was cached.\nlines = ['4x23x21', '22x29x19', '11x4x11', '8x10x5', '24x18x16']\n\n\n\n\nTransform the input\n\nllines = (line.split('x') for line in lines)\npdims = [[int(x) for x in lline] for lline in llines]\nprint(f\"pdims = {pdims[:5]}\")\n\npdims = [[4, 23, 21], [22, 29, 19], [11, 4, 11], [8, 10, 5], [24, 18, 16]]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(paper_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 1598415\n\n\n\n\nPart 2\n\nprint(f\"Solution = {sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 3812909\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(paper_needed(l, w, h) for (l, w, h) in pdims)\nprint('Part 2 =')\n%timeit sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)\n\nPart 1 =\n433 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPart 2 =\n468 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html",
    "title": "Matched Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a 1-1 matched case-control study.\nData was taken from a case-control study undertaken to identify some of the riskfactors associated with death during heatwave in Chicago that occured from 12 July 1995 to 16 July 1995. Cases were persons aged 24+ years who died between 14-17 July 1995, with a cause mentioned on the death certificate that was possibly heat related. For each case, a matched control was selected of the same age and living in the same neighbourhood. The risk factor of interest was participation in group activities involving social interactions. (Semenza, Rubin, Falter, et al (1996))\nThe results were as follows.\n\n\n\n\n\n\n\n\nCases / Control\nParticipated (+)\nDid not participate (-)\n\n\n\n\nParticipated (+)\n77\n63\n\n\nDid not participate (-)\n90\n74\n\n\n\nA function was defined to return the Mantel-Haenszel odds ratio. The results were initialised as a NumPy array.1 A Mantel-Haenszel odds ratio for the association between participation in group activities and dying of heat-related disease was calculated, including a 95% confidence interval estimate. Finally, McNemar’s test2 was performed to test the null hypothesis of no association between participation in group activities and dying of heat-related disease.\nThese topics are covered by M249 Book 1, Part 2."
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html#dependencies",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html#dependencies",
    "title": "Matched Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm\nfrom numpy.typing import ArrayLike"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html#constants",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html#constants",
    "title": "Matched Case-Control Studies",
    "section": "Constants",
    "text": "Constants\nThese are the results from the study.\n\nOBS = np.array([[77, 63], [90, 74]])"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html#functions",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html#functions",
    "title": "Matched Case-Control Studies",
    "section": "Functions",
    "text": "Functions\n\ndef mh_odds_ratio(ctable: ArrayLike, alpha: float = 0.05) -> tuple:\n    \"\"\"Return point and 100(1-alpha)% intervel estimates of the\n    Mantel-Haenszel odds ratio.\n\n    Pre-conditions:\n    - arr represents the results of a 1-1 matched case-control study\n        - shape(arr) = 2, 2\n        - rows represent cases, columns represent controls\n        - row 0, col 0 represent (+)\n        - row 1, col 1 represent (-)\n    - 0 < alpha < 1\n\n    Post-conditions:\n    - tuple of float estimates, (point, lcb, ucb)\n    \"\"\"\n\n    f, g = ctable[0, 1], ctable[1, 0]\n    ste = (st.norm.ppf(1 - (alpha/2)) * np.sqrt(1/f + 1/g))\n    return (\n        f / g,\n        f / g * np.exp(-ste),\n        f / g * np.exp(ste)\n    )"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html#main",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html#main",
    "title": "Matched Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nMatel-Haenszel odds ratio\n\nres = mh_odds_ratio(OBS)\npd.Series(\n    data=[res[0], res[1], res[2]],\n    index=['point', 'lcb', 'ucb'],\n    name='mantel-haenszel odds ratio'\n)\n\npoint    0.700000\nlcb      0.507309\nucb      0.965881\nName: mantel-haenszel odds ratio, dtype: float64\n\n\n\n\nMcNemar’s test\n\nres = sm.stats.mcnemar(OBS, exact=False)\npd.Series(\n    data=[res.statistic, res.pvalue],\n    index=['statistc', 'pvalue'],\n    name='mcnemar''s test'\n)\n\nstatistc    4.418301\npvalue      0.035555\nName: mcnemars test, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p2_matched_case_control.html#references",
    "href": "posts/2022-08-29-m249_b1p2_matched_case_control.html#references",
    "title": "Matched Case-Control Studies",
    "section": "References",
    "text": "References\nSemenza, J. C., Rubin, C. H., Falter, K. H., Selanikio, J. D., Flanders, W. D., Howe, H. L., & Wilhelm, J. L. (1996). Heat-related deaths during the July 1995 heat wave in Chicago. New England journal of medicine, 335, 84-90. https://doi.org/10.1056/NEJM199607113350203.\n\n%load_ext watermark\n%watermark --iversions\n\nscipy      : 1.9.0\npandas     : 1.4.3\nsys        : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnumpy      : 1.23.2\nstatsmodels: 0.13.2"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "title": "College Tuition Fees in the USA",
    "section": "",
    "text": "Visualising the average annual cost of College tuition fees in the USA.\n\n\nHappy to announce the newest #R4DS online learning community project! #TidyTuesday is your weekly #tidyverse practice!Each week we'll post data and a plot at https://t.co/8NaXR93uIX under the datasets link.You clean the data and tweak the plot in R!#rstats #ggplot2 pic.twitter.com/sDaHsB8uwL\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 2, 2018"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "title": "College Tuition Fees in the USA",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport pandas as pd\nimport polars as pl\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "title": "College Tuition Fees in the USA",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "title": "College Tuition Fees in the USA",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\nus_avg_tuition_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-02/us_avg_tuition.xlsx?raw=true'),\n   fname='us_avg_tuition.xlsx'\n)\n\n\nansi_path = cache_file(\n    url=('https://www2.census.gov/geo/docs/reference/state.txt'),\n    fname='state.txt'\n)\n\n\n\nLoad the data\n\nus_avg_tuition = pl.DataFrame(pd.read_excel(us_avg_tuition_path)).lazy()\nansi = pl.DataFrame(pd.read_csv(ansi_path, sep='|')).lazy()\nstates = alt.topo_feature(vdata.us_10m.url, 'states')\n\n\n\nPrepare the data\n\nlazy_query = us_avg_tuition.select(\n    ['State',\n     '2010-11',\n     '2015-16']\n).melt(\n    id_vars='State',\n    variable_name='year',\n    value_name='tuition'\n).join(\n    other=ansi,\n    left_on='State',\n    right_on='STATE_NAME',\n    how='inner'\n).with_column(\n    pl.col('tuition').pct_change().over('State').alias('pct_change')\n).filter(\n    pl.col('pct_change').is_not_null()\n).select(\n    [pl.col('STATE').alias('state_id'),\n     pl.col('State').alias('state_name'),\n     pl.col('tuition'),\n     pl.col('pct_change').apply(lambda x: x * 100).round(1)]\n)\nlazy_query.schema\n\n{'state_id': polars.datatypes.Int64,\n 'state_name': polars.datatypes.Utf8,\n 'tuition': polars.datatypes.Float64,\n 'pct_change': polars.datatypes.Float64}\n\n\n\n\nVisualise the data\nBoth visualisations will use the same source, so we initialise a single instance of alt.Chart.\n\nch = alt.Chart(lazy_query.collect().to_pandas())\n\nPlot the percentage change in college annual tuition costs from 2010 to 2015 as a chloropleth heatmap.\n\nch.mark_geoshape(\n    stroke='black'\n).encode(\n    shape='geo:G',\n    color=alt.Color(\n                \"pct_change\",\n                scale=alt.Scale(scheme=\"oranges\"),\n                legend=alt.Legend(title='Change (%)')\n    ),\n    tooltip=[alt.Tooltip('state_name', title='State'),\n             alt.Tooltip('pct_change', title='Change (%)')]\n).transform_lookup(\n    lookup='state_id',\n    from_=alt.LookupData(data=states, key='id'),\n    as_='geo'\n).project(\n    type='albersUsa'\n).properties(\n    title='Percentage change in college tuition costs between 2010 and 2015',\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nPlot the annual cost of college tuition in the USA in 2015-16 as a dot plot.\n\nch.mark_circle(\n    size=60\n).encode(\n    x=alt.X('tuition', title='Tuition ($)'),\n    y=alt.Y('state_name', sort='-x', axis=alt.Axis(grid=True), title='State')\n).properties(\n    title='College tuition costs in the USA (2015-16)',\n    width=400,\n    height=600\n)\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iversions\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\npolars  : 0.14.6\npandas  : 1.4.3\naltair  : 4.2.0\nrequests: 2.28.1"
  }
]