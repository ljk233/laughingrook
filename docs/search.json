[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaughingRook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 13, 2022\n\n\nGlobal Causes of Mortality\n\n\nTidyTuesday\n\n\n\n\nSep 12, 2022\n\n\nSetting Up a Time Series With Pandas\n\n\nM249,Statistics,TimeSeries\n\n\n\n\nSep 10, 2022\n\n\nAdvent of Code 2015, Day 4\n\n\nAdventOfCode\n\n\n\n\nSep 8, 2022\n\n\nQueue ADT\n\n\nAbstractDataType\n\n\n\n\nSep 6, 2022\n\n\nNFL Salaries\n\n\nTidyTuesday\n\n\n\n\nSep 3, 2022\n\n\nAdvent of Code 2015, Day 3\n\n\nAdventOfCode\n\n\n\n\nSep 1, 2022\n\n\nStack ADT\n\n\nAbstractDataType,Algorithm\n\n\n\n\nAug 30, 2022\n\n\nCollege Tuition Fees in the USA\n\n\nTidyTuesday\n\n\n\n\nAug 29, 2022\n\n\nCohort Studies With Three or More Exposures\n\n\nM249,Statistics\n\n\n\n\nAug 27, 2022\n\n\nAdvent of Code 2015, Day 2\n\n\nAdventOfCode\n\n\n\n\nAug 25, 2022\n\n\nTravelling Salesperson Problem (Brute-force Search)\n\n\nAlgorithm,Graph\n\n\n\n\nAug 22, 2022\n\n\nCase-Control Studies With Two Exposures\n\n\nM249,Statistics\n\n\n\n\nAug 20, 2022\n\n\nAdvent of Code 2015, Day 1\n\n\nAdventOfCode\n\n\n\n\nAug 18, 2022\n\n\nInitialising an Undirected Graph in NetworkX\n\n\nGraph\n\n\n\n\nAug 15, 2022\n\n\nCohort Studies With Two Exposures\n\n\nM249,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html",
    "title": "Cohort Studies With Two Exposures",
    "section": "",
    "text": "Perform an epidemiological analysis on the results of a cohort study with two categories of exposure.\n\n\n\nA cohort study has been undertaken, and the summarised results have been stored in a tidy manner, with the following schema.\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ndisease\nstr\nDescriptive label indicating category of disease\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: We use singulars rather plurals for the column titles.\n\n\nThe data were taken from a cohort study analysing the possible association between compulsory redundancies and incidents of serious self-inflicted injury (SSI) (Keefe, V., et al (2002)).\n\n\n\nWe use StatsModels’s Table2x2 to perform much of the analysis, which expects the data as a sequence with shape (2, 2). This sequence should mirror the following table, where a, b, c, d are integers.\n\n\n\nexposure/disease\ndisease (+)\nno disease (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nThe primary issue in this collection of recipes is ensuring that the data is ordered as needed to perform the analysis. We cannot guarentee that the data will meet this criteria when we come to reshape it, so we solve this by casting exposure, disease to ordered pd.Categorical data types."
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#dependencies",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#dependencies",
    "title": "Cohort Studies With Two Exposures",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#main",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#main",
    "title": "Cohort Studies With Two Exposures",
    "section": "Main",
    "text": "Main\n\nLoad the data\nInitialise data, a DataFrame. Output a view of its schema and the data.\n\ndata = pd.read_csv(\n            'https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n            + '/main/m249/medical/redundancy_ssi.csv'\n)\n\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   disease   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      disease\n    \n  \n  \n    \n      0\n      14\n      redundant\n      ssi\n    \n    \n      1\n      1931\n      redundant\n      no ssi\n    \n    \n      2\n      4\n      not redundant\n      ssi\n    \n    \n      3\n      1763\n      not redundant\n      no ssi\n    \n  \n\n\n\n\n\n\nPrepare the data\n\nInitialise the categories\nInitialise exposures, diseases as ordered pd.CategoricalDTypes, where:\n\nexposures = (exposed, not exposed)\ndiseases = (diseased, not diseased)\n\n\nexposures = pd.CategoricalDtype(\n    categories=['redundant', 'not redundant'],\n    ordered=True\n)\ndiseases = pd.CategoricalDtype(\n    categories=['ssi', 'no ssi'],\n    ordered=True\n)\n\n\n\nInitialise a categorical DataFrame\nInitialise cat_data, a DataFrame, using data with the columns exposure, disease casted to ordered Categorical variables. Sort cat_data by exposure, disease.\n\ncat_data = (\n    pd.DataFrame(\n    ).assign(\n        n=data['n'].to_numpy(),\n        exposure=data['exposure'].astype(exposures),\n        disease=data['disease'].astype(diseases)\n    ).sort_values(\n        by=['exposure', 'disease']\n    )\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   disease   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\n\n\nInitialise a 2x2 contingency table\nInitialise ctable, a Table2x2.\n\nctable = sm.stats.Table2x2(\n    cat_data['n'].to_numpy().reshape(2, 2)\n)\n\nConfirm that the contingency table initialised as expected.\n\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[  14. 1931.]\n [   4. 1763.]]\n\n\n\n\n\nAnalyse the data\n\nOutput a table with row marginal totals\nCohort studies require only the row marginal totals, so we filter out the column marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='disease',\n    aggfunc='sum',\n    margins=True,\n    margins_name='total'\n).query(\n    \"exposure != 'total'\"\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n      total\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      redundant\n      14\n      1931\n      1945\n    \n    \n      not redundant\n      4\n      1763\n      1767\n    \n  \n\n\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    3.195495\nlcb      1.049877\nucb      9.726081\nName: odds ratio, dtype: float64\n\n\nReturn point and interval estimates of the relative risk.\n\npd.Series(\n    data=[ctable.riskratio,\n          ctable.riskratio_confint()[0],\n          ctable.riskratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='relative risk'\n)\n\npoint    3.179692\nlcb      1.048602\nucb      9.641829\nName: relative risk, dtype: float64\n\n\n\n\nChi-squared test of no association\nReturn the expected frequencies under the null hypothesis of no association.\n\npd.DataFrame(\n    data=ctable.fittedvalues,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n).assign(\n    total=lambda df: df.sum(axis=1).astype(int)\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n      total\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      redundant\n      9.431573\n      1935.568427\n      1944\n    \n    \n      not redundant\n      8.568427\n      1758.431573\n      1767\n    \n  \n\n\n\n\nReturn the differences between the observed and expected frequencies.\n\npd.DataFrame(\n    data=ctable.table - ctable.fittedvalues,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      redundant\n      4.568427\n      -4.568427\n    \n    \n      not redundant\n      -4.568427\n      4.568427\n    \n  \n\n\n\n\nReturn the contributions to the chi-squared test statistic.\n\npd.DataFrame(\n    data=ctable.chi2_contribs,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      ssi\n      no ssi\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      redundant\n      2.212836\n      0.010783\n    \n    \n      not redundant\n      2.435747\n      0.011869\n    \n  \n\n\n\n\nReturn the results of the chi-squared test.\n\n\n\n\n\n\nNote\n\n\n\nWe pass the argument dtype=object when initialising the Series, so it can handle both float and int data types.\n\n\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    4.671235\npvalue      0.030672\ndf                 1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n\n\n\n\n\nNote\n\n\n\nThere is no implementation of Fisher’s exact test is StatsModels, so we use SciPy’s implementation.\n\n\n\npd.Series(\n    data=st.fisher_exact(ctable.table)[1],\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.033877\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#references",
    "href": "posts/2022-08-15-m249_b1p1i_cohort_studies_two_exposures.html#references",
    "title": "Cohort Studies With Two Exposures",
    "section": "References",
    "text": "References\nVera Keefe, Papaarangi Reid, Clint Ormsby, Bridget Robson, Gordon Purdie, Joanne Baxter, Ngäti Kahungunu Iwi Incorporated, Serious health events following involuntary job loss in New Zealand meat processing workers, International Journal of Epidemiology, Volume 31, Issue 6, December 2002, Pages 1155–1161, https://doi.org/10.1093/ije/31.6.1155\n\n%load_ext watermark\n%watermark --iv\n\nstatsmodels: 0.13.2\nnumpy      : 1.23.2\nscipy      : 1.9.0\npandas     : 1.4.3"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "",
    "text": "Initialise and populate an undirected weighted graph using NetworkX.\nThe source data is a CSV file listing the road network in Europe as an edge list.1\nWe first imported the data into a Pandas DataFrame.2 Next, we exported the DataFrame as a collection of dictionaries.3 We initialised an empty graph,4 and populated it. We closed the notebook by showing how to access the nodes, neighbors, and edges of the graph.\nWhilst we could populate the graph during initialisation, we found it added unneeded complexity.\nThe |edges| ≠ |edge list| because NetworkX’s Graph class does not permit parallel edges between two nodes.5"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#dependencies",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom dataclasses import dataclass\nimport pandas as pd\nimport networkx as nx\n\n\nClasses\n\n@dataclass(frozen=True)\nclass PandasERoad:\n    \"\"\"A dataclass to help the conversion of the raod network data as a\n    graph.\n\n    Stores the remote url and maps the column titles to common graph\n    terminology\n    \"\"\"\n\n    url: str = ('https://raw.githubusercontent.com/ljk233'\n                + '/laughingrook-datasets/main/graphs/eroads_edge_list.csv')\n    u: str = 'origin_reference_place'\n    v: str = 'destination_reference_place'\n    uco: str = 'origin_country_code'\n    vco: str = 'destination_country_code'\n    w: str = 'distance'\n    rn: str = 'road_number'\n    wc: str = 'watercrossing'"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#functions",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Functions",
    "text": "Functions\n\ndef edge_to_tuple(edge: dict, er: PandasERoad) -> tuple:\n    return (\n        edge[er.u],\n        edge[er.v],\n        {'weight': edge[er.w], er.rn: edge[er.rn], er.wc: edge[er.wc]}\n    )"
  },
  {
    "objectID": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "href": "posts/2022-08-18-initialise_undirected_graph_networkx.html#main",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Main",
    "text": "Main\n\ner = PandasERoad()\n\n\nImport the data\n\neroads = pd.read_csv(er.url)\neroads.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 7 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   road_number                  1250 non-null   object\n 1   origin_country_code          1250 non-null   object\n 2   origin_reference_place       1250 non-null   object\n 3   destination_country_code     1250 non-null   object\n 4   destination_reference_place  1250 non-null   object\n 5   distance                     1250 non-null   int64 \n 6   watercrossing                1250 non-null   bool  \ndtypes: bool(1), int64(1), object(5)\nmemory usage: 59.9+ KB\n\n\n\n\nExport data to a dictionary\nEach entry in the list is a dictionary representing a single row, where the keys are the column titles.\n\nedges = eroads.to_dict(orient='records')\nedges[0]\n\n{'road_number': 'E01',\n 'origin_country_code': 'GB',\n 'origin_reference_place': 'Larne',\n 'destination_country_code': 'GB',\n 'destination_reference_place': 'Belfast',\n 'distance': 36,\n 'watercrossing': False}\n\n\n\n\nInitalise and populate the graph\n\ng = nx.Graph()\n\nAdds the nodes. We perform it on both the source and destination nodes in the edge list to ensure we populate all the cities, given there’s a chance that a city does not appear as a source city in the data.\n\ng.add_nodes_from((e[er.u], {'country': e[er.uco]}) for e in edges)\ng.add_nodes_from((e[er.v], {'country': e[er.vco]}) for e in edges)\n\nAdd the edges. Given this is an undirected graph, there is no need to add the reverse edges V → U.\nExample of output from edge_to_tuple\n\nedge_to_tuple(edges[0], er)\n\n('Larne',\n 'Belfast',\n {'weight': 36, 'road_number': 'E01', 'watercrossing': False})\n\n\nPopulate the edges.\n\ng.add_edges_from(edge_to_tuple(edge, er) for edge in edges)\n\n\n\nInspect the graph\nGet a description of the graph.\n\nprint(g)\n\nGraph with 894 nodes and 1198 edges\n\n\nGet a selection of the nodes.\n\n[n for n in g][:5]\n\n['Larne', 'Belfast', 'Dublin', 'Wexford', 'Rosslare']\n\n\nOutput a more descriptive list of nodes by calling the nodes() method.\n\n[n for n in g.nodes(data=True)][:5]\n\n[('Larne', {'country': 'GB'}),\n ('Belfast', {'country': 'GB'}),\n ('Dublin', {'country': 'IRL'}),\n ('Wexford', {'country': 'IRL'}),\n ('Rosslare', {'country': 'IRL'})]\n\n\nView the neighbours of the Roma node.\n\n[neighbor for neighbor in g['Roma']]\n\n['Arezzo', 'Grosseto', 'Pescara', 'San Cesareo']\n\n\nWe can get a more descriptive output of a node’s neighbours by not using list comprehension.\n\ng['Roma']\n\nAtlasView({'Arezzo': {'weight': 219, 'road_number': 'E35', 'watercrossing': False}, 'Grosseto': {'weight': 182, 'road_number': 'E80', 'watercrossing': False}, 'Pescara': {'weight': 209, 'road_number': 'E80', 'watercrossing': False}, 'San Cesareo': {'weight': 36, 'road_number': 'E821', 'watercrossing': False}})\n\n\nFinally, we can simply output the edges of the Roma node.\n\n[e for e in g.edges('Roma', data=True)]\n\n[('Roma',\n  'Arezzo',\n  {'weight': 219, 'road_number': 'E35', 'watercrossing': False}),\n ('Roma',\n  'Grosseto',\n  {'weight': 182, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'Pescara',\n  {'weight': 209, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'San Cesareo',\n  {'weight': 36, 'road_number': 'E821', 'watercrossing': False})]\n\n\n\n%load_ext watermark\n%watermark --iversions\n\nnetworkx: 2.8.6\npandas  : 1.4.3\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html",
    "title": "Advent of Code 2015, Day 1",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 1: Not Quite Lisp."
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#dependencies",
    "title": "Advent of Code 2015, Day 1",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom itertools import accumulate\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#functions",
    "title": "Advent of Code 2015, Day 1",
    "section": "Functions",
    "text": "Functions\n\ndef find_first(x, A) -> int:\n    \"\"\"Find the first index i where A[i] = x.\n\n    Precondtions:\n    - x in A\n    - A is 1-dimensional\n    - A support iteration\n    \"\"\"\n    return next(i for i, a in enumerate(A) if a == x)"
  },
  {
    "objectID": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "href": "posts/2022-08-20-advent_of_code_2015_day1.html#main",
    "title": "Advent of Code 2015, Day 1",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 1)\nprint(f\"line = '{line[:5]}'\")\n\nline = '()((('\n\n\n\n\nTransform the input\n\nm = {'(': 1, ')': -1}\ndirections = [m[bracket] for bracket in line]\nprint(f\"directions = {directions[:5]}\")\n\ndirections = [1, -1, 1, 1, 1]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(directions)}\")\n\nSolution = 138\n\n\n\n\nPart 2\n\nprint(f\"Solution = {find_first(-1, accumulate(directions)) + 1}\")\n\nSolution = 1771\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(directions)\nprint('Part 2 =')\n%timeit find_first(-1, accumulate(directions)) + 1\n\nPart 1 =\n62.5 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nPart 2 =\n73.6 µs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html",
    "href": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html",
    "title": "Case-Control Studies With Two Exposures",
    "section": "",
    "text": "Perform an epidemiological analysis on the results of a case-control study with two categories of exposure.\n\n\n\nA case-control study has been undertaken. The results have been stored in a tidy manner, with the following schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ncasecon\nstr\nDescriptive label indicating category of case or control\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: We use singulars rather plurals for the column titles.\n\n\nThe data were taken from a case-control analysing the possible association between political activity and death by homicide was sourced (Mian, A., Mahmood, S.F., et al (2022)).\n\n\n\nWe use StatsModels’s Table2x2 to perform much of the analysis, which expects the data as a sequence with shape (2, 2). This sequence should mirror the following table, where a, b, c, d are integers.\n\n\n\nexposure/disease\ncase (+)\ncontrol (-)\n\n\n\n\nexposed (+)\na\nb\n\n\nnot exposed (-)\nc\nd\n\n\n\nThe primary issue in this collection of recipes is ensuring that the data is ordered as needed to perform the analysis. We cannot guarentee that the data will meet this criteria when we come to reshape it, so we solve this by casting exposure, casecon to ordered pd.Categorical data types."
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#dependencies",
    "href": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#dependencies",
    "title": "Case-Control Studies With Two Exposures",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#main",
    "href": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#main",
    "title": "Case-Control Studies With Two Exposures",
    "section": "Main",
    "text": "Main\n\nLoad the data\nInitialise data, a DataFrame. Output a view of its schema and the data.\n\ndata = pd.read_csv(\n            'https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n            + '/main/m249/medical/karachi.csv'\n)\n\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         4 non-null      int64 \n 1   exposure  4 non-null      object\n 2   casecon   4 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 224.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      casecon\n    \n  \n  \n    \n      0\n      11\n      attended\n      homicide\n    \n    \n      1\n      2\n      attended\n      not homicide\n    \n    \n      2\n      24\n      did not attend\n      homicide\n    \n    \n      3\n      83\n      did not attend\n      not homicide\n    \n  \n\n\n\n\n\n\nPrepare the data\n\nInitialise the categories\nInitialise exposures, casecons as ordered pd.CategoricalDTypes, where:\n\nexposures = (exposed, not exposed)\ncasecons = (case, control)\n\n\nexposures = pd.CategoricalDtype(\n    categories=['attended', 'did not attend'],\n    ordered=True\n)\ncasecons = pd.CategoricalDtype(\n    categories=['homicide', 'not homicide'],\n    ordered=True\n)\n\n\n\nInitialise a categorical DataFrame\nInitialise cat_data, a DataFrame, using data with the columns exposure, casecon as ordered Categorical variables. Sort cat_data by exposure, casecon.\n\ncat_data = (\n    pd.DataFrame(\n    ).assign(\n        n=data['n'].to_numpy(),\n        exposure=data['exposure'].astype(exposures),\n        casecon=data['casecon'].astype(casecons)\n    ).sort_values(\n        by=['exposure', 'casecon']\n    )\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         4 non-null      int64   \n 1   exposure  4 non-null      category\n 2   casecon   4 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 320.0 bytes\n\n\n\n\nInitialise a 2x2 contingency table\nInitialise ctable, a Table2x2.\n\nctable = sm.stats.Table2x2(\n    cat_data['n'].to_numpy().reshape(2, 2)\n)\n\n\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[11.  2.]\n [24. 83.]]\n\n\n\n\n\nAnalyse the data\n\nTable with columnar marginal totals\nCase-control studies require only the columnar marginal totals, so we drop the row marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='casecon',\n    aggfunc='sum',\n    margins=True,\n    margins_name='total'\n).drop(\n    columns='total'\n)\n\n\n\n\n\n  \n    \n      casecon\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      11\n      2\n    \n    \n      did not attend\n      24\n      83\n    \n    \n      total\n      35\n      85\n    \n  \n\n\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\npd.Series(\n    data=[ctable.oddsratio,\n          ctable.oddsratio_confint()[0],\n          ctable.oddsratio_confint()[1]],\n    index=['point', 'lcb', 'ucb'],\n    name='odds ratio'\n)\n\npoint    19.020833\nlcb       3.942873\nucb      91.758497\nName: odds ratio, dtype: float64\n\n\n\n\nChi-squared test of no association\nReturn the expected frequencies under the null hypothesis of no association.\n\npd.DataFrame(\n    data=[*ctable.fittedvalues, ctable.fittedvalues.sum(axis=0)],\n    index=pd.Index([*exposures.categories, 'total'], name='exposure'),\n    columns=pd.Index(casecons.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      3.791667\n      9.208333\n    \n    \n      did not attend\n      31.208333\n      75.791667\n    \n    \n      total\n      35.000000\n      85.000000\n    \n  \n\n\n\n\nReturn the differences between the observed and expected frequencies.\n\npd.DataFrame(\n    data=ctable.table - ctable.fittedvalues,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(casecons.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      7.208333\n      -7.208333\n    \n    \n      did not attend\n      -7.208333\n      7.208333\n    \n  \n\n\n\n\nReturn the contributions to the chi-squared test statistic.\n\npd.DataFrame(\n    data=ctable.chi2_contribs,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(casecons.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      homicide\n      not homicide\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      attended\n      13.703755\n      5.642722\n    \n    \n      did not attend\n      1.664942\n      0.685564\n    \n  \n\n\n\n\nReturn the results of the chi-squared test.\n\n\n\n\n\n\nNote\n\n\n\nWe pass the argument dtype=object when initialising the Series, so it can handle both float and int data types.\n\n\n\n_res = ctable.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    21.696984\npvalue       0.000003\ndf                  1\nName: chi-squared test, dtype: object\n\n\n\n\n\nFisher’s exact test\nReturn the results of Fisher’s exact test.\n\n\n\n\n\n\nNote\n\n\n\nThere is no implementation of Fisher’s exact test is StatsModels, so we use SciPy’s implementation.\n\n\n\n_, _pvalue = st.fisher_exact(ctable.table)\npd.Series(\n    data=_pvalue,\n    index=['pvalue'],\n    name='fisher''s exact'\n)\n\npvalue    0.000018\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#references",
    "href": "posts/2022-08-22-m249_b1p1ii_case_control_studies_two_exposures.html#references",
    "title": "Case-Control Studies With Two Exposures",
    "section": "References",
    "text": "References\nMian, A., Mahmood, S.F., Chotani, H. and Luby, S., 2002. Vulnerability to homicide in Karachi: political activity as a risk factor. International journal of epidemiology, 31(3), 581-585.\n\n%load_ext watermark\n%watermark --iv\n\npandas     : 1.4.3\nnumpy      : 1.23.2\nscipy      : 1.9.0\nstatsmodels: 0.13.2"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html",
    "href": "posts/2022-08-25-tsp_bruteforce.html",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "",
    "text": "A solution to the Travelling Salesperson Problem using a brute-force search.\nThe Travelling Salesperson Problem is defined as:\n\nGiven a set of cities and distances between every pair of cities, the [Travelling Salesperson] problem is to find the shortest possible route that visits every city exactly once and returns to the starting point.\nTraveling Salesman Problem (TSP) Implementation (GeeksForGeeks)\n\nIn this implementation, we generate permutations and check if the |path| < |min path|.\nWe use a complete undirected graph, with each edge being assigned a random weight (representing the distance).\nWhilst the function works, it is unusuable when |nodes(g)| ≥ 11, given P(11, 11) = 36720000 permutations to check!"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "href": "posts/2022-08-25-tsp_bruteforce.html#dependencies",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport random as rand\nimport math\nimport itertools as it\nimport networkx as nx"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#function",
    "href": "posts/2022-08-25-tsp_bruteforce.html#function",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Function",
    "text": "Function\n\ndef bruteforce_tsp(G: nx.Graph, start: object) -> float | int:\n    \"\"\"Return the shortest route that visits every city exactly once and\n    ends back at the start.\n\n    Solves the travelling salesperson with a brute-force search using\n    permutations.\n\n    Preconditions:\n    - G is a complete weighted graph\n    - start in G\n    - WG[u, v]['weight'] is the distance u -> v\n    \"\"\"\n    neighbours = set((node for node in G.nodes if node != start))\n    min_dist = math.inf\n    for path in it.permutations(neighbours):\n        u, dist = start, 0\n        for v in path:\n            dist += G.edges[u, v]['weight']\n            u = v\n        min_dist = min(min_dist, dist + G.edges[u, start]['weight'])\n\n    return min_dist"
  },
  {
    "objectID": "posts/2022-08-25-tsp_bruteforce.html#main",
    "href": "posts/2022-08-25-tsp_bruteforce.html#main",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Main",
    "text": "Main\n\nInitialise the graph\nWe initialise a complete weighted undirected graph with 5 nodes.\n\ncg = nx.complete_graph(['origin', 'a', 'b', 'c', 'd'])\ng = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\nprint(f\"g = {g}\")\n\ng = Graph with 5 nodes and 10 edges\n\n\n\n\nFind the shortest path from the origin\n\nprint(f\"Shortest path from the origin = {bruteforce_tsp(g, 'origin')}\")\n\nShortest path from the origin = 27\n\n\n\n\nPerformance\n\nfor n in [4, 6, 8, 10]:\n    print(f\"|nodes(g)| = {n}\")\n    cg = nx.complete_graph(n)\n    g = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\n    %timeit bruteforce_tsp(g, 1)\n\n|nodes(g)| = 4\n8.19 µs ± 61.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n|nodes(g)| = 6\n206 µs ± 1.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n|nodes(g)| = 8\n11.1 ms ± 84.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n|nodes(g)| = 10\n969 ms ± 6.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\nnetworkx: 2.8.6"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html",
    "title": "Advent of Code 2015, Day 2",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 2: I Was Told There Would Be No Math."
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#dependencies",
    "title": "Advent of Code 2015, Day 2",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#functions",
    "title": "Advent of Code 2015, Day 2",
    "section": "Functions",
    "text": "Functions\n\ndef paper_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the wrapping paper needed for a present with dims (l, w, h).\n    \"\"\"\n    planes = [(length * width), (width * height), (height * length)]\n    return min(planes) + (2 * sum(planes))\n\n\ndef ribbon_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the ribbon needed to cover a present with dims (l, w, h).\n    \"\"\"\n    bow = length * width * height\n    ribbon = 2 * sum(sorted([length, width, height])[:2])\n    return bow + ribbon"
  },
  {
    "objectID": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "href": "posts/2022-08-27-advent_of_code_2015_day2.html#main",
    "title": "Advent of Code 2015, Day 2",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nlines = lr.datasets.get_advent_input(2015, 2)\nprint(f\"lines = {lines[:5]}\")\n\nfile was cached.\nlines = ['4x23x21', '22x29x19', '11x4x11', '8x10x5', '24x18x16']\n\n\n\n\nTransform the input\n\nllines = (line.split('x') for line in lines)\npdims = [[int(x) for x in lline] for lline in llines]\nprint(f\"pdims = {pdims[:5]}\")\n\npdims = [[4, 23, 21], [22, 29, 19], [11, 4, 11], [8, 10, 5], [24, 18, 16]]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(paper_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 1598415\n\n\n\n\nPart 2\n\nprint(f\"Solution = {sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 3812909\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(paper_needed(l, w, h) for (l, w, h) in pdims)\nprint('Part 2 =')\n%timeit sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)\n\nPart 1 =\n433 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPart 2 =\n468 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html",
    "href": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html",
    "title": "Cohort Studies With Three or More Exposures",
    "section": "",
    "text": "Perform an epidemiological analysis on the results of a cohort study with more than categories of exposure.\n\n\n\nA cohort study has been undertaken. The results have been stored in a tidy manner, with the following schema:\n\n\n\n\n\n\n\n\ncolumn\ndtype\ndescription\n\n\n\n\nn\nint\nNumber of observations\n\n\nexposure\nstr\nDescriptive label indicating category of exposure\n\n\ndisease\nstr\nDescriptive label indicating category of disease\n\n\n\nThe data were taken from a cohort study analysing the possible association between gestational age (i.e., the duration of pregnancy) and hospitalisation for asthma. (Yuan, W. et al, 2002)\n\n\n\nThis problem has the same issues that we looked at when solving for cohort studies with two exposures, with an extra layer of complexity. When we have more than two exposures, we need a reference exposure that the other exposures will be compared. We solve this by constructing a sequence of Table2x2s, with each instance comparing a specific exposure with the reference exposure. We also initialise an instance of Table, an Rx2 contingency table, that we will use to perform the chi-squared test of no association.\n\n\n\n\n\n\nWarning\n\n\n\nThis note makes use of “private” lambda functions. There is no reason they could not be “public” functions but, if you decide to make it part of the public API, then please be mindful of the function arguments."
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#dependencies",
    "href": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#dependencies",
    "title": "Cohort Studies With Three or More Exposures",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#main",
    "href": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#main",
    "title": "Cohort Studies With Three or More Exposures",
    "section": "Main",
    "text": "Main\n\nnp.set_printoptions(suppress=True)\n\n\nLoad the data\nInitialise data, a DataFrame. Output a view of its schema and the data.\n\ndata = pd.read_csv(\n            'https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n            + '/main/m249/medical/asthmagest.csv'\n)\n\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6 entries, 0 to 5\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   n         6 non-null      int64 \n 1   exposure  6 non-null      object\n 2   disease   6 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 272.0+ bytes\n\n\n\ndata\n\n\n\n\n\n  \n    \n      \n      n\n      exposure\n      disease\n    \n  \n  \n    \n      0\n      18\n      pre-term\n      hospitalised\n    \n    \n      1\n      266\n      pre-term\n      not hospitalised\n    \n    \n      2\n      402\n      term\n      hospitalised\n    \n    \n      3\n      8565\n      term\n      not hospitalised\n    \n    \n      4\n      45\n      post-term\n      hospitalised\n    \n    \n      5\n      1100\n      post-term\n      not hospitalised\n    \n  \n\n\n\n\n\n\nPrepare the data\n\nInitialise the categories\nInitialise exposures, diseases as ordered pd.CategoricalDTypes, where:\n\nexposures = (exposed1, exposed2, …, reference exposure)\ndiseases = (diseased, not diseased)\n\n\n\n\n\n\n\nWarning\n\n\n\nThe reference exposure must be the final item in exposures, as it will represent no exposure.\n\n\n\nexposures = pd.CategoricalDtype(\n    categories=['pre-term', 'post-term', 'term'],\n    ordered=True\n)\ndiseases = pd.CategoricalDtype(\n    categories=['hospitalised', 'not hospitalised'],\n    ordered=True\n)\n\nInitialise cat_data, a DataFrame, using data with the columns exposure, disease as ordered Categorical variables. Sort cat_data by exposure, disease.\n\ncat_data = (\n    pd.DataFrame(\n    ).assign(\n        n=data['n'].to_numpy(),\n        exposure=data['exposure'].astype(exposures),\n        disease=data['disease'].astype(diseases)\n    ).sort_values(\n        by=['exposure', 'disease']\n    )\n)\ncat_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6 entries, 0 to 3\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   n         6 non-null      int64   \n 1   exposure  6 non-null      category\n 2   disease   6 non-null      category\ndtypes: category(2), int64(1)\nmemory usage: 364.0 bytes\n\n\n\n\nInitialise the collection of 2x2 contingency tables\nInitialise ctables, a sequence of Table2x2s, where each instance of Table2x2 holds one exposure against the reference exposure.\n\n_get_table2x2 = lambda df, item: (\n    sm.stats.Table2x2(\n        df.query('exposure in [@item, @exposures.categories[-1]]')['n']\n          .to_numpy()\n          .reshape(2, 2)\n    )\n)\nctables = [_get_table2x2(cat_data, exp) for exp in exposures.categories[:-1]]\n\nConfirm that the contingency tables initialised as expected.\n\nfor ctable in ctables:\n    print(ctable, '\\n')\n\nA 2x2 contingency table with counts:\n[[  18.  266.]\n [ 402. 8565.]] \n\nA 2x2 contingency table with counts:\n[[  45. 1100.]\n [ 402. 8565.]] \n\n\n\n\n\nInitialise a Rx2 contingency table\nInitialise table, a Table with shape (Rx2), where R is the number of exposure categories in the data.\n\nThis is used for the chi-squared test of no association.\n\n\ntable = sm.stats.Table(\n    cat_data['n'].to_numpy().reshape(len(exposures.categories), 2)\n)\n\nConfirm that the table initialised as expected.\n\nprint(table)\n\nA 3x2 contingency table with counts:\n[[  18.  266.]\n [  45. 1100.]\n [ 402. 8565.]]\n\n\n\n\n\nAnalyse the data\n\nOutput a table with row marginal totals\nCohort studies require only the row marginal totals, so we filter out the column marginal totals.\n\ncat_data.pivot_table(\n    values='n',\n    index='exposure',\n    columns='disease',\n    aggfunc='sum',\n    margins=True,\n    margins_name='total'\n).query(\n    \"exposure != 'total'\"\n)\n\n\n\n\n\n  \n    \n      disease\n      hospitalised\n      not hospitalised\n      total\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      pre-term\n      18\n      266\n      284\n    \n    \n      post-term\n      45\n      1100\n      1145\n    \n    \n      term\n      402\n      8565\n      8967\n    \n  \n\n\n\n\n\n\nMeasures of association\nReturn point and interval estimates of the odds ratio.\n\n_collect_odds_ratio = lambda ct: (\n    [ct.oddsratio, ct.oddsratio_confint()[0], ct.oddsratio_confint()[1]]\n)\npd.DataFrame(\n    data=[_collect_odds_ratio(ct) for ct in ctables],\n    index=pd.Index(exposures.categories[:-1], name='exposure'),\n    columns=pd.Index(['point', 'lcb', 'ucb'], name='odds ratio')\n)\n\n\n\n\n\n  \n    \n      odds ratio\n      point\n      lcb\n      ucb\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      pre-term\n      1.441757\n      0.885284\n      2.348019\n    \n    \n      post-term\n      0.871608\n      0.636456\n      1.193641\n    \n  \n\n\n\n\nReturn point and interval estimates of the relative risk.\n\n_collect_rel_risk = lambda ct: (\n    [ct.riskratio, ct.riskratio_confint()[0], ct.riskratio_confint()[1]]\n)\npd.DataFrame(\n    data=[_collect_rel_risk(ct) for ct in ctables],\n    index=pd.Index(exposures.categories[:-1], name='exposure'),\n    columns=pd.Index(['point', 'lcb', 'ucb'], name='relative risk')\n)\n\n\n\n\n\n  \n    \n      relative risk\n      point\n      lcb\n      ucb\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      pre-term\n      1.413759\n      0.895001\n      2.233197\n    \n    \n      post-term\n      0.876654\n      0.648214\n      1.185599\n    \n  \n\n\n\n\n\n\nChi-squared test of no association\nReturn the expected frequencies under the null hypothesis of no association.\n\npd.DataFrame(\n    data=table.fittedvalues,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n).assign(\n    total=lambda df: df.sum(axis=1).astype(int)\n)\n\n\n\n\n\n  \n    \n      disease\n      hospitalised\n      not hospitalised\n      total\n    \n    \n      exposure\n      \n      \n      \n    \n  \n  \n    \n      pre-term\n      12.702963\n      271.297037\n      284\n    \n    \n      post-term\n      51.214409\n      1093.785591\n      1144\n    \n    \n      term\n      401.082628\n      8565.917372\n      8966\n    \n  \n\n\n\n\nReturn the differences between the observed and expected frequencies.\n\npd.DataFrame(\n    data=table.table - table.fittedvalues,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      hospitalised\n      not hospitalised\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      pre-term\n      5.297037\n      -5.297037\n    \n    \n      post-term\n      -6.214409\n      6.214409\n    \n    \n      term\n      0.917372\n      -0.917372\n    \n  \n\n\n\n\nReturn the contributions to the chi-squared test statistic.\n\npd.DataFrame(\n    data=table.chi2_contribs,\n    index=pd.Index(exposures.categories, name='exposure'),\n    columns=pd.Index(diseases.categories, name='disease')\n)\n\n\n\n\n\n  \n    \n      disease\n      hospitalised\n      not hospitalised\n    \n    \n      exposure\n      \n      \n    \n  \n  \n    \n      pre-term\n      2.208824\n      0.103424\n    \n    \n      post-term\n      0.754063\n      0.035308\n    \n    \n      term\n      0.002098\n      0.000098\n    \n  \n\n\n\n\nReturn the results of the chi-squared test.\n\n\n\n\n\n\nNote\n\n\n\nWe pass the argument dtype=object when initialising the Series, so it can handle both float and int data types.\n\n\n\n_res = table.test_nominal_association()\npd.Series(\n    data=[_res.statistic, _res.pvalue, int(_res.df)],\n    index=['statistc', 'pvalue', 'df'],\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistc    3.103814\npvalue      0.211844\ndf                 2\nName: chi-squared test, dtype: object"
  },
  {
    "objectID": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#references",
    "href": "posts/2022-08-29-m249_b1p1iii_cohort_studies_n_exposures.html#references",
    "title": "Cohort Studies With Three or More Exposures",
    "section": "References",
    "text": "References\nYuan, W., Basso, O., Sorensen, H.T. and Olsen, J. (2002) Fetal growth and hospitalization with asthma during early childhood: a follow-up study in Denmark. International Journal of Epidemiology, 31, 1240–1245.\n\n%load_ext watermark\n%watermark --iv\n\nstatsmodels: 0.13.2\nscipy      : 1.9.0\npandas     : 1.4.3\nnumpy      : 1.23.2"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html",
    "title": "College Tuition Fees in the USA",
    "section": "",
    "text": "Visualising the average annual cost of College tuition fees in the USA.\n\n\nHappy to announce the newest #R4DS online learning community project! #TidyTuesday is your weekly #tidyverse practice!Each week we'll post data and a plot at https://t.co/8NaXR93uIX under the datasets link.You clean the data and tweak the plot in R!#rstats #ggplot2 pic.twitter.com/sDaHsB8uwL\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 2, 2018"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#dependencies",
    "title": "College Tuition Fees in the USA",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport pandas as pd\nimport polars as pl\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#functions",
    "title": "College Tuition Fees in the USA",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "href": "posts/2022-08-30-tt_y2018_w01_us_avg_tuition.html#main",
    "title": "College Tuition Fees in the USA",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\nus_avg_tuition_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-02/us_avg_tuition.xlsx?raw=true'),\n    fname='us_avg_tuition.xlsx'\n)\n\n\nansi_path = cache_file(\n    url=('https://www2.census.gov/geo/docs/reference/state.txt'),\n    fname='state.txt'\n)\n\n\n\nLoad the data\n\nus_avg_tuition = pl.DataFrame(pd.read_excel(us_avg_tuition_path)).lazy()\nus_avg_tuition.schema\n\n{'State': polars.datatypes.Utf8,\n '2004-05': polars.datatypes.Float64,\n '2005-06': polars.datatypes.Float64,\n '2006-07': polars.datatypes.Float64,\n '  2007-08 ': polars.datatypes.Float64,\n '2008-09': polars.datatypes.Float64,\n '2009-10': polars.datatypes.Float64,\n '2010-11': polars.datatypes.Float64,\n '2011-12': polars.datatypes.Float64,\n '2012-13': polars.datatypes.Float64,\n '2013-14': polars.datatypes.Float64,\n '2014-15': polars.datatypes.Float64,\n '2015-16': polars.datatypes.Float64}\n\n\n\nansi = pl.DataFrame(pd.read_csv(ansi_path, sep='|')).lazy()\nansi.schema\n\n{'STATE': polars.datatypes.Int64,\n 'STUSAB': polars.datatypes.Utf8,\n 'STATE_NAME': polars.datatypes.Utf8,\n 'STATENS': polars.datatypes.Int64}\n\n\n\nstates = alt.topo_feature(vdata.us_10m.url, 'states')\n\n\n\nPrepare the data\n\nlazy_query = us_avg_tuition.select(\n    ['State',\n     '2010-11',\n     '2015-16']\n).melt(\n    id_vars='State',\n    variable_name='year',\n    value_name='tuition'\n).join(\n    other=ansi,\n    left_on='State',\n    right_on='STATE_NAME',\n    how='inner'\n).with_column(\n    pl.col('tuition').pct_change().over('State').alias('pct_change')\n).filter(\n    pl.col('pct_change').is_not_null()\n).select(\n    [pl.col('STATE').alias('state_id'),\n     pl.col('State').alias('state_name'),\n     pl.col('tuition'),\n     pl.col('pct_change').apply(lambda x: x * 100).round(1)]\n)\nlazy_query.schema\n\n{'state_id': polars.datatypes.Int64,\n 'state_name': polars.datatypes.Utf8,\n 'tuition': polars.datatypes.Float64,\n 'pct_change': polars.datatypes.Float64}\n\n\n\n\nVisualise the data\nBoth visualisations will use the same source, so we initialise a single instance of alt.Chart.\n\nch = alt.Chart(lazy_query.collect().to_pandas())\n\nPlot the percentage change in college annual tuition costs from 2010 to 2015 as a chloropleth heatmap.\n\nch.mark_geoshape(\n    stroke='black'\n).encode(\n    shape='geo:G',\n    color=alt.Color(\n                \"pct_change\",\n                scale=alt.Scale(scheme=\"oranges\"),\n                legend=alt.Legend(title='Change (%)')\n    ),\n    tooltip=[alt.Tooltip('state_name', title='State'),\n             alt.Tooltip('pct_change', title='Change (%)')]\n).transform_lookup(\n    lookup='state_id',\n    from_=alt.LookupData(data=states, key='id'),\n    as_='geo'\n).project(\n    type='albersUsa'\n).properties(\n    title='Percentage change in college tuition costs between 2010 and 2015',\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nPlot the annual cost of college tuition in the USA in 2015-16 as a dot plot.\n\nch.mark_circle(\n    size=60\n).encode(\n    x=alt.X('tuition', title='Tuition ($)'),\n    y=alt.Y('state_name', sort='-x', axis=alt.Axis(grid=True), title='State')\n).properties(\n    title='College tuition costs in the USA (2015-16)',\n    width=400,\n    height=600\n)\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys     : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\npolars  : 0.14.6\naltair  : 4.2.0\nrequests: 2.28.1\npandas  : 1.4.3"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html",
    "href": "posts/2022-09-01-stack_adt.html",
    "title": "Stack ADT",
    "section": "",
    "text": "Using Python’s collections.deque[^3] class as an implementation of the Stack ADT. A stack is:"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#dependencies",
    "href": "posts/2022-09-01-stack_adt.html#dependencies",
    "title": "Stack ADT",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import deque"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#function",
    "href": "posts/2022-09-01-stack_adt.html#function",
    "title": "Stack ADT",
    "section": "Function",
    "text": "Function\nWe used a deque for the Stack in this implementation of has_balanced_brackets.\n\ndef has_balanced_brackets(expression: str) -> bool:\n    \"\"\"Return true if the given expression has balanced brackets.\n    \"\"\"\n    stack = deque()\n    matching = {')': '(', '}': '{', ']': '[', '>': '<'}\n    left, right = set(matching.values()), matching.keys()\n    for char in expression:\n        if char in left:\n            stack.append(char)\n        elif char in right:\n            if len(stack) == 0 or stack[-1] != matching[char]:\n                return False\n            stack.pop()\n    return len(stack) == 0"
  },
  {
    "objectID": "posts/2022-09-01-stack_adt.html#main",
    "href": "posts/2022-09-01-stack_adt.html#main",
    "title": "Stack ADT",
    "section": "Main",
    "text": "Main\n\nTesting\n(Apologies for the non-standard table representation!)\n\n#         desc           expression                          exp result   \ncases = [['no text',     '',                                 True],\n         ['no brackets', 'brackets are like Russian dolls',  True],\n         ['matched',     '(3 + 4)',                          True],\n         ['mismatched',  '(3 + 4]',                          False],\n         ['not opened',  '3 + 4]',                           False],\n         ['not closed',  '(3 + 4',                           False],\n         ['wrong order', 'close ) before open (',            False],\n         ['no nesting',  '()[]\\{\\}<>',                       True],\n         ['nested',      '([{(<[{}]>)}])',                   True],\n         ['nested pair', 'items[(i - 1):(i + 1)]',           True]]\n\nfor (test, expr, exp_res) in cases:\n    assert has_balanced_brackets(expr) == exp_res, f'Test {test} failed'\nprint('Testing complete.')\n\nTesting complete.\n\n\n\n\nPerformance\nThe performance tests show a doubling in has_balanced_brackets’s time-to-run, as the |expression| doubles. It has a linear complexity.\n\nbrackets = \"()\"\nsizes = [100, 200, 400, 800]\nfor i, n in enumerate(sizes):\n    print(f'Test {i+1} (n={n}) =')\n    expr = brackets * n\n    %timeit -r 3 -n 5000 has_balanced_brackets(expr)\n\nTest 1 (n=100) =\n21.8 µs ± 917 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 2 (n=200) =\n41.7 µs ± 166 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 3 (n=400) =\n84 µs ± 795 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\nTest 4 (n=800) =\n158 µs ± 996 ns per loop (mean ± std. dev. of 3 runs, 5,000 loops each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nsys: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html",
    "title": "Advent of Code 2015, Day 3",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 3: Perfectly Spherical Houses in a Vacuum."
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#dependencies",
    "title": "Advent of Code 2015, Day 3",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#functions",
    "title": "Advent of Code 2015, Day 3",
    "section": "Functions",
    "text": "Functions\n\ndef get_next(position, direction) -> tuple:\n    \"\"\"Return the next position based on the given direction.\n    \"\"\"\n    x, y = position\n    if direction == '>':\n        return (x+1, y)\n    elif direction == '<':\n        return (x-1, y)\n    elif direction == '^':\n        return (x, y+1)\n    else:\n        return (x, y-1)\n\n\ndef deliver_presents(directions: str) -> set:\n    \"\"\"Return a set of tuples representing the positions of houses where\n    presents were delivered.\n    \"\"\"\n    houses, position = set(), (0, 0)\n    houses.add(position)\n    for direction in directions:\n        next_position = get_next(position, direction)\n        houses.add(next_position)\n        position = next_position\n    return houses"
  },
  {
    "objectID": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "href": "posts/2022-09-03-advent_of_code_2015_day3.html#main",
    "title": "Advent of Code 2015, Day 3",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 3)\nprint(f\"line = '{line[:5]}'\")\n\nline = '^^<<v'\n\n\n\n\nPart 1\n\nprint(f'Solution = {len(deliver_presents(line))}')\n\nSolution = 2565\n\n\n\n\nPart 2\n\nsanta_houses = deliver_presents(line[::2])\nrobot_houses = deliver_presents(line[1::2])\nprint(f'Solution = {len(santa_houses.union(robot_houses))}')\n\nSolution = 2639\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit len(deliver_presents(line))\nprint('Part 2 =')\n%timeit len(deliver_presents(line[::2]).union(deliver_presents(line[1::2])))\n\nPart 1 =\n2.09 ms ± 38 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nPart 2 =\n2.16 ms ± 12.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html",
    "title": "NFL Salaries",
    "section": "",
    "text": "Visualising the change in median earnings of NFL positions over time.\n\n\nWelcome to Week 2 of #TidyTuesday! We'll be exploring a 538 article on NFL salaries! Good luck & have fun!Data: https://t.co/8NaXR93uIXArticle: https://t.co/6GdoPb0hJcData Source: https://t.co/vliDMOl9LcBlog: https://t.co/cZJ94Hhz7U#tidyverse #rstats #dataviz #ggplot2 #r4ds pic.twitter.com/AF7qTFLvkj\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 9, 2018"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#dependencies",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#dependencies",
    "title": "NFL Salaries",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport polars as pl\nimport pandas as pd\nimport altair as alt\nfrom matplotlib import pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#functions",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#functions",
    "title": "NFL Salaries",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#main",
    "href": "posts/2022-09-06-tt_y2018_w02_nfl_salary.html#main",
    "title": "NFL Salaries",
    "section": "Main",
    "text": "Main\n\nCache the data\n\nnfl_salary_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob'\n         + '/master/data/2018/2018-04-09/nfl_salary.xlsx?raw=true'),\n    fname='nfl_salary.xlsx'\n)\n\n\n\nLoad the data\n\nnfl_salary = pl.DataFrame(pd.read_excel(nfl_salary_path)).lazy()\nnfl_salary.schema\n\n{'year': polars.datatypes.Int64,\n 'Cornerback': polars.datatypes.Int64,\n 'Defensive Lineman': polars.datatypes.Int64,\n 'Linebacker': polars.datatypes.Int64,\n 'Offensive Lineman': polars.datatypes.Int64,\n 'Quarterback': polars.datatypes.Float64,\n 'Running Back': polars.datatypes.Int64,\n 'Safety': polars.datatypes.Int64,\n 'Special Teamer': polars.datatypes.Float64,\n 'Tight End': polars.datatypes.Int64,\n 'Wide Receiver': polars.datatypes.Int64}\n\n\n\n\nPrepare the data\nWe remove high outliers, those players with a salary greater than $30M.\n\nlazy_query = nfl_salary.melt(\n    id_vars='year',\n    variable_name='position',\n    value_name='salary'\n).with_column(\n    pl.col('salary').cast(int)\n).drop_nulls(\n).sort(\n    by=['position', 'year', 'salary'],\n    reverse=[False, False, True]\n).filter(\n    pl.col('salary') < 30_000_000\n).groupby(\n    by=['position', 'year'],\n    maintain_order=True\n).head(\n    16\n)\nlazy_query.schema\n\n{'position': polars.datatypes.Utf8,\n 'year': polars.datatypes.Int64,\n 'salary': polars.datatypes.Int64}\n\n\n\n\nVisualise the data\n\n_ch = alt.Chart(\n        lazy_query.collect().to_pandas()\n).properties(\n    width=110,\n    height=200\n)\n_line = _ch.mark_line(\n    color='darkorange'\n).encode(\n    x=alt.X('year:N', title=''),\n    y=alt.Y('mean(salary)', title='Average cap value ($USD)'),\n)\n_scatter = _ch.mark_circle(color='lightgrey').encode(\n    x='year:N',\n    y='salary'\n)\nalt.layer(_scatter, _line).facet(\n    facet=alt.Facet('position', title=''),\n    columns=5,\n    title=alt.TitleParams(\n        'The average pay for top running backs has stalled',\n        subtitle=('Average cap value of 16 highest-paid players in each '\n                  + ' position'),\n        fontSize=18\n    )\n)\n\n\n\n\n\n\n\n_gsource = lazy_query.with_columns(\n    [pl.col('salary').sum().over(['position', 'year']).alias('total_by_pos'),\n     pl.col('salary').sum().over(['year']).alias('total')]\n).with_column(\n    (pl.col('total_by_pos') / pl.col('total')).alias('prop_by_pos')\n).with_column(\n    (pl.col('prop_by_pos') * 100).round(1).alias('pct_by_pos')\n).select(\n    ['position',\n     'year',\n     'pct_by_pos']\n).unique(\n)\n\n_ch = alt.Chart(\n        _gsource.collect().to_pandas()\n).properties(\n    width=600,\n    height=400,\n    title=alt.TitleParams(\n        'Teams are spending less on RBs',\n        subtitle=('Percent of money spent on the top 16 players at each'\n                  + ' position'),\n        anchor='start',\n        fontSize=18\n    )\n)\n_line = _ch.mark_line(color='darkorange').encode(\n    x=alt.X('year:N', title=''),\n    y=alt.Y('pct_by_pos', title='Percent spent at each position'),\n    color='position'\n)\n_scatter = _ch.mark_circle(color='lightgrey').encode(\n    x='year:N',\n    y='pct_by_pos',\n    color='position'\n)\n_scatter + _line\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iv\n\nrequests  : 2.28.1\npolars    : 0.14.6\npandas    : 1.4.3\nsys       : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\naltair    : 4.2.0\nmatplotlib: 3.5.3\nseaborn   : 0.11.2"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html",
    "href": "posts/2022-09-08-queue_adt.html",
    "title": "Queue ADT",
    "section": "",
    "text": "Using Python’s deque class as an implementation of the Queue ADT. A queue is:\n\n[A]n ordered collection of items where the addition of new items happens at one end, called the “rear,” and the removal of existing items occurs at the other end, commonly called the “front.” As an element enters the queue it starts at the rear and makes its way toward the front, waiting until that time when it is the next element to be removed.\nThe most recently added item in the queue must wait at the end of the collection. The item that has been in the collection the longest is at the front. This ordering principle is sometimes called FIFO, first-in first-out. It is also known as “first-come first-served.”\nWhat is a Queue? (Problem Solving with Algorithms and Data Structures using Python)\n\nIn this example, we take the left-hand side of the deque as the front of the queue, and the right-hand side as the back.\nThe table below shows the common operations of a queue. We take the left-hand side of the deque as the front of the queue, and the right-hand side as the back.\n\n\n\n\n\n\n\n\noperation\ndescription\npython\n\n\n\n\nnew\nInitialise an empty stack\ndeque()\n\n\nsize\nReturn the number of items in the queue\nlen(q)\n\n\nis empty\nReturn if the queue contains no items\nlen(q) == 0\n\n\nenqueue\nAdd an item to the back of the queue\nq.append(x)\n\n\nfront\nReturn the item at the front of the queue\nq[0]\n\n\nback\nReturn the item at the back of the queue\nq[-1]\n\n\ndequeue\nRemove and return the item at the front of the queue\nq.popleft()\n\n\n\nWe showed one example of how a Queue can be used: Perform a level-first traversal of a binary tree.\nThe function collect_by_level returns the values of the nodes of a binary tree as a level-order list.\nBelow is the algorithm we used to implement the level-first traveral. Queue operations are emphasised so they can be easily identified.\n\nlet q be a new queue\nlet collection be an empty list\nenqueue(q, tree)\nwhile not is empty(q)\n\nlet node = front(q)\nif node is not an empty node:\n\nappend value(node) to collection\nenqueue(q, left(node))\nenqueue(q, right(node))\n\ndequeue(q)"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#dependencies",
    "href": "posts/2022-09-08-queue_adt.html#dependencies",
    "title": "Queue ADT",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom collections import deque\nimport binarytree as bt"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#functions",
    "href": "posts/2022-09-08-queue_adt.html#functions",
    "title": "Queue ADT",
    "section": "Functions",
    "text": "Functions\n\ndef collect_by_level(tree: bt.Node) -> list:\n    \"\"\"Return root and its descendant nodes as a list in a level-first\n    order.\n    \"\"\"\n    q = deque()\n    collection = []\n    q.append(tree)\n    while len(q) >= 1:\n        node = q[0]\n        if node is not None:\n            collection.append(node.val)\n            q.append(node.left)\n            q.append(node.right)\n        q.popleft()\n\n    return collection"
  },
  {
    "objectID": "posts/2022-09-08-queue_adt.html#main",
    "href": "posts/2022-09-08-queue_adt.html#main",
    "title": "Queue ADT",
    "section": "Main",
    "text": "Main\n\nTesting\n\nN = 10\narange = lambda n: [x for x in range(n)]\nassert all(\n    collect_by_level(bt.build2(list(range(n)))) == list(range(n))\n    for n in range(N)\n)\nprint('Testing complete')\n\nTesting complete\n\n\n\n\nExample use\nPopulate a binary tree with letters ‘a’ through ‘g’.\n\nletter_tree = bt.build2([chr(x) for x in range(97, 104)])\nprint(letter_tree)\n\n\n    __a__\n   /     \\\n  b       c\n / \\     / \\\nd   e   f   g\n\n\n\nPrint letter_tree in level-first order.\n\ncollect_by_level(letter_tree)\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\n\n\n\nPerformance\nThe performance tests show a doubling in collect_by_level’s time-to-run, as the |tree| doubles. It has a linear complexity.\n\nsizes = [100, 200, 400, 800]\nfor i, n in enumerate(sizes):\n    print(f'Test {i+1} (n={n}) =')\n    root = bt.build2(list(range(n)))\n    %timeit -r 3 -n 3000 collect_by_level(root)\n\nTest 1 (n=100) =\n30.7 µs ± 524 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 2 (n=200) =\n58.6 µs ± 564 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 3 (n=400) =\n120 µs ± 707 ns per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\nTest 4 (n=800) =\n231 µs ± 1.16 µs per loop (mean ± std. dev. of 3 runs, 3,000 loops each)\n\n\n\n%load_ext watermark\n%watermark --iv\n\nbinarytree: 6.5.1\nsys       : 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html",
    "title": "Advent of Code 2015, Day 4",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 4: The Ideal Stocking Stuffer."
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#dependencies",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#dependencies",
    "title": "Advent of Code 2015, Day 4",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport hashlib"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#functions",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#functions",
    "title": "Advent of Code 2015, Day 4",
    "section": "Functions",
    "text": "Functions\n\ndef mine_advent_coins(key: str, n: int) -> int:\n    \"\"\"Return the suffix needed so md5(key + suffix).hex begins with\n    n_zeroes.\n    \"\"\"\n    def hex_prefix(suffix, n) -> str:\n        return hashlib.md5(f'{(key + str(suffix))}'.encode()).hexdigest()[:n]\n\n    suffix = 1\n    prefix, target_prefix =  hex_prefix(suffix, n), '0' * n\n    while prefix != target_prefix:\n        suffix += 1\n        prefix = hex_prefix(suffix, n)\n\n    return suffix"
  },
  {
    "objectID": "posts/2022-09-10-advent_of_code_2015_day4.html#main",
    "href": "posts/2022-09-10-advent_of_code_2015_day4.html#main",
    "title": "Advent of Code 2015, Day 4",
    "section": "Main",
    "text": "Main\n\nInitialise the input\n\nKEY = 'ckczppom'\n\n\n\nPart 1\n\nprint(f'Solution = {mine_advent_coins(KEY, 5)}')\n\nSolution = 117946\n\n\n\n\nPart 2\n\nprint(f'Solution = {mine_advent_coins(KEY, 6)}')\n\nSolution = 3938038\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit mine_advent_coins(KEY, 5)\nprint('Part 2 =')\n%timeit mine_advent_coins(KEY, 6)\n\nPart 1 =\n145 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nPart 2 =\n4.91 s ± 34.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html",
    "href": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html",
    "title": "Setting Up a Time Series With Pandas",
    "section": "",
    "text": "Initialise a Pandas Series with a DatetimeIndex or PeriodIndex to represent a time series.\nData was sourced from Rdatasets1 via StatsModels2.\nThis note is split into three sections, dealing with annual, monthly, and quarterly time series. The general workflow for each section is:\n\nLoad the data\nIdentify the date of the initial observation\nInitialise the Series\nPlot the time series\n\nWhen initialising the Series:\n\npass a NumPy array to the data argument\npass an actual argument to the name argument\n\nthis is useful when plotting the time series\n\nannual or monthly time series: set a DatetimeIndex with date_range3\nquarterly time series: set a PeriodIndex with period_range4"
  },
  {
    "objectID": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html#dependencies",
    "href": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html#dependencies",
    "title": "Setting Up a Time Series With Pandas",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom statsmodels import datasets\nfrom matplotlib import pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html#main",
    "href": "posts/2022-09-12-m249_b2p1i_time_series_pandas.html#main",
    "title": "Setting Up a Time Series With Pandas",
    "section": "Main",
    "text": "Main\n\nsns.set_theme()\n\n\nAnnual time series\nLoad the data.\n\nbomregions = datasets.get_rdataset('bomregions', package='DAAG', cache=True)\nbomregions.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 109 entries, 0 to 108\nData columns (total 22 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Year       109 non-null    int64  \n 1   eastAVt    99 non-null     float64\n 2   seAVt      99 non-null     float64\n 3   southAVt   99 non-null     float64\n 4   swAVt      99 non-null     float64\n 5   westAVt    99 non-null     float64\n 6   northAVt   99 non-null     float64\n 7   mdbAVt     99 non-null     float64\n 8   auAVt      99 non-null     float64\n 9   eastRain   109 non-null    float64\n 10  seRain     109 non-null    float64\n 11  southRain  109 non-null    float64\n 12  swRain     109 non-null    float64\n 13  westRain   109 non-null    float64\n 14  northRain  109 non-null    float64\n 15  mdbRain    109 non-null    float64\n 16  auRain     109 non-null    float64\n 17  SOI        109 non-null    float64\n 18  co2mlo     50 non-null     float64\n 19  co2law     79 non-null     float64\n 20  CO2        109 non-null    float64\n 21  sunspot    109 non-null    float64\ndtypes: float64(21), int64(1)\nmemory usage: 18.9 KB\n\n\nIdentify the initial year.\n\nbomregions.data['Year'].head(1)\n\n0    1900\nName: Year, dtype: int64\n\n\nInitialise the annual Series.\n\nts_srain = pd.Series(\n    data=bomregions.data['southRain'].to_numpy(),\n    name='obs',\n    index=pd.date_range(\n        start='1900',\n        periods=bomregions.data['southRain'].size,\n        freq='A-DEC',\n        name='year'\n    )\n)\nts_srain.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 109 entries, 1900-12-31 to 2008-12-31\nFreq: A-DEC\nSeries name: obs\nNon-Null Count  Dtype  \n--------------  -----  \n109 non-null    float64\ndtypes: float64(1)\nmemory usage: 1.7 KB\n\n\nPlot the time series.\n\n_g = sns.relplot(\n            x=ts_srain.index,\n            y=ts_srain,\n            kind='line',\n            height=4,\n            aspect=1.5\n)\nplt.ylabel('south rainfall')\nplt.show()\n\n\n\n\n\n\nMonthly time series\nLoad the data.\n\nelecequip = datasets.get_rdataset('elecequip', package='fpp2', cache=True)\nelecequip.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   time    195 non-null    float64\n 1   value   195 non-null    float64\ndtypes: float64(2)\nmemory usage: 3.2 KB\n\n\nIdentify the initial month.\n\nelecequip.data['time'].head(1)\n\n0    1996.0\nName: time, dtype: float64\n\n\nInitialise the monthly Series.\n\nts_elecequip = pd.Series(\n    data=elecequip.data['value'].to_numpy(),\n    name='new orders index',\n    index=pd.date_range(\n        start='1996-01',\n        periods=elecequip.data['value'].size,\n        freq='M',\n        name='month'\n    )\n)\nts_elecequip.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 195 entries, 1996-01-31 to 2012-03-31\nFreq: M\nSeries name: new orders index\nNon-Null Count  Dtype  \n--------------  -----  \n195 non-null    float64\ndtypes: float64(1)\nmemory usage: 3.0 KB\n\n\nPlot the monthly time series.\n\n_g = sns.relplot(\n            x=ts_elecequip.index,\n            y=ts_elecequip,\n            kind='line',\n            height=4,\n            aspect=1.5\n)\nplt.show()\n\n\n\n\n\n\nQuarterly time series\nLoad the data.\n\nmacrodat = datasets.get_rdataset('Macrodat', package='Ecdat', cache=True)\nmacrodat.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 168 entries, 0 to 167\nData columns (total 7 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   lhur    168 non-null    float64\n 1   punew   168 non-null    float64\n 2   fyff    168 non-null    float64\n 3   fygm3   168 non-null    float64\n 4   fygt1   168 non-null    float64\n 5   exruk   168 non-null    float64\n 6   gdpjp   162 non-null    float64\ndtypes: float64(7)\nmemory usage: 9.3 KB\n\n\nAccording to the documentation, the initial quarter is the first quarter of 1959.\nInitialise the quarterly Series. Note that we use a period_range, not a date_range.\n\nts_exruk = pd.Series(\n    data=macrodat.data['exruk'].to_numpy(),\n    name='USD-GBP EXCHANGE RATE',\n    index=pd.period_range(\n        start='1959-01-01',\n        periods=macrodat.data['exruk'].size,\n        freq='Q',\n        name='QUARTER'\n    )\n)\nts_exruk.info()\n\n<class 'pandas.core.series.Series'>\nPeriodIndex: 168 entries, 1959Q1 to 2000Q4\nFreq: Q-DEC\nSeries name: USD-GBP EXCHANGE RATE\nNon-Null Count  Dtype  \n--------------  -----  \n168 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.6 KB\n\n\nPlot the quarterly time series.5\n\nts_exruk.plot(kind='line', figsize=(6, 4), ylabel=ts_exruk.name)\nplt.show()"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html",
    "title": "Global Causes of Mortality",
    "section": "",
    "text": "Visualising the share of deaths by different causes by country.\n\n\n#r4ds presents Week 3 of #TidyTuesday! Let's explore global causes of mortality!Make a meaningful graphic, and post your code!Data: https://t.co/ygKv8PqOfIArticle: https://t.co/MOnlCBzdaLBlog: https://t.co/cZJ94Hhz7U #tidyverse #rstats #dataviz #ggplot2 @R4DScommunity pic.twitter.com/52rktsOcSQ\n\n— Tom Mock ❤️ Quarto (@thomas_mock) April 16, 2018"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#dependencies",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#dependencies",
    "title": "Global Causes of Mortality",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport os\nimport requests\nimport polars as pl\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data as vdata"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#functions",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#functions",
    "title": "Global Causes of Mortality",
    "section": "Functions",
    "text": "Functions\n\ndef cache_file(url: str, fname: str, dir_: str = './__cache') -> str:\n    \"\"\"Cache the file at given url in the given dir_ with the given\n    fname and return the local path.\n\n    Preconditions:\n    - dir_ exists\n    \"\"\"\n    local_path = f'{dir_}/{fname}'\n    if fname not in os.listdir(dir_):\n        r = requests.get(url, allow_redirects=True)\n        open(local_path, 'wb').write(r.content)\n    return local_path"
  },
  {
    "objectID": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#main",
    "href": "posts/2022-09-13-tt_y2018_w03_global_mortality.html#main",
    "title": "Global Causes of Mortality",
    "section": "Main",
    "text": "Main\n\nSet theme\n\nalt.themes.enable('latimes')\n\nThemeRegistry.enable('latimes')\n\n\n\n\nCache the data\n\ngmortality_path = cache_file(\n    url=('https://github.com/rfordatascience/tidytuesday/blob/master'\n         + '/data/2018/2018-04-16/global_mortality.xlsx?raw=true'),\n    fname='global_mortality.xlsx'\n)\n\n\niso_path = cache_file(\n    url=('https://raw.githubusercontent.com/lukes/'\n         + 'ISO-3166-Countries-with-Regional-Codes/master/all/all.csv'),\n    fname='iso_3166.csv'\n)\n\n\n\nLoad the data\n\nmortality = pl.DataFrame(pd.read_excel(gmortality_path)).lazy()\nmortality.schema\n\n{'country': polars.datatypes.Utf8,\n 'country_code': polars.datatypes.Utf8,\n 'year': polars.datatypes.Int64,\n 'Cardiovascular diseases (%)': polars.datatypes.Float64,\n 'Cancers (%)': polars.datatypes.Float64,\n 'Respiratory diseases (%)': polars.datatypes.Float64,\n 'Diabetes (%)': polars.datatypes.Float64,\n 'Dementia (%)': polars.datatypes.Float64,\n 'Lower respiratory infections (%)': polars.datatypes.Float64,\n 'Neonatal deaths (%)': polars.datatypes.Float64,\n 'Diarrheal diseases (%)': polars.datatypes.Float64,\n 'Road accidents (%)': polars.datatypes.Float64,\n 'Liver disease (%)': polars.datatypes.Float64,\n 'Tuberculosis (%)': polars.datatypes.Float64,\n 'Kidney disease (%)': polars.datatypes.Float64,\n 'Digestive diseases (%)': polars.datatypes.Float64,\n 'HIV/AIDS (%)': polars.datatypes.Float64,\n 'Suicide (%)': polars.datatypes.Float64,\n 'Malaria (%)': polars.datatypes.Float64,\n 'Homicide (%)': polars.datatypes.Float64,\n 'Nutritional deficiencies (%)': polars.datatypes.Float64,\n 'Meningitis (%)': polars.datatypes.Float64,\n 'Protein-energy malnutrition (%)': polars.datatypes.Float64,\n 'Drowning (%)': polars.datatypes.Float64,\n 'Maternal deaths (%)': polars.datatypes.Float64,\n 'Parkinson disease (%)': polars.datatypes.Float64,\n 'Alcohol disorders (%)': polars.datatypes.Float64,\n 'Intestinal infectious diseases (%)': polars.datatypes.Float64,\n 'Drug disorders (%)': polars.datatypes.Float64,\n 'Hepatitis (%)': polars.datatypes.Float64,\n 'Fire (%)': polars.datatypes.Float64,\n 'Heat-related (hot and cold exposure) (%)': polars.datatypes.Float64,\n 'Natural disasters (%)': polars.datatypes.Float64,\n 'Conflict (%)': polars.datatypes.Float64,\n 'Terrorism (%)': polars.datatypes.Float64}\n\n\n\niso = pl.DataFrame(pd.read_csv(iso_path)).lazy()\niso.schema\n\n{'name': polars.datatypes.Utf8,\n 'alpha-2': polars.datatypes.Utf8,\n 'alpha-3': polars.datatypes.Utf8,\n 'country-code': polars.datatypes.Int64,\n 'iso_3166-2': polars.datatypes.Utf8,\n 'region': polars.datatypes.Utf8,\n 'sub-region': polars.datatypes.Utf8,\n 'intermediate-region': polars.datatypes.Utf8,\n 'region-code': polars.datatypes.Float64,\n 'sub-region-code': polars.datatypes.Float64,\n 'intermediate-region-code': polars.datatypes.Float64}\n\n\n\ncountries = alt.topo_feature(vdata.world_110m.url, 'countries')\n\n\n\nVisualise the data\n\n_query = mortality.filter(\n    (pl.col('country') == 'Vanuatu')\n    & (pl.col('year') == 2003)\n).melt(\n    id_vars=['country', 'country_code', 'year'],\n    variable_name='cause',\n    value_name='share (%)'\n).with_columns(\n    [pl.col('cause').str.replace(r' (%)', '', literal=True),\n     pl.col('share (%)').round(2)]\n)\n\n_ch = alt.Chart(\n    _query.collect().to_pandas()\n).encode(\n    x='share (%):Q',\n    y=alt.Y('cause:N', sort='-x'),\n)\n_bars = _ch.mark_bar().encode(\n    color=alt.Color('cause:N', legend=None)\n)\n_text = _ch.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='share (%):Q',\n)\n(_bars + _text).properties(\n    width=500,\n    title='Share of deaths by cause, Vanuatu, 2003'\n).configure_title(\n    fontSize=20,\n    anchor='start'\n)\n\n\n\n\n\n\n\n_query = mortality.join(\n    other=iso,\n    left_on='country_code',\n    right_on='alpha-3'\n).filter(\n    (pl.col('year') == 2016)\n    & (pl.col('region') == 'Europe')\n).select(\n    ['country',\n     pl.col('country-code').alias('country_id'),\n     pl.col('Cardiovascular diseases (%)').round(2).alias('share (%)')]\n).collect(\n).to_pandas(\n)\n\nalt.Chart(\n    _query\n).mark_geoshape(\n    stroke='black'  # adds country borders\n).encode(\n    shape='geo:G',\n    color=alt.Color('share (%):Q', scale=alt.Scale(scheme=\"reds\")),\n    tooltip=['country:N',\n             'share (%):Q']\n).transform_lookup(\n    lookup='country_id',\n    from_=alt.LookupData(data=countries, key='id'),\n    as_='geo'\n).project(\n    type='mercator',\n    scale=415,\n    center=[15, 54],\n    clipExtent=[[0, 0], [600, 400]],\n).properties(\n    width=650,\n    height=400,\n    title=('Percentage share of deaths by Cardiovascular diseases'\n           + ' in Europe, 2016')\n)\n\n\n\n\n\n\nDue to an issue with plotting world maps in Altair1, we used a different kernel to prevent conflicts with dependencies."
  }
]