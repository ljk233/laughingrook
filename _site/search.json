[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "x + 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Laughing Rook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJul 25, 2022\n\n\nTesting Hypotheses: Z-Tests And Population Means\n\n\nM248,Statistics\n\n\n\n\nJul 25, 2022\n\n\nTesting Hypotheses: Z-Tests And Population Proportion\n\n\nM248,Statistics\n\n\n\n\nJul 5, 2022\n\n\nAn Exploratory Data Analysis of the American Community Survey\n\n\nTidyTuesday\n\n\n\n\nMay 28, 2022\n\n\nAdvent of Code 2015, Day 3\n\n\nAdventOfCode\n\n\n\n\nMay 26, 2022\n\n\nStack ADT\n\n\nadt\n\n\n\n\nMay 23, 2022\n\n\nDecomposing Seasonal Time Series\n\n\nM249,Statistics,TimeSeries\n\n\n\n\nMay 19, 2022\n\n\nAdvent of Code 2015, Day 2\n\n\nAdventOfCode\n\n\n\n\nMay 16, 2022\n\n\nDecomposing Non-Seasonal Time Series\n\n\nM249,Statistics,TimeSeries\n\n\n\n\nMay 12, 2022\n\n\nTravelling Salesperson Problem (Brute-force Search)\n\n\nAlgorithm,Graph\n\n\n\n\nMay 9, 2022\n\n\nSetting Up a Time Series With Pandas\n\n\nM249,Statistics,TimeSeries\n\n\n\n\nMay 5, 2022\n\n\nAdvent of Code 2015, Day 1\n\n\nAdventOfCode\n\n\n\n\nMay 2, 2022\n\n\nStratified Analyses\n\n\nM249,Statistics\n\n\n\n\nApr 28, 2022\n\n\nInitialising an Undirected Graph in NetworkX\n\n\nGraph\n\n\n\n\nApr 25, 2022\n\n\nCohort and Case-Control Studies\n\n\nM249,Statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-25-cohort_case_control_studies.html",
    "href": "posts/2022-04-25-cohort_case_control_studies.html",
    "title": "Cohort and Case-Control Studies",
    "section": "",
    "text": "Perform an epidemiological study on the results of a cohort study analysing the possible association between compulsory redundancies and incidents of serious self-inflicted injury (SSI) (Keefe, V., et al (2002)). The exposure is being made compulsorily redundant; and the disease is incidents of serious self-inflicted injury.\nThe study results were as follows.\n\n\n\n\nSSI (+)\nno SSI (-)\n\n\n\n\nmade redundant (+)\n14\n1931\n\n\nnot made redundant (-)\n4\n1763\n\n\n\nThe results were initialised as a NumPy array. Measures of association (odds ratio and relative risk) were calculated, including confidence interval estimates. A chi-squared test of no association was used to test the strength of evidence of an association. For completeness, we rounded-off the analysis by performing Fisher’s exact test.\nNote, some of the results were outputted as Pandas Series, rather than using the default return type. This is optional, and only done to provide a more standardised results output.\nThese topics are covered by M249, Book 1, Part 1."
  },
  {
    "objectID": "posts/2022-04-25-cohort_case_control_studies.html#dependencies",
    "href": "posts/2022-04-25-cohort_case_control_studies.html#dependencies",
    "title": "Cohort and Case-Control Studies",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-04-25-cohort_case_control_studies.html#global-constants",
    "href": "posts/2022-04-25-cohort_case_control_studies.html#global-constants",
    "title": "Cohort and Case-Control Studies",
    "section": "Global constants",
    "text": "Global constants\nThese are the results from the study.\n\nOBS = np.array([[14, 1931], [4, 1763]])"
  },
  {
    "objectID": "posts/2022-04-25-cohort_case_control_studies.html#main",
    "href": "posts/2022-04-25-cohort_case_control_studies.html#main",
    "title": "Cohort and Case-Control Studies",
    "section": "Main",
    "text": "Main\n\nInitialise the results\n\nctable = sm.stats.Table2x2(OBS)\nprint(ctable)\n\nA 2x2 contingency table with counts:\n[[  14. 1931.]\n [   4. 1763.]]\n\n\n\n\nMeasures of association\n\nOdds ratio\nReturn the point and interval estimates of the odds ratio.\n\npd.Series(\n    data={\n        'point': ctable.oddsratio,\n        'lcb': ctable.oddsratio_confint()[0],\n        'ucb': ctable.oddsratio_confint()[1],\n    },\n    name='odds ratio'\n)\n\npoint    3.195495\nlcb      1.049877\nucb      9.726081\nName: odds ratio, dtype: float64\n\n\n\n\nRelative risk\nReturn a point and interval estimate for the relative risk.\n\npd.Series(\n    data={\n        'point': ctable.riskratio,\n        'lcb': ctable.riskratio_confint()[0],\n        'ucb': ctable.riskratio_confint()[1],\n    },\n    name='relative risk'\n)\n\npoint    3.179692\nlcb      1.048602\nucb      9.641829\nName: relative risk, dtype: float64\n\n\n\n\n\nChi-squared test for no association\nThe expected frequencies under the null hypothesis of no association.\n\nctable.fittedvalues\n\narray([[   9.43157328, 1935.56842672],\n       [   8.56842672, 1758.43157328]])\n\n\nThe differences between the observed and expected frequencies.\n\nOBS - ctable.fittedvalues\n\narray([[ 4.56842672, -4.56842672],\n       [-4.56842672,  4.56842672]])\n\n\nThe contributions to the chi-squared test statistic.\n\nctable.chi2_contribs\n\narray([[2.21283577, 0.01078263],\n       [2.43574736, 0.01186883]])\n\n\nThe results of the chi-squared test.\n\nres = ctable.test_nominal_association()\npd.Series(\n    data={'statistic': res.statistic, 'pval': res.pvalue, 'df': int(res.df)},\n    name='chi-squared test',\n    dtype=object\n)\n\nstatistic    4.671235\npval         0.030672\ndf                  1\nName: chi-squared test, dtype: object\n\n\n\n\nFisher’s exact test\nThis study would not need Fisher’s exact test, given all expected frequencies are greater than 5, but we show it for completeness. There is no version of Fisher’s exact test in StatsModels, so we use SciPy instead.\n\n_, pval = st.fisher_exact(ctable.table)\npd.Series(data={'pval': pval}, name='fisher''s exact')\n\npval    0.033877\nName: fishers exact, dtype: float64"
  },
  {
    "objectID": "posts/2022-04-25-cohort_case_control_studies.html#references",
    "href": "posts/2022-04-25-cohort_case_control_studies.html#references",
    "title": "Cohort and Case-Control Studies",
    "section": "References",
    "text": "References\nVera Keefe, Papaarangi Reid, Clint Ormsby, Bridget Robson, Gordon Purdie, Joanne Baxter, Ngäti Kahungunu Iwi Incorporated, Serious health events following involuntary job loss in New Zealand meat processing workers, International Journal of Epidemiology, Volume 31, Issue 6, December 2002, Pages 1155–1161, https://doi.org/10.1093/ije/31.6.1155"
  },
  {
    "objectID": "posts/2022-04-28-initialise_undirected_graph_networkx.html",
    "href": "posts/2022-04-28-initialise_undirected_graph_networkx.html",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "",
    "text": "Initialise and populate an undirected weighted graph using NetworkX. The source data is a CSV file listing the road network in Europe as an edge list. (Nodes are cities, and the edges represent roads connecting cities.)\nWe first import the data into a Pandas DataFrame. You don’t have to use a DataFrame to hold the source data, but there are special characters in the source data, and so we outsourced dealing with them to Pandas. Next, we exported the DataFrame as a dictionary of dictionaries. We initialised an empty graph, and populated it (using list comprehension and the dictionary.) We closed the note by showing how to access the nodes, neighbors, and edges of the graph.\nWhilst we could populate the graph during initialisation, we found it added unneeded complexity. Final note, the |edges| ≠ |edge list| because the NetworkX Graph class does not permit parallel edges between two nodes. (In other words, the source list has multiple roads connecting some cities.)"
  },
  {
    "objectID": "posts/2022-04-28-initialise_undirected_graph_networkx.html#dependencies",
    "href": "posts/2022-04-28-initialise_undirected_graph_networkx.html#dependencies",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nimport networkx as nx"
  },
  {
    "objectID": "posts/2022-04-28-initialise_undirected_graph_networkx.html#global-constants",
    "href": "posts/2022-04-28-initialise_undirected_graph_networkx.html#global-constants",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Global constants",
    "text": "Global constants\nThis is the URL to the source data.\n\nEROADS_URL = ('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n              + '/main/graphs/eroads_edge_list.csv')\n\nColumn titles in the source data. We feel it improves the readability of the code when populating the graph, but it is optional, and you could instead directly pass the column titles.\n\nU = 'origin_reference_place'\nV = 'destination_reference_place'\nUCO = 'origin_country_code'\nVCO = 'destination_country_code'\nW = 'distance'\nRN = 'road_number'\nWC = 'watercrossing'"
  },
  {
    "objectID": "posts/2022-04-28-initialise_undirected_graph_networkx.html#main",
    "href": "posts/2022-04-28-initialise_undirected_graph_networkx.html#main",
    "title": "Initialising an Undirected Graph in NetworkX",
    "section": "Main",
    "text": "Main\n\nImport the data\n\neroads = pd.read_csv(EROADS_URL)\neroads.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 7 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   road_number                  1250 non-null   object\n 1   origin_country_code          1250 non-null   object\n 2   origin_reference_place       1250 non-null   object\n 3   destination_country_code     1250 non-null   object\n 4   destination_reference_place  1250 non-null   object\n 5   distance                     1250 non-null   int64 \n 6   watercrossing                1250 non-null   bool  \ndtypes: bool(1), int64(1), object(5)\nmemory usage: 59.9+ KB\n\n\n\n\nExport data to a dictionary\nEach entry in the dictionary is a dictionary representing a single row, where the keys are the column titles.\n\nedges = eroads.to_dict(orient='records')\nedges[0]\n\n{'road_number': 'E01',\n 'origin_country_code': 'GB',\n 'origin_reference_place': 'Larne',\n 'destination_country_code': 'GB',\n 'destination_reference_place': 'Belfast',\n 'distance': 36,\n 'watercrossing': False}\n\n\n\n\nInitalise the graph\n\ng = nx.Graph()\n\n\n\nPopulate the graph\nAdds the nodes. We perform it on both the source and destination nodes in the edge list to ensure we populate all the cities. (There’s a chance that a city does not appear as a source city in the data.) The dictionary we pass in the tuple are data describing a node.\n\ng.add_nodes_from((e[U], {'country': e[UCO]}) for e in edges)\ng.add_nodes_from((e[V], {'country': e[VCO]}) for e in edges)\n\nAdd the edges. Given this is an undirected graph, there is no need to add the reverse edges, v → u. The dictionary we pass in the tuple are data that describe an edge.\n\ng.add_edges_from((e[U], e[V], {'weight': e[W], RN: e[RN], WC: e[WC]},)\n                 for e in edges)\n\n\n\nInspect the graph\nGet a description of the graph.\n\nprint(g)\n\nGraph with 894 nodes and 1198 edges\n\n\nGet a selection of the nodes.\n\n[n for n in g][:5]\n\n['Larne', 'Belfast', 'Dublin', 'Wexford', 'Rosslare']\n\n\nOutput a more descriptive list of nodes by calling the nodes() method.\n\n[n for n in g.nodes(data=True)][:5]\n\n[('Larne', {'country': 'GB'}),\n ('Belfast', {'country': 'GB'}),\n ('Dublin', {'country': 'IRL'}),\n ('Wexford', {'country': 'IRL'}),\n ('Rosslare', {'country': 'IRL'})]\n\n\nView the neighbours of the Roma node.\n\n[neighbor for neighbor in g['Roma']]\n\n['Arezzo', 'Grosseto', 'Pescara', 'San Cesareo']\n\n\nWe can get a more descriptive list of a node’s neighbours by not using list comprehension.\n\nprint(g['Roma'])\n\n{'Arezzo': {'weight': 219, 'road_number': 'E35', 'watercrossing': False}, 'Grosseto': {'weight': 182, 'road_number': 'E80', 'watercrossing': False}, 'Pescara': {'weight': 209, 'road_number': 'E80', 'watercrossing': False}, 'San Cesareo': {'weight': 36, 'road_number': 'E821', 'watercrossing': False}}\n\n\nFinally, we can simply output the edges of the Roma node.\n\n[e for e in g.edges('Roma', data=True)]\n\n[('Roma',\n  'Arezzo',\n  {'weight': 219, 'road_number': 'E35', 'watercrossing': False}),\n ('Roma',\n  'Grosseto',\n  {'weight': 182, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'Pescara',\n  {'weight': 209, 'road_number': 'E80', 'watercrossing': False}),\n ('Roma',\n  'San Cesareo',\n  {'weight': 36, 'road_number': 'E821', 'watercrossing': False})]"
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html",
    "href": "posts/2022-05-02-stratified_analyses.html",
    "title": "Stratified Analyses",
    "section": "",
    "text": "Perform a stratified analyses on the results of a stratified case-control study.\nData was taken from investigating the possible association between alcohol consumption and fatal car accidents in New York (J.R. McCarroll and W. Haddon Jr, 1962). The data was stratified by marital status, which was believed to be a possible confounder. The exposure was blood alcohol level of 100mg% or greater. Cases were drivers who were killed in car accidents for which they were considered to be responsible, and controls were selected drivers passing the locations where the accidents of the cases occurred, at the same time of day and on the same day of the week.\nThe results were as follows.\n\n\n\nMarried\ncases (+)\ncontrols (-)\n\n\n\n\nexposed (+)\n4\n5\n\n\nnot exposed (-)\n5\n103\n\n\n\n\n\n\nNot married\ncases (+)\ncontrols (-)\n\n\n\n\nexposed (+)\n10\n3\n\n\nnot exposed (-)\n5\n43\n\n\n\nThe tables were initialised as two NumPy NDArrays, one for each stratum/level. The analysis was performed using two classes from StatsModels: StratifiedTable1 and Table2x2.2 The results were outputted to either a Pandas Series or DataFrame, depending on the dimensionality of the result. (This is optional, and done so to provide a standardised output.)\nThese topics are covered in M249, Book 1, Part 2."
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html#dependencies",
    "href": "posts/2022-05-02-stratified_analyses.html#dependencies",
    "title": "Stratified Analyses",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels import api as sm"
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html#global-constants",
    "href": "posts/2022-05-02-stratified_analyses.html#global-constants",
    "title": "Stratified Analyses",
    "section": "Global constants",
    "text": "Global constants\nThese are the results from the study.\n\nMARRIED = np.array([[4, 5], [5, 103]])\nNOT_MARRIED = np.array([[10, 3], [5, 43]])"
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html#main",
    "href": "posts/2022-05-02-stratified_analyses.html#main",
    "title": "Stratified Analyses",
    "section": "Main",
    "text": "Main\n\nInitialise the contingency tables\n\nctables = sm.stats.StratifiedTable([MARRIED, NOT_MARRIED])\nctables.table\n\narray([[[  4.,  10.],\n        [  5.,   3.]],\n\n       [[  5.,   5.],\n        [103.,  43.]]])\n\n\n\n\nOdds ratios\n\nStratum-specific odds ratios\n\nres = pd.DataFrame(index=['odds ratio', 'lcb', 'ucb'])\nfor level, arr in zip(['married', 'not married'], [MARRIED, NOT_MARRIED]):\n    ctable = sm.stats.Table2x2(arr)\n    res[level] = (ctable.oddsratio,\n                  ctable.oddsratio_confint()[0],\n                  ctable.oddsratio_confint()[1])\nres.T\n\n\n\n\n\n  \n    \n      \n      odds ratio\n      lcb\n      ucb\n    \n  \n  \n    \n      married\n      16.480000\n      3.354211\n      80.969975\n    \n    \n      not married\n      28.666667\n      5.856619\n      140.316070\n    \n  \n\n\n\n\n\n\nCrude odds ratio\n\nctable = sm.stats.Table2x2(MARRIED + NOT_MARRIED)\npd.Series(\n    data={\n        'point': ctable.oddsratio,\n        'lcb': ctable.oddsratio_confint()[0],\n        'ucb': ctable.oddsratio_confint()[1]\n     },\n    name='crude odds ratio'\n)\n\npoint    25.550000\nlcb       8.682174\nucb      75.188827\nName: crude odds ratio, dtype: float64\n\n\n\n\nAdjusted odds ratio (Mantel–Haenszel odds ratio)\n\npd.Series(\n    data={\n        'point': ctables.oddsratio_pooled,\n        'lcb': ctables.oddsratio_pooled_confint()[0],\n        'ucb': ctables.oddsratio_pooled_confint()[1]\n     },\n    name='Mantel-Haeszel odds ratio'\n)\n\npoint    23.000610\nlcb       7.465154\nucb      70.866332\nName: Mantel-Haeszel odds ratio, dtype: float64\n\n\n\n\n\nTest for no association\nThis is the Mantel–Haenszel test.\n\nres = ctables.test_null_odds(correction=True)\npd.Series(\n    data={'statistic': res.statistic, 'pval': res.pvalue},\n    name='test for no association'\n).round(5)\n\nstatistic    36.60431\npval          0.00000\nName: test for no association, dtype: float64\n\n\n\n\nTest for homogeneity\nThis is Tarone’s test.\n\nres = ctables.test_equal_odds(adjust=True)\npd.Series(\n    data={'statistic': res.statistic.round(5), 'pval': res.pvalue.round(5)},\n    name='test for homogeneity'\n)\n\nstatistic    0.23557\npval         0.62742\nName: test for homogeneity, dtype: float64"
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html#references",
    "href": "posts/2022-05-02-stratified_analyses.html#references",
    "title": "Stratified Analyses",
    "section": "References",
    "text": "References\nMcCarroll, J.R. and Haddon Jr, W., 1962. A controlled study of fatal automobile accidents in New York City. Journal of chronic diseases, 15(8), pp.811-826."
  },
  {
    "objectID": "posts/2022-05-02-stratified_analyses.html#footnotes",
    "href": "posts/2022-05-02-stratified_analyses.html#footnotes",
    "title": "Stratified Analyses",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-05-05-advent_of_code_2015_day1.html",
    "href": "posts/2022-05-05-advent_of_code_2015_day1.html",
    "title": "Advent of Code 2015, Day 1",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 1: Not Quite Lisp."
  },
  {
    "objectID": "posts/2022-05-05-advent_of_code_2015_day1.html#dependencies",
    "href": "posts/2022-05-05-advent_of_code_2015_day1.html#dependencies",
    "title": "Advent of Code 2015, Day 1",
    "section": "Dependencies",
    "text": "Dependencies\n\nfrom itertools import accumulate\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-05-05-advent_of_code_2015_day1.html#functions",
    "href": "posts/2022-05-05-advent_of_code_2015_day1.html#functions",
    "title": "Advent of Code 2015, Day 1",
    "section": "Functions",
    "text": "Functions\n\ndef find_first(x, A) -> int:\n    \"\"\"Find the first index i where A[i] = x.\n\n    Precondtions:\n    - x in A\n    - A is 1-dimensional\n    - A support iteration\n    \"\"\"\n    return next(i for i, a in enumerate(A) if a == x)"
  },
  {
    "objectID": "posts/2022-05-05-advent_of_code_2015_day1.html#main",
    "href": "posts/2022-05-05-advent_of_code_2015_day1.html#main",
    "title": "Advent of Code 2015, Day 1",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 1)\nprint(f\"line = '{line[:5]}'\")\n\nline = '()((('\n\n\n\n\nTransform the input\n\nm = {'(': 1, ')': -1}\ndirections = [m[bracket] for bracket in line]\nprint(f\"directions = {directions[:5]}\")\n\ndirections = [1, -1, 1, 1, 1]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(directions)}\")\n\nSolution = 138\n\n\n\n\nPart 2\n\nprint(f\"Solution = {find_first(-1, accumulate(directions)) + 1}\")\n\nSolution = 1771\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(directions)\nprint('Part 2 =')\n%timeit find_first(-1, accumulate(directions)) + 1\n\nPart 1 =\n62.5 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nPart 2 =\n73.6 µs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "posts/2022-05-09-time_series_pandas.html",
    "href": "posts/2022-05-09-time_series_pandas.html",
    "title": "Setting Up a Time Series With Pandas",
    "section": "",
    "text": "Initialise a Pandas Series with a DatetimeIndex or PeriodIndex to represent a time series.\nData was sourced from Rdatasets1 via StatsModels2.\nWhen initialising the Series:\n\npass a NumPy array to the data argument\npass an actual argument to the name argument\n\nthis is useful when plotting the time series\n\nannual or monthly time series: set a DatetimeIndex with date_range3\nquarterly time series: set a PeriodIndex with period_range4\n\nThis note is split into three sections, dealing with annual, monthly, and quarterly time series. The general workflow for each section is:\n\nLoad the data\nIdentify the date of the initial observation\nInitialise the Series\nPlot the time series\n\nThere is currently an issue with plotting a Series with a PeriodIndex using Seaborn, so we instead use the plot method of Series for quarterly data.5"
  },
  {
    "objectID": "posts/2022-05-09-time_series_pandas.html#dependencies",
    "href": "posts/2022-05-09-time_series_pandas.html#dependencies",
    "title": "Setting Up a Time Series With Pandas",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom statsmodels import datasets\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nSet the graphing defaults. (This is optional.)\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/2022-05-09-time_series_pandas.html#main",
    "href": "posts/2022-05-09-time_series_pandas.html#main",
    "title": "Setting Up a Time Series With Pandas",
    "section": "Main",
    "text": "Main\n\nAnnual time series\nLoad the data.\n\nbomregions = datasets.get_rdataset('bomregions', package='DAAG', cache=True)\nbomregions.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 109 entries, 0 to 108\nData columns (total 22 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Year       109 non-null    int64  \n 1   eastAVt    99 non-null     float64\n 2   seAVt      99 non-null     float64\n 3   southAVt   99 non-null     float64\n 4   swAVt      99 non-null     float64\n 5   westAVt    99 non-null     float64\n 6   northAVt   99 non-null     float64\n 7   mdbAVt     99 non-null     float64\n 8   auAVt      99 non-null     float64\n 9   eastRain   109 non-null    float64\n 10  seRain     109 non-null    float64\n 11  southRain  109 non-null    float64\n 12  swRain     109 non-null    float64\n 13  westRain   109 non-null    float64\n 14  northRain  109 non-null    float64\n 15  mdbRain    109 non-null    float64\n 16  auRain     109 non-null    float64\n 17  SOI        109 non-null    float64\n 18  co2mlo     50 non-null     float64\n 19  co2law     79 non-null     float64\n 20  CO2        109 non-null    float64\n 21  sunspot    109 non-null    float64\ndtypes: float64(21), int64(1)\nmemory usage: 18.9 KB\n\n\nIdentify the initial year.\n\nbomregions.data['Year'].head(1)\n\n0    1900\nName: Year, dtype: int64\n\n\nInitialise the annual Series.\n\nts_south_rain = pd.Series(\n    data=bomregions.data['southRain'].to_numpy(),\n    name='SOUTHERN RAINFALL',\n    index=pd.date_range(\n        start='1900',\n        periods=bomregions.data['southRain'].size,\n        freq='A-DEC',\n        name='YEAR'\n    )\n)\nts_south_rain.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 109 entries, 1900-12-31 to 2008-12-31\nFreq: A-DEC\nSeries name: SOUTHERN RAINFALL\nNon-Null Count  Dtype  \n--------------  -----  \n109 non-null    float64\ndtypes: float64(1)\nmemory usage: 1.7 KB\n\n\nPlot the time series.\n\nsns.relplot(\n    x=ts_south_rain.index,\n    y=ts_south_rain,\n    kind='line',\n    aspect=2\n)\nplt.show()\n\n\n\n\n\n\nMonthly time series\nLoad the data.\n\nelecequip = datasets.get_rdataset('elecequip', package='fpp2', cache=True)\nelecequip.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   time    195 non-null    float64\n 1   value   195 non-null    float64\ndtypes: float64(2)\nmemory usage: 3.2 KB\n\n\nIdentify the initial month.\n\nelecequip.data['time'].head(1)\n\n0    1996.0\nName: time, dtype: float64\n\n\nInitialise the monthly Series.\n\nts_elecequip = pd.Series(\n    data=elecequip.data['value'].to_numpy(),\n    name='NEW ORDERS INDEX',\n    index=pd.date_range(\n        start='1996-01',\n        periods=elecequip.data['value'].size,\n        freq='M',\n        name='MONTH'\n    )\n)\nts_elecequip.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 195 entries, 1996-01-31 to 2012-03-31\nFreq: M\nSeries name: NEW ORDERS INDEX\nNon-Null Count  Dtype  \n--------------  -----  \n195 non-null    float64\ndtypes: float64(1)\nmemory usage: 3.0 KB\n\n\nPlot the monthly time series.\n\nsns.relplot(\n    x=ts_elecequip.index,\n    y=ts_elecequip,\n    kind='line',\n    aspect=2\n)\nplt.show()\n\n\n\n\n\n\nQuarterly time series\nLoad the data.\n\nmacrodat = datasets.get_rdataset('Macrodat', package='Ecdat', cache=True)\nmacrodat.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 168 entries, 0 to 167\nData columns (total 7 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   lhur    168 non-null    float64\n 1   punew   168 non-null    float64\n 2   fyff    168 non-null    float64\n 3   fygm3   168 non-null    float64\n 4   fygt1   168 non-null    float64\n 5   exruk   168 non-null    float64\n 6   gdpjp   162 non-null    float64\ndtypes: float64(7)\nmemory usage: 9.3 KB\n\n\nIdentify the initial quarter.\nAccording to the documentation, the initial quarter is the first quarter of 1959.\nInitialise the quarterly Series.\nNote that we use a period_range, not a date_range.\n\nts_exruk = pd.Series(\n    data=macrodat.data['exruk'].to_numpy(),\n    name='USD-GBP EXCHANGE RATE',\n    index=pd.period_range(\n        start='1959-01-01',\n        periods=macrodat.data['exruk'].size,\n        freq='Q',\n        name='QUARTER'\n    )\n)\nts_exruk.info()\n\n<class 'pandas.core.series.Series'>\nPeriodIndex: 168 entries, 1959Q1 to 2000Q4\nFreq: Q-DEC\nSeries name: USD-GBP EXCHANGE RATE\nNon-Null Count  Dtype  \n--------------  -----  \n168 non-null    float64\ndtypes: float64(1)\nmemory usage: 2.6 KB\n\n\nPlot the quarterly time series.\n\nts_exruk.plot(kind='line', figsize=(12, 6), ylabel=ts_exruk.name)\nplt.show()"
  },
  {
    "objectID": "posts/2022-05-09-time_series_pandas.html#footnotes",
    "href": "posts/2022-05-09-time_series_pandas.html#footnotes",
    "title": "Setting Up a Time Series With Pandas",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-05-12-tsp_bruteforce.html",
    "href": "posts/2022-05-12-tsp_bruteforce.html",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "",
    "text": "A solution to the travelling salesperson problem using a brute-force search.\nThe Travelling Salesperson Problem is defined as:\n\nGiven a set of cities and distances between every pair of cities, the [Travelling Salesperson] problem is to find the shortest possible route that visits every city exactly once and returns to the starting point.\nTraveling Salesman Problem (TSP) Implementation (GeeksForGeeks)\n\nWe use a complete undirected graph, with each edge being assigned a random weight (representing the distance).\nIn this implementation, we generate permutations and check if the |path| < |min path|.\nWhilst the function works, it is unusuable when |nodes(g)| ≥ 11, given P(11, 11) = 36720000 permutations to check!"
  },
  {
    "objectID": "posts/2022-05-12-tsp_bruteforce.html#dependencies",
    "href": "posts/2022-05-12-tsp_bruteforce.html#dependencies",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport random as rand\nimport math\nimport itertools as it\nimport networkx as nx"
  },
  {
    "objectID": "posts/2022-05-12-tsp_bruteforce.html#function",
    "href": "posts/2022-05-12-tsp_bruteforce.html#function",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Function",
    "text": "Function\n\ndef bruteforce_tsp(G: nx.Graph, start: object) -> float | int:\n    \"\"\"Return the shortest route that visits every city exactly once and\n    ends back at the start.\n\n    Solves the travelling salesperson with a brute-force search using\n    permutations.\n\n    Preconditions:\n    - G is a complete weighted graph\n    - start in G\n    - WG[u, v]['weight'] is the distance u -> v\n    \"\"\"\n    neighbours = set((node for node in G.nodes if node != start))\n    min_dist = math.inf\n    for path in it.permutations(neighbours):\n        u, dist = start, 0\n        for v in path:\n            dist += G.edges[u, v]['weight']\n            u = v\n        min_dist = min(min_dist, dist + G.edges[u, start]['weight'])\n\n    return min_dist"
  },
  {
    "objectID": "posts/2022-05-12-tsp_bruteforce.html#main",
    "href": "posts/2022-05-12-tsp_bruteforce.html#main",
    "title": "Travelling Salesperson Problem (Brute-force Search)",
    "section": "Main",
    "text": "Main\n\nInitialise the graph\nWe initialise a complete weighted undirected graph with 5 nodes.\n\ncg = nx.complete_graph(['origin', 'a', 'b', 'c', 'd'])\ng = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\nprint(f\"g = {g}\")\n\ng = Graph with 5 nodes and 10 edges\n\n\n\n\nFind the shortest path from the origin\n\nprint(f\"Shortest path from the origin = {bruteforce_tsp(g, 'origin')}\")\n\nShortest path from the origin = 18\n\n\n\n\nCheck the performance\n\nfor n in [4, 6, 8, 10]:\n    print(f\"|nodes(g)| = {n}\")\n    cg = nx.complete_graph(n)\n    g = nx.Graph((u, v, {'weight': rand.randint(1, 10)}) for u, v in cg.edges)\n    %timeit bruteforce_tsp(g, 1)\n\n|nodes(g)| = 4\n16.6 µs ± 50.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n|nodes(g)| = 6\n444 µs ± 2.77 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n|nodes(g)| = 8\n24.4 ms ± 14 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n|nodes(g)| = 10\n2.18 s ± 8.88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "posts/2022-05-16-decomposing_non_seasonal.html",
    "href": "posts/2022-05-16-decomposing_non_seasonal.html",
    "title": "Decomposing Non-Seasonal Time Series",
    "section": "",
    "text": "Return the estimate of the trend component of a non-seasonal time series by taking the simple moving average.\nData was sourced from Rdatasets1 using StatsModels Datasets package.2\nThree trend estimates were obtained, using the workflow: rolling3 → mean → dropna\nThe choice of order must be an odd number. Too low an order risks under-smoothing, meaning much of the irrelavent noise is kept. Coversely, too high an order risks over-smoothing, meaning any subtle (but important) changes in the trend are ironed out\nThis topic was covered in M249, Book 2, Part 1.4."
  },
  {
    "objectID": "posts/2022-05-16-decomposing_non_seasonal.html#dependencies",
    "href": "posts/2022-05-16-decomposing_non_seasonal.html#dependencies",
    "title": "Decomposing Non-Seasonal Time Series",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom statsmodels import datasets\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nSet the graphing defaults. (This is optional.)\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/2022-05-16-decomposing_non_seasonal.html#main",
    "href": "posts/2022-05-16-decomposing_non_seasonal.html#main",
    "title": "Decomposing Non-Seasonal Time Series",
    "section": "Main",
    "text": "Main\n\nLoad the data\n\nbomregions = datasets.get_rdataset('bomregions', package='DAAG', cache=True)\nbomregions.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 109 entries, 0 to 108\nData columns (total 22 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Year       109 non-null    int64  \n 1   eastAVt    99 non-null     float64\n 2   seAVt      99 non-null     float64\n 3   southAVt   99 non-null     float64\n 4   swAVt      99 non-null     float64\n 5   westAVt    99 non-null     float64\n 6   northAVt   99 non-null     float64\n 7   mdbAVt     99 non-null     float64\n 8   auAVt      99 non-null     float64\n 9   eastRain   109 non-null    float64\n 10  seRain     109 non-null    float64\n 11  southRain  109 non-null    float64\n 12  swRain     109 non-null    float64\n 13  westRain   109 non-null    float64\n 14  northRain  109 non-null    float64\n 15  mdbRain    109 non-null    float64\n 16  auRain     109 non-null    float64\n 17  SOI        109 non-null    float64\n 18  co2mlo     50 non-null     float64\n 19  co2law     79 non-null     float64\n 20  CO2        109 non-null    float64\n 21  sunspot    109 non-null    float64\ndtypes: float64(21), int64(1)\nmemory usage: 18.9 KB\n\n\n\n\nInitialise and plot the time series\nIdentify the initial year.\n\nbomregions.data['Year'].head(1)\n\n0    1900\nName: Year, dtype: int64\n\n\nInitialise the Series.\n\nts_south_rain = pd.Series(\n    data=bomregions.data['southRain'].to_numpy(),\n    name='SOUTHERN RAINFALL',\n    index=pd.date_range(\n        start='1900',\n        periods=bomregions.data['southRain'].size,\n        freq='A-DEC',\n        name='YEAR'\n    )\n)\nts_south_rain.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 109 entries, 1900-12-31 to 2008-12-31\nFreq: A-DEC\nSeries name: SOUTHERN RAINFALL\nNon-Null Count  Dtype  \n--------------  -----  \n109 non-null    float64\ndtypes: float64(1)\nmemory usage: 1.7 KB\n\n\nPlot the time series.\n\nsns.relplot(\n    x=ts_south_rain.index,\n    y=ts_south_rain,\n    kind='line',\n    aspect=2\n)\nplt.show()\n\n\n\n\n\n\nDecompose the time series\nTry order = 5.\n\nma5 = ts_south_rain.rolling(window=5, center=True).mean().dropna()\ng = sns.relplot(\n    x=ma5.index,\n    y=ma5,\n    kind='line',\n    aspect=2,\n)\ng.set_ylabels('ma5')\nplt.show()\n\n\n\n\nTry order = 15.\n\nma15 = ts_south_rain.rolling(window=15, center=True).mean().dropna()\ng = sns.relplot(\n    x=ma15.index,\n    y=ma15,\n    kind='line',\n    aspect=2,\n)\ng.set_ylabels('ma15')\nplt.show()\n\n\n\n\nTry order = 25.\n\nma25 = ts_south_rain.rolling(window=25, center=True).mean().dropna()\ng = sns.relplot(\n    x=ma25.index,\n    y=ma25,\n    kind='line',\n    aspect=2,\n)\ng.set_ylabels('ma25')\nplt.show()"
  },
  {
    "objectID": "posts/2022-05-16-decomposing_non_seasonal.html#footnotes",
    "href": "posts/2022-05-16-decomposing_non_seasonal.html#footnotes",
    "title": "Decomposing Non-Seasonal Time Series",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-05-19-advent_of_code_2015_day2.html",
    "href": "posts/2022-05-19-advent_of_code_2015_day2.html",
    "title": "Advent of Code 2015, Day 2",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 2: I Was Told There Would Be No Math."
  },
  {
    "objectID": "posts/2022-05-19-advent_of_code_2015_day2.html#dependencies",
    "href": "posts/2022-05-19-advent_of_code_2015_day2.html#dependencies",
    "title": "Advent of Code 2015, Day 2",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-05-19-advent_of_code_2015_day2.html#functions",
    "href": "posts/2022-05-19-advent_of_code_2015_day2.html#functions",
    "title": "Advent of Code 2015, Day 2",
    "section": "Functions",
    "text": "Functions\n\ndef paper_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the wrapping paper needed for a present with dims (l, w, h).\n    \"\"\"\n    planes = [(length * width), (width * height), (height * length)]\n    return min(planes) + (2 * sum(planes))\n\n\ndef ribbon_needed(length: int, width: int, height: int) -> int:\n    \"\"\"Return the ribbon needed to cover a present with dims (l, w, h).\n    \"\"\"\n    bow = length * width * height\n    ribbon = 2 * sum(sorted([length, width, height])[:2])\n    return bow + ribbon"
  },
  {
    "objectID": "posts/2022-05-19-advent_of_code_2015_day2.html#main",
    "href": "posts/2022-05-19-advent_of_code_2015_day2.html#main",
    "title": "Advent of Code 2015, Day 2",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nlines = lr.datasets.get_advent_input(2015, 2)\nprint(f\"lines = {lines[:5]}\")\n\nfile was cached.\nlines = ['4x23x21', '22x29x19', '11x4x11', '8x10x5', '24x18x16']\n\n\n\n\nTransform the input\n\nllines = (line.split('x') for line in lines)\npdims = [[int(x) for x in lline] for lline in llines]\nprint(f\"pdims = {pdims[:5]}\")\n\npdims = [[4, 23, 21], [22, 29, 19], [11, 4, 11], [8, 10, 5], [24, 18, 16]]\n\n\n\n\nPart 1\n\nprint(f\"Solution = {sum(paper_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 1598415\n\n\n\n\nPart 2\n\nprint(f\"Solution = {sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)}\")\n\nSolution = 3812909\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit sum(paper_needed(l, w, h) for (l, w, h) in pdims)\nprint('Part 2 =')\n%timeit sum(ribbon_needed(l, w, h) for (l, w, h) in pdims)\n\nPart 1 =\n433 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nPart 2 =\n468 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "posts/2022-05-23-decomposing_seasonal.html",
    "href": "posts/2022-05-23-decomposing_seasonal.html",
    "title": "Decomposing Seasonal Time Series",
    "section": "",
    "text": "Return estimates of the trend, seasoanl, and irregular components of a seaonal time series using the seasonal_decompose function in StasModels.\nData was sourced from Rdatasets1 using StatsModels Datasets package.2\nThe seasonal_decompose function in StasModels returns an instance of DecomposeResult.3\nThe plot method of DecomposeResult returns a summary plot showing the original time series and its three components.4 Not that this method does not work for quarterly time series;5 these should instead be plotted using the plot method of a Pandas Series.\nWe plotted the seasonally adjusted series by subtracting seasonal component from the original time series.\nThis topic was covered in M249, Book 2, Part 1.4."
  },
  {
    "objectID": "posts/2022-05-23-decomposing_seasonal.html#dependencies",
    "href": "posts/2022-05-23-decomposing_seasonal.html#dependencies",
    "title": "Decomposing Seasonal Time Series",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom statsmodels import datasets\nfrom statsmodels.tsa import api as tsa\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nSet the graphing defaults. (This is optional.)\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/2022-05-23-decomposing_seasonal.html#main",
    "href": "posts/2022-05-23-decomposing_seasonal.html#main",
    "title": "Decomposing Seasonal Time Series",
    "section": "Main",
    "text": "Main\n\nLoad the data\n\nelecequip = datasets.get_rdataset('elecequip', package='fpp2', cache=True)\nelecequip.data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 195 entries, 0 to 194\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   time    195 non-null    float64\n 1   value   195 non-null    float64\ndtypes: float64(2)\nmemory usage: 3.2 KB\n\n\n\n\nInitialise and plot the time series\nThe first observation is in January 1996.\n\nelecequip.data['time'].head(1)\n\n0    1996.0\nName: time, dtype: float64\n\n\nInitialise the Series.\n\nts_elecequip = pd.Series(\n    data=elecequip.data['value'].to_numpy(),\n    name='NEW ORDERS INDEX',\n    index=pd.date_range(\n        start='1996-01',\n        periods=elecequip.data['value'].size,\n        freq='M',\n        name='MONTH'\n    )\n)\nts_elecequip.info()\n\n<class 'pandas.core.series.Series'>\nDatetimeIndex: 195 entries, 1996-01-31 to 2012-03-31\nFreq: M\nSeries name: NEW ORDERS INDEX\nNon-Null Count  Dtype  \n--------------  -----  \n195 non-null    float64\ndtypes: float64(1)\nmemory usage: 3.0 KB\n\n\nPlot the time series.\n\nsns.relplot(\n    x=ts_elecequip.index,\n    y=ts_elecequip,\n    kind='line',\n    aspect=2\n)\n\n<seaborn.axisgrid.FacetGrid at 0x20d20e9a230>\n\n\n\n\n\n\n\nDecompose the time series\n\ndecomp_ts = tsa.seasonal_decompose(ts_elecequip)\n\nPlot the decomposed time series. We pass observed=True as we’ve already plotted the time series above.\n\nf = decomp_ts.plot(observed=False)\nf.set_tight_layout(True)\nf.set_figheight(6)\nf.set_figwidth(10)\nplt.show()\n\n\n\n\nPlot a specific component by accessing the relevant attribute.\n\ng = sns.relplot(\n    x=decomp_ts.trend.index,\n    y=decomp_ts.trend,\n    kind='line',\n    aspect=2\n)\n\n\n\n\nGet the seasonal factors.\n\nsfactors = decomp_ts.seasonal[:12]\nsfactors.name = 'seasonal factors'\nsfactors.index = pd.Index(range(1, 13), name='period')\nsfactors\n\nperiod\n1     -5.887662\n2     -6.199273\n3      8.083171\n4     -6.314968\n5     -4.818468\n6      7.976088\n7     -1.575338\n8    -16.870416\n9      7.304324\n10     3.007671\n11     3.847366\n12    11.447504\nName: seasonal factors, dtype: float64\n\n\nPlot the seasonally adjusted series.\n\nsadjusted = decomp_ts.observed - decomp_ts.seasonal\ng = sns.relplot(x=sadjusted.index, y=sadjusted, kind='line', aspect=2)\ng.set_ylabels('SADJUSTED')\nplt.show()"
  },
  {
    "objectID": "posts/2022-05-23-decomposing_seasonal.html#footnotes",
    "href": "posts/2022-05-23-decomposing_seasonal.html#footnotes",
    "title": "Decomposing Seasonal Time Series",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-05-26-stack_adt.html",
    "href": "posts/2022-05-26-stack_adt.html",
    "title": "Stack ADT",
    "section": "",
    "text": "Implementation of the Stack ADT.\nA stack is….\n\nan ordered collection of items where the addition of new items and the removal of existing items always takes place at the same end. This end is commonly referred to as the “top.” The end opposite the top is known as the “base.”\nThe base of the stack is significant since items stored in the stack that are closer to the base represent those that have been in the stack the longest. The most recently added item is the one that is in position to be removed first. This ordering principle is sometimes called LIFO, last-in first-out. It provides an ordering based on length of time in the collection. Newer items are near the top, while older items are near the base.\nWhat is a Stack? (Problem Solving with Algorithms and Data Structures using Python)\n\nThe Stack ADT….\n\n\n\n\n\n\n\n\noperation\ndescription\nsignature\n\n\n\n\nnew\nInitialise an empty stack\nStack()\n\n\nis empty\nReturn if the stack contains no items\nis_empty() -> bool\n\n\npush\nAdd x to the top of the stack\npush(x: object) -> None\n\n\npeek\nReturn the top item of the stack\npeek() -> object\n\n\npop\nRemove the top item from the stack\npop() -> None"
  },
  {
    "objectID": "posts/2022-05-26-stack_adt.html#class",
    "href": "posts/2022-05-26-stack_adt.html#class",
    "title": "Stack ADT",
    "section": "Class",
    "text": "Class\n\nclass Stack:\n    \"\"\"Implementation of the Stack ADT using a Python list.\n\n    The underlying data structure is a list.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialise an empty stack.\n        \"\"\"\n        self.items: list[object] = []\n\n    def is_empty(self) -> bool:\n        \"\"\"Return true if the stack is empty, otherwise false.\n        \"\"\"\n        return len(self.items) == 0\n\n    def push(self, x: object) -> None:\n        \"\"\"Add x to the top of the stack.\n        \"\"\"\n        self.items.append(x)\n\n    def peek(self) -> object:\n        \"\"\"Return the top of the stack.\n\n        Preconditions:\n        - self is not empty.\n        \"\"\"\n        return self.items[-1]\n\n    def pop(self) -> None:\n        \"\"\"Remove the top of the stack.\n\n        Preconditions:\n        - self is not empty.\n        \"\"\"\n        self.items.pop()\n\n    def __str__(self) -> str:\n        return f\"stack({self.items})\""
  },
  {
    "objectID": "posts/2022-05-26-stack_adt.html#example-usage",
    "href": "posts/2022-05-26-stack_adt.html#example-usage",
    "title": "Stack ADT",
    "section": "Example usage",
    "text": "Example usage\nInitialise a new stack.\n\ns = Stack()\nprint(f\"s = {s}\")\n\ns = stack([])\n\n\nPopulate a stack.\n\ns = Stack()\nfor x in range(3):\n    s.push(x)\n    print(f\"push(s, {x})\")\nprint(f\"s = {s}\")\n\npush(s, 0)\npush(s, 1)\npush(s, 2)\ns = stack([0, 1, 2])\n\n\nCheck if stack is empty.\n\ns = Stack()\nprint(f\"Pre-push: is empty(s) = {s.is_empty()}\")\nprint('push(s, 0)...')\ns.push(0)\nprint(f\"Post-push: is empty(s) = {s.is_empty()}\")\n\nPre-push: is empty(s) = True\npush(s, 0)...\nPost-push: is empty(s) = False\n\n\nPeek at the top of the stack.\n\ns = Stack()\ns.push(0)\nprint(f\"peek(s) = {s.peek()}\")\n\npeek(s) = 0\n\n\nEmpty the stack.\n\ns = Stack()\nprint(f'Pre-populate: is empty(s) = {s.is_empty()}')\nprint('Populate the stack...')\nfor x in range(3):\n    s.push(x)\n    print(f\"  push(s, {x})\")\nprint(f'Post-populate: is empty(s) = {s.is_empty()}')\nprint('***')\nprint('Empty the stack...')\nwhile not s.is_empty():\n    print(f'  peek(s) = {s.peek()}; pop(s)...')\n    s.pop()\nprint(f'Post-emptying: is empty(s) = {s.is_empty()}')\n\nPre-populate: is empty(s) = True\nPopulate the stack...\n  push(s, 0)\n  push(s, 1)\n  push(s, 2)\nPost-populate: is empty(s) = False\n***\nEmpty the stack...\n  peek(s) = 2; pop(s)...\n  peek(s) = 1; pop(s)...\n  peek(s) = 0; pop(s)...\nPost-emptying: is empty(s) = True"
  },
  {
    "objectID": "posts/2022-05-28-advent_of_code_2015_day3.html",
    "href": "posts/2022-05-28-advent_of_code_2015_day3.html",
    "title": "Advent of Code 2015, Day 3",
    "section": "",
    "text": "This is my solution to Advent of Code 2015, Day 3: Perfectly Spherical Houses in a Vacuum."
  },
  {
    "objectID": "posts/2022-05-28-advent_of_code_2015_day3.html#dependencies",
    "href": "posts/2022-05-28-advent_of_code_2015_day3.html#dependencies",
    "title": "Advent of Code 2015, Day 3",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport laughingrook as lr"
  },
  {
    "objectID": "posts/2022-05-28-advent_of_code_2015_day3.html#functions",
    "href": "posts/2022-05-28-advent_of_code_2015_day3.html#functions",
    "title": "Advent of Code 2015, Day 3",
    "section": "Functions",
    "text": "Functions\n\ndef get_next(position, direction) -> tuple:\n    \"\"\"Return the next position based on the given direction.\n    \"\"\"\n    x, y = position\n    if direction == '>':\n        return (x+1, y)\n    elif direction == '<':\n        return (x-1, y)\n    elif direction == '^':\n        return (x, y+1)\n    else:\n        return (x, y-1)\n\n\ndef deliver_presents(directions: str) -> set:\n    \"\"\"Return a set of tuples representing the positions of houses where\n    presents were delivered.\n    \"\"\"\n    houses = set()\n    position = (0, 0)\n    houses.add(position)\n    for direction in directions:\n        next_position = get_next(position, direction)\n        houses.add(next_position)\n        position = next_position\n    return houses"
  },
  {
    "objectID": "posts/2022-05-28-advent_of_code_2015_day3.html#main",
    "href": "posts/2022-05-28-advent_of_code_2015_day3.html#main",
    "title": "Advent of Code 2015, Day 3",
    "section": "Main",
    "text": "Main\n\nLoad the input\n\nline = lr.datasets.get_advent_input(2015, 3)\nprint(f\"line = '{line[:5]}'\")\n\nfile was cached.\nline = '^^<<v'\n\n\n\n\nPart 1\n\nprint(f'Solution = {len(deliver_presents(line))}')\n\nSolution = 2565\n\n\n\n\nPart 2\n\nsanta_houses = deliver_presents(line[::2])\nrobot_houses = deliver_presents(line[1::2])\nprint(f'Solution = {len(santa_houses.union(robot_houses))}')\n\nSolution = 2639\n\n\n\n\nPerformance\n\nprint('Part 1 =')\n%timeit len(deliver_presents(line))\nprint('Part 2 =')\n%timeit len(deliver_presents(line[::2]).union(deliver_presents(line[1::2])))\n\nPart 1 =\n2.18 ms ± 37.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nPart 2 =\n2.15 ms ± 27.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/2022-07-05-tt_acs.html",
    "href": "posts/2022-07-05-tt_acs.html",
    "title": "An Exploratory Data Analysis of the American Community Survey",
    "section": "",
    "text": "This was the initial #TidyTuesday project, posted back on 30th April, 2018. Here is the motivating tweet from @thomas_mock\n\n\nWelcome to #TidyTuesday week 5, let's explore the 2015 American Community Survey! Data are 5 year average estimates at the county level from https://t.co/MB0WutGQLL via kaggleData: https://t.co/sElb4fcv3u Source: https://t.co/4lhpdlcEcD#rstats #tidyverse #r4ds #dataviz pic.twitter.com/5zAtlMBiq0\n\n— Tom Mock (@thomas_mock) April 30, 2018\n\n\nWe followed @thomas_mock‘s’ advice and plotted a selected set of graphs that would be useful in an exploratory data analysis (EDA).\nSource data was taken from census.gov (via kaggle.com).\nThe data was tidy. The column titles were in PascalCase, so we transformed them to snake_case. A new multi-index was constructed from the unique identifiers, (state, county, census_id,). Any missing values were replaced with the column mean.\nThe data was further processed to find the percentage of the population that were women, and the income per captita ($000). The change to the income per capita was done to reduce its range, so all four selected variables shared approximately the same range. Given the size of the data, we selected a subset of the variables to explore. These were:\n\nwomen (%age of population)\npoverty\npublic work\nincome per cap ($000)\n\nWe sampled the data (n = 1250) to get around Altair’s max row limit for in-notebook DataFrames. A standardised view of the data was also created for the correlation and covariance heatmaps. (See the standardise function.)\nFour visualisations were produced: The first shows a multiple boxplot; second is a matrix scatterplot; and third and fourth are heatmaps of the correlation and covariance matrices (of the standardised data.)"
  },
  {
    "objectID": "posts/2022-07-05-tt_acs.html#dependencies",
    "href": "posts/2022-07-05-tt_acs.html#dependencies",
    "title": "An Exploratory Data Analysis of the American Community Survey",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nimport altair as alt\nfrom matplotlib import pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2022-07-05-tt_acs.html#constants",
    "href": "posts/2022-07-05-tt_acs.html#constants",
    "title": "An Exploratory Data Analysis of the American Community Survey",
    "section": "Constants",
    "text": "Constants\n\nACS_URL = ('https://raw.githubusercontent.com/rfordatascience/tidytuesday/'\n           + 'master/data/2018/2018-04-30/week5_acs2015_county_data.csv')"
  },
  {
    "objectID": "posts/2022-07-05-tt_acs.html#functions",
    "href": "posts/2022-07-05-tt_acs.html#functions",
    "title": "An Exploratory Data Analysis of the American Community Survey",
    "section": "Functions",
    "text": "Functions\n\ndef pascal_to_snake_case(s: str) -> str:\n    return ''.join(['_' + c.lower() if c.isupper() else c for c in s])[1:]\n\n\ndef standardise(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"@pipeline.\n    Standardise the dataframe.\n    \"\"\"\n    df = df.transform(lambda x: (x - x.mean()) / x.std())\n    return df"
  },
  {
    "objectID": "posts/2022-07-05-tt_acs.html#main",
    "href": "posts/2022-07-05-tt_acs.html#main",
    "title": "An Exploratory Data Analysis of the American Community Survey",
    "section": "Main",
    "text": "Main\n\nLoad the data\n\n\nLoad the raw data as a Pandas DataFrame\nacs = pd.read_csv(ACS_URL, encoding='latin-1')\nacs.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3220 entries, 0 to 3219\nData columns (total 37 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   CensusId         3220 non-null   int64  \n 1   State            3220 non-null   object \n 2   County           3220 non-null   object \n 3   TotalPop         3220 non-null   int64  \n 4   Men              3220 non-null   int64  \n 5   Women            3220 non-null   int64  \n 6   Hispanic         3220 non-null   float64\n 7   White            3220 non-null   float64\n 8   Black            3220 non-null   float64\n 9   Native           3220 non-null   float64\n 10  Asian            3220 non-null   float64\n 11  Pacific          3220 non-null   float64\n 12  Citizen          3220 non-null   int64  \n 13  Income           3219 non-null   float64\n 14  IncomeErr        3219 non-null   float64\n 15  IncomePerCap     3220 non-null   int64  \n 16  IncomePerCapErr  3220 non-null   int64  \n 17  Poverty          3220 non-null   float64\n 18  ChildPoverty     3219 non-null   float64\n 19  Professional     3220 non-null   float64\n 20  Service          3220 non-null   float64\n 21  Office           3220 non-null   float64\n 22  Construction     3220 non-null   float64\n 23  Production       3220 non-null   float64\n 24  Drive            3220 non-null   float64\n 25  Carpool          3220 non-null   float64\n 26  Transit          3220 non-null   float64\n 27  Walk             3220 non-null   float64\n 28  OtherTransp      3220 non-null   float64\n 29  WorkAtHome       3220 non-null   float64\n 30  MeanCommute      3220 non-null   float64\n 31  Employed         3220 non-null   int64  \n 32  PrivateWork      3220 non-null   float64\n 33  PublicWork       3220 non-null   float64\n 34  SelfEmployed     3220 non-null   float64\n 35  FamilyWork       3220 non-null   float64\n 36  Unemployment     3220 non-null   float64\ndtypes: float64(27), int64(8), object(2)\nmemory usage: 930.9+ KB\n\n\n\n\nProcess the data\n\n\nMake view of ACS\nv_acs = acs\n\n\n\n\nGet the proportion of popn that are female\nv_acs['PropWomen'] = (\n    v_acs['Women']\n    .div(v_acs[['Men', 'Women']].sum(axis=1))\n    .mul(100)\n    .round(3)\n)\n\n\n\n\nGet the income_per_cap in $000\nv_acs['IncomePerCap_000'] = v_acs['IncomePerCap'].div(1000)\n\n\n\n\nSelect and sample the data (n=1250)\nv_sample = (\n    v_acs\n    .get(['IncomePerCap_000', 'PropWomen', 'Poverty', 'PublicWork'])\n    .sample(n=1250, random_state=20180430)\n)\n\n\n\n\nGet the correlation matrix\ncorr_df = (\n    v_sample.transform(standardise)\n    .corr()\n    .reset_index(drop=False)\n    .rename(columns={'index': 'X1'})\n    .melt(id_vars='X1', var_name='X2', value_name='r')\n    .round(2)\n)\n\n\n\n\nGet the covariance matrix\ncov_df = (\n    v_sample.transform(standardise)\n    .cov()\n    .reset_index(drop=False)\n    .rename(columns={'index': 'X1'})\n    .melt(id_vars='X1', var_name='X2', value_name='cov')\n    .round(2)\n)\n\n\n\n\nVisualise the data\n\n\nPlot multiple boxplots of the data\nalt.Chart(v_sample.melt()).mark_boxplot(size=50).encode(\n    x='variable',\n    y='value',\n    color=alt.Color('variable', legend=None)\n).properties(\n    height=400,\n    width=600,\n    title='Multiple boxplots of the selected ACS data'\n)\n\n\n\n\n\n\n\n\n\nPlot a matrix scatterplot of the selected data\ng = sns.pairplot(\n    v_sample,\n    diag_kws={'bins': 20},\n    height=1.8,\n)\ng.fig.suptitle('Matrix scatterplot of the selected ACS data', y= 1.04)\nplt.show()\n\n\n\n\n\n\n\nPlot a heatmap the correlation matrix\nbase_r = alt.Chart(corr_df).encode(\n    x='X1',\n    y='X2'\n)\n\nheatmap = base_r.mark_rect().encode(\n    color=alt.Color(\"r\", scale=alt.Scale(scheme=\"turbo\"))\n).properties(\n)\n\ntext = base_r.mark_text(baseline='middle', size=16).encode(\n    text='r',\n    color=alt.condition(\n        abs(alt.datum.r) > 0.5,\n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\n(heatmap + text).properties(\n    title='Correlation heatmap',\n    width=400,\n    height=400,\n)\n\n\n\n\n\n\n\n\n\nPlot a heatmap of the covariance matrix\nbase_cov = alt.Chart(cov_df).encode(\n    x='X1',\n    y='X2'\n)\n\nheatmap = base_cov.mark_rect().encode(\n    color=alt.Color(\"cov\", scale=alt.Scale(scheme=\"turbo\"))\n)\n\ntext = base_cov.mark_text(baseline='middle', size=16).encode(\n    text='cov',\n    color=alt.condition(\n        abs(alt.datum.cov) > 0.5,\n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\n(heatmap + text).properties(\n    width=400,\n    height=400,\n    title='Covariance heatmap'\n)"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_means.html",
    "href": "posts/2022-07-25-ztest_pop_means.html",
    "title": "Testing Hypotheses: Z-Tests And Population Means",
    "section": "",
    "text": "Perform a one-sample \\(z\\)-test of a population mean and a two-sample \\(z\\)-test of the difference between two population means.\nData on the mean pass rate across all UK test centres during the period from April 2014 to March 2015 was obtained and analysed using an approximate normal model. (Data were taken from the Open University, who did not provide the primary source.)\nTwo two-sided \\(z\\)-tests were performed:\n\nA one-sample \\(z\\)-test of the null hypothesis that the mean total pass rate for the UK practical driving test in 2014/15 was the same as the 2013/2014 (which was given as 47.1%).1\nA two-sample \\(z\\)-test of the null hypothesis that the mean total pass rate of females for the UK practical driving test in 2014/15 was the same as that of males.2\n\nNormality of the three data were checked using normal probability plots.3\nGeneral workflow:\n\nLoad the data\nDescribe the data\nPlot the data\nGet an interval estimate\nCheck the normality of the data\nPerform the hypothesis test\n\nThese topics were covered in M248, Units 8 and 9."
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_means.html#dependencies",
    "href": "posts/2022-07-25-ztest_pop_means.html#dependencies",
    "title": "Testing Hypotheses: Z-Tests And Population Means",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom scipy import stats as st\nfrom statsmodels.stats import weightstats as ws\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_means.html#constants",
    "href": "posts/2022-07-25-ztest_pop_means.html#constants",
    "title": "Testing Hypotheses: Z-Tests And Population Means",
    "section": "Constants",
    "text": "Constants\n\nURL = ('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n       + '/main/uk_prac_driving_tests/pass_rates.csv')"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_means.html#main",
    "href": "posts/2022-07-25-ztest_pop_means.html#main",
    "title": "Testing Hypotheses: Z-Tests And Population Means",
    "section": "Main",
    "text": "Main\n\nLoad data\n\npass_rates = pd.read_csv(URL)\npass_rates.info()\n\n\n\nTest 1: Was the mean total pass rate in 2014/15 equal to that in 2013/14?\nHere we test the hypotheses:\n\\[\nH_{0}: \\mu_{2014} = \\mu_{2013};\n\\hspace{3mm} H_{1}: \\mu_{2014} \\ne \\mu_{2013},\n\\]\nwhere \\(\\mu_{2013}=\\) 47.1%.\nDescribe the total pass rate.\n\npass_rates['total'].describe()\n\nInititialise an instance of DescrStatsW.\n\nd = ws.DescrStatsW(pass_rates['total'])\n\nPlot the distribution of total pass rates in 2014/15.\n\n_g = sns.displot(\n            x=d.data,\n            kind='hist',\n            kde=True,\n            stat='density',\n            aspect=2\n)\n\nReturn an interval estimate of the mean total pass rate.\n\npd.Series(data=d.zconfint_mean(), index=['lcb', 'ucb']).round(6)\n\nCheck the normality of the data.\n\n_f, _ax = plt.subplots(figsize=(11.8, 6))\n_res = st.probplot(x=d.data, plot=_ax)\n\nPerform the one-sample \\(z\\)-test.\n\npd.Series(data=d.ztest_mean(value=47.1), index=['zstat', 'pvalue']).round(6)\n\n\n\nTest 2: Was the mean pass rate of females equal to that of males?\nHere we test the hypotheses:\n\\[\nH_{0}: \\mu_{f} = \\mu_{m};\n\\hspace{3mm} H_{1}: \\mu_{f} \\ne \\mu_{m}.\n\\]\nDescribe the data.\n\npass_rates[['female', 'male']].describe().T\n\nInitialise instance of CompareMeans.\n\ncm = ws.CompareMeans(\n    ws.DescrStatsW(pass_rates['female']),\n    ws.DescrStatsW(pass_rates['male'])\n)\n\nReturn interval estimates of the mean female and male pass rates.\n\npd.DataFrame(\n    data=[cm.d1.zconfint_mean(), cm.d2.zconfint_mean()],\n    columns=['lcb', 'ucb'],\n    index=['female', 'male']\n)\n\nPlot the distributions of the pass rates.\n\n_g = sns.displot(\n            data=pass_rates[['female', 'male']].melt(),\n            x='value',\n            kind='hist',\n            col='variable',\n            hue='variable',\n            legend=False,\n            kde=True,\n            stat='density'\n)\n\nCheck the normality of both data.\n\n_f, _axs = plt.subplots(1, 2, figsize=(11.8, 6), sharey=True)\nst.probplot(x=cm.d1.data, plot=_axs[0])\nst.probplot(x=cm.d2.data, plot=_axs[1])\n_f.suptitle('Probability Plots', fontsize=16)\n_axs[0].set_title('female')\n_axs[1].set_title('male')\nplt.show()\n\nPerform the two-sample \\(z\\)-test.\n\npd.Series(data=cm.ztest_ind(), index=['zstat', 'pvalue']).round(6)"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_means.html#footnotes",
    "href": "posts/2022-07-25-ztest_pop_means.html#footnotes",
    "title": "Testing Hypotheses: Z-Tests And Population Means",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html",
    "href": "posts/2022-07-25-ztest_pop_proportion.html",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "",
    "text": "Perform a one-sample, one-sided \\(z\\)-test of a population proportion.\nData on the full time results of all football games in the English premier Leage 2018/19 season were obtained from www.football-data.co.uk.\nWe used a one-sample z-test to test the null hypothesis that there is no home advantage. If there was no home advantage, then we would expect that approximately a third of the games would be won by the home team. (This was the null hypothesis.) Otherwise, if there is a home advantage, then we would expect that the proportion of games won by the ome team would be greater than a third. (This was the alternative hypothesis.)\nMore formally, we tested the hypotheses:\n\\[\nH_{0}: p = \\frac{1}{3};\n\\hspace{3mm} H_{1}: p > \\frac{1}{3},\n\\]\nwhere \\(p\\) is the proportion of games won by the home team.\nGeneral workflow:\n\nLoad the data\nDescribe the data\n\nGiven the data were nominal, we used a frequency table to describe them\n\nPlot the data\nGet an interval estimate1\nPerform the hypothesis test2\n\nThis topic was covered in M248, Units 8 and 9."
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html#dependencies",
    "href": "posts/2022-07-25-ztest_pop_proportion.html#dependencies",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "Dependencies",
    "text": "Dependencies\n\nimport pandas as pd\nfrom statsmodels.stats import proportion as pr\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nsns.set_theme()"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html#functions",
    "href": "posts/2022-07-25-ztest_pop_proportion.html#functions",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "Functions",
    "text": "Functions\n\ndef frequency(s: pd.Series, x: object) -> int:\n    return s.map(lambda y: 1 if x == y else 0).sum()"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html#constants",
    "href": "posts/2022-07-25-ztest_pop_proportion.html#constants",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "Constants",
    "text": "Constants\n\nURL = ('https://raw.githubusercontent.com/ljk233/laughingrook-datasets'\n       + '/main/epl_results/season-1819.csv')"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html#main",
    "href": "posts/2022-07-25-ztest_pop_proportion.html#main",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "Main",
    "text": "Main\n\nLoad data\n\nresults = pd.read_csv(URL)\n\n\n\nGet data of interest\n\nftr = results.get(\n    'FTR'\n).rename(\n    'result'\n).replace(\n    {'H': 'home_win', 'A': 'away_win', 'D': 'draw'}\n)\nftr.info()\n\n\n\nDescribe the data\n\n_g = ftr.groupby(ftr).size().rename('frequency').to_frame()\n_g['proportion'] = _g['frequency'].transform(lambda x: x / _g.sum())\n_g\n\n\n\nVisualise the data\n\n_gs = sns.countplot(x=ftr)\nplt.title('Final results of the English Premier Leage 2018/19 season')\nplt.ylabel('frequency')\nplt.show()\n\n\n\nGet the results\nVariable x is the number of games won by the home team; and variable n is the total number of games in the season.\n\nx = ftr.transform(lambda x: 1 if x == 'home_win' else 0).sum()\nn = ftr.size\n\n\n\nInterval estimate of the proportion\n\npd.Series(data=pr.proportion_confint(x, n), index=['lcb', 'ucb'])\n\n\n\nPerform the z-test\n\npd.Series(\n    data=pr.proportions_ztest(x, n, value=1/3, alternative='larger'),\n    index=['zstat', 'pvalue']\n).round(6)"
  },
  {
    "objectID": "posts/2022-07-25-ztest_pop_proportion.html#footnotes",
    "href": "posts/2022-07-25-ztest_pop_proportion.html#footnotes",
    "title": "Testing Hypotheses: Z-Tests And Population Proportion",
    "section": "Footnotes",
    "text": "Footnotes"
  }
]